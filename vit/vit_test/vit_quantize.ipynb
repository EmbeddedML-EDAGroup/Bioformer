{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "voluntary-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.datasets.db6 import DB6MultiSession\n",
    "from pickle import load\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controlled-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "soviet-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "def bmm(a, b):\n",
    "    r = []\n",
    "    for i in range(a.shape[0]):\n",
    "        r.append(nn.functional.linear(a[i], b[i].T))\n",
    "    return torch.stack(r)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            torch.quantization.DeQuantStub(),\n",
    "            nn.GELU(),\n",
    "            torch.quantization.QuantStub(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        #self._init_parameters(dim, hidden_dim)\n",
    "\n",
    "    def _init_parameters(self, dim, hidden_dim):\n",
    "        bound1 = 1 / (dim ** .5)\n",
    "        bound2 = 1 / (hidden_dim ** .5)\n",
    "        nn.init.uniform_(self.net[0].weight, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[3].weight, -bound2, bound2)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound2, bound2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.dim_head = dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.quant_k = torch.quantization.QuantStub()\n",
    "        self.dequant_k = torch.quantization.DeQuantStub()\n",
    "        self.quant_sm = torch.quantization.QuantStub()\n",
    "        self.dequant_sm = torch.quantization.DeQuantStub()\n",
    "        self.quant_v = torch.quantization.QuantStub()\n",
    "        self.dequant_v = torch.quantization.DeQuantStub()\n",
    "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "        #self._init_parameters(dim, inner_dim)\n",
    "\n",
    "    def _init_parameters(self, dim, inner_dim):\n",
    "        bound = 1 / (dim ** .5)\n",
    "        nn.init.uniform_(self.to_q.weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_k.weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_v.weight, -bound, bound)\n",
    "\n",
    "        bound = 1 / (inner_dim ** .5)\n",
    "        nn.init.uniform_(self.to_out[0].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_out[0].bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n",
    "        q = q.reshape(b, n, h, -1).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(b, n, h, -1).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(b, n, h, -1).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #dots = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        k = self.quant_k(self.dequant_k(k))\n",
    "        k = k.transpose(-2, -1)\n",
    "        dots = []\n",
    "        for i in range(b):\n",
    "            dots.append(bmm(q[i], k[i]))\n",
    "        dots = torch.stack(dots)\n",
    "        \n",
    "        attn = self.quant_sm(self.attend(self.dequant_sm(dots)))\n",
    "        \n",
    "        #out = (attn @ v).transpose(1, 2).reshape(b, n, -1)\n",
    "        v_ = self.quant_v(self.dequant_v(v))\n",
    "        out = []\n",
    "        for i in range(b):\n",
    "            out.append(bmm(attn[i], v_[i]))\n",
    "        out = torch.stack(out).transpose(1, 2).reshape(b, n, -1)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "        self.quant_s1 = torch.quantization.QuantStub()\n",
    "        self.dequant_s1 = torch.quantization.DeQuantStub()\n",
    "        self.quant_s2 = torch.quantization.QuantStub()\n",
    "        self.dequant_s2 = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = self.quant_s1(self.dequant_s1(attn(x)) + self.dequant_s1(x))\n",
    "            x = self.quant_s2( self.dequant_s2(ff(x)) + self.dequant_s1(x))\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, window_size=(14, 300), patch_length=10, num_classes=8, dim=64, depth=1, heads=8, mlp_dim=128, pool='cls', dim_head=32, dropout=.2, emb_dropout=0., use_cls_token=True):\n",
    "        super().__init__()\n",
    "\n",
    "        channels, window_length = window_size\n",
    "        num_patches = (window_length // patch_length)\n",
    "        patch_dim = channels * patch_length\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.patch_conv = nn.Conv1d(in_channels=channels, out_channels=dim, kernel_size=patch_length, stride=patch_length, padding=0, bias=True)\n",
    "\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "        self._init_parameters(patch_dim)\n",
    "\n",
    "    def _init_parameters(self, patch_dim):\n",
    "        bound = 1 / (patch_dim ** .5)\n",
    "        nn.init.uniform_(self.patch_conv.weight, -bound, bound)\n",
    "        nn.init.uniform_(self.patch_conv.bias, -bound, bound)\n",
    "        nn.init.zeros_(self.pos_embedding)\n",
    "        nn.init.zeros_(self.mlp_head[1].weight)\n",
    "        nn.init.zeros_(self.mlp_head[1].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_conv(x).flatten(2).transpose(-2, -1)\n",
    "\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        \n",
    "        \"\"\"if self.use_cls_token:\n",
    "            cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efficient-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir, subject):\n",
    "    \n",
    "    all_other_subjects = ','.join([str(s) for s in range(1, 11) if s != subject])\n",
    "    minmax_picklename = f'./minmax/ds_minmax_sessions=5subjects={all_other_subjects}.pickle'\n",
    "    minmax = load(open(minmax_picklename, 'rb'))\n",
    "    \n",
    "    test_ds = DB6MultiSession(folder=os.path.expanduser(dataset_dir), \n",
    "                              subjects=[subject], sessions=list(range(5, 10)), \n",
    "                              minmax=minmax, n_classes='7+1', steady=True).to(device)\n",
    "    \n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arranged-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_set(dataset_dir, subject):\n",
    "    \n",
    "    all_other_subjects = ','.join([str(s) for s in range(1, 11) if s != subject])\n",
    "    minmax_picklename = f'./minmax/ds_minmax_sessions=5subjects={all_other_subjects}.pickle'\n",
    "    minmax = load(open(minmax_picklename, 'rb'))\n",
    "    \n",
    "    ds = DB6MultiSession(folder=os.path.expanduser(dataset_dir), \n",
    "                              subjects=[subject], sessions=list(range(5)), \n",
    "                              minmax=minmax, n_classes='7+1', steady=True).to(device)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "another-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(subject, training_fold):\n",
    "    net = ViT()\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    net.load_state_dict((torch.load(f\"checkpoints/vit_subject{subject}_fold{training_fold}.pth\")))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increased-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_loss_preds(net, criterion, loader):\n",
    "    y_pred, y_true = [], []\n",
    "    loss = 0\n",
    "    for X_batch, Y_batch in loader:\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        \n",
    "        outputs = net(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss += criterion(outputs, Y_batch).item()\n",
    "\n",
    "        y_pred.append(predicted.cpu())\n",
    "        y_true.append(Y_batch.cpu())\n",
    "        \n",
    "    y_pred, y_true = torch.cat(y_pred), torch.cat(y_true)\n",
    "    loss /= len(loader)\n",
    "    \n",
    "    return loss, (y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "silver-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n"
     ]
    }
   ],
   "source": [
    "test_ds = load_dataset(dataset_dir='../../dataset_DB6', subject=5)\n",
    "ds_loader = DataLoader(test_ds, batch_size=1000, shuffle=False, pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "criminal-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_training_set(dataset_dir='../../dataset_DB6', subject=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "universal-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5841)\n"
     ]
    }
   ],
   "source": [
    "net = load_model(subject=5, training_fold=1)\n",
    "_, (y_pred, y_true) = get_loss_preds(net, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()\n",
    "\n",
    "net = load_model(subject=5, training_fold=2)\n",
    "_, (y_pred, y_true) = get_loss_preds(net, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold2 = (y_pred == y_true).float().mean()\n",
    "\n",
    "print(.5 * (accuracy_fold1 + accuracy_fold2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indian-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.vit = ViT(*args, **kwargs)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # during the convert step, this will be replaced with a\n",
    "        # `quantize_per_tensor` call\n",
    "        x = self.quant(x)\n",
    "        x = self.vit(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wooden-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject, training_fold = 5, 1\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "model_fp32.vit.load_state_dict((torch.load(f\"checkpoints/vit_subject{subject}_fold{training_fold}.pth\")))\n",
    "\n",
    "# create a quantized model instance\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,  # the original model\n",
    "    {torch.nn.Linear, torch.nn.GELU, torch.nn.Softmax, torch.nn.Conv1d, torch.nn.LayerNorm},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "breathing-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-014cc4e12590>\", line 1, in <module>\n",
      "    _, (y_pred, y_true) = get_loss_preds_q(model_int8, nn.CrossEntropyLoss(), ds_loader)\n",
      "NameError: name 'get_loss_preds_q' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-014cc4e12590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loss_preds_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_int8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maccuracy_fold1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_loss_preds_q' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NameError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2064\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1368\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1268\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             )\n\u001b[0;32m   1270\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1125\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "_, (y_pred, y_true) = get_loss_preds_q(model_int8, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_model(subject=5, training_fold=1)\n",
    "_, (y_pred, y_true) = get_loss_preds(net, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "widespread-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject, training_fold = 5, 1\n",
    "model_fp32 = M()\n",
    "sd = torch.load(f\"checkpoints/vit_subject{subject}_fold{training_fold}.pth\")\n",
    "sd['transformer.layers.0.1.fn.net.5.weight'] = sd.pop('transformer.layers.0.1.fn.net.3.weight')\n",
    "sd['transformer.layers.0.1.fn.net.5.bias'] = sd.pop('transformer.layers.0.1.fn.net.3.bias')\n",
    "model_fp32.vit.load_state_dict(sd)\n",
    "#load_model(subject=5, training_fold=1)\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "#model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "qconfig_a_qint8 = torch.quantization.QConfig(\n",
    "    activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8),\n",
    "    weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8),\n",
    ")\n",
    "\n",
    "\n",
    "#model_fp32.vit.qconfig = torch.quantization.default_qconfig\n",
    "#model_fp32.vit.transformer.layers[0][0].fn.to_k.qconfig = qconfig_a_qint8\n",
    "#model_fp32.vit.transformer.layers[0][0].fn.to_v.qconfig = qconfig_a_qint8\n",
    "model_fp32.vit.transformer.layers[0][0].fn.quant_k.qconfig = qconfig_a_qint8\n",
    "model_fp32.vit.transformer.layers[0][0].fn.quant_v.qconfig = qconfig_a_qint8\n",
    "model_fp32.qconfig = torch.quantization.default_qconfig\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "#model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "model_fp32_fused = model_fp32\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "acknowledged-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm(a, b):\n",
    "    r = []\n",
    "    for i in range(a.shape[0]):\n",
    "        r.append(nn.functional.linear(a[i], b[i].T))\n",
    "    return torch.stack(r)\n",
    "\n",
    "train_ds_loader = DataLoader(train_ds.split(total_folds=2, val_fold=0)[0], batch_size=1000, shuffle=False, pin_memory=False, drop_last=False)\n",
    "for X_batch, _ in train_ds_loader:\n",
    "    X_batch = X_batch.to(device)\n",
    "    model_fp32_prepared(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "absent-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = torch.quantization.convert(model_fp32_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "organic-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm(a, b):\n",
    "    r = []\n",
    "    #print(a.type(), b.type())\n",
    "    for i in range(a.shape[0]):\n",
    "        r.append(nn.quantized.functional.linear(a[i], b[i].T))\n",
    "    return torch.stack(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "civil-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y_pred, y_true) = get_loss_preds(model_int8, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "finished-channels",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5324)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "julian-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(7, 100, 12)\n",
    "b = torch.randn(7, 12, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "understanding-homework",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "right-doctor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 100, 100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a @ b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "criminal-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bmm(a, b) != (a @b)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
