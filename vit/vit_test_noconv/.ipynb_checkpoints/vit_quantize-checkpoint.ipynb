{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "voluntary-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.datasets.db6 import DB6MultiSession\n",
    "from pickle import load\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controlled-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "soviet-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "def bmm(a, b):\n",
    "    r = []\n",
    "    for i in range(a.shape[0]):\n",
    "        r.append(nn.functional.linear(a[i], b[i].T))\n",
    "    return torch.stack(r)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self._init_parameters(dim, hidden_dim)\n",
    "\n",
    "    def _init_parameters(self, dim, hidden_dim):\n",
    "        bound1 = 1 / (dim ** .5)\n",
    "        bound2 = 1 / (hidden_dim ** .5)\n",
    "        nn.init.uniform_(self.net[0].weight, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[3].weight, -bound2, bound2)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound2, bound2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.dim_head = dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "        self._init_parameters(dim, inner_dim)\n",
    "\n",
    "    def _init_parameters(self, dim, inner_dim):\n",
    "        bound = 1 / (dim ** .5)\n",
    "        nn.init.uniform_(self.to_q.weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_k.weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_v.weight, -bound, bound)\n",
    "\n",
    "        bound = 1 / (inner_dim ** .5)\n",
    "        nn.init.uniform_(self.to_out[0].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_out[0].bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n",
    "        q = q.reshape(b, n, h, -1).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(b, n, h, -1).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(b, n, h, -1).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #dots = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        k = k.transpose(-2, -1)\n",
    "        dots = []\n",
    "        for i in range(b):\n",
    "            dots.append(bmm(q[i], k[i]))\n",
    "        dots = torch.stack(dots)\n",
    "        \n",
    "        attn = self.attend(dots)\n",
    "        \n",
    "        #out = (attn @ v).transpose(1, 2).reshape(b, n, -1)\n",
    "        out = []\n",
    "        for i in range(b):\n",
    "            out.append(bmm(attn[i], v[i]))\n",
    "        out = torch.stack(out).transpose(1, 2).reshape(b, n, -1)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, window_size=(14, 300), patch_length=10, num_classes=8, dim=64, depth=1, heads=8, mlp_dim=128, pool='cls', dim_head=32, dropout=.2, emb_dropout=0., use_cls_token=True):\n",
    "        super().__init__()\n",
    "\n",
    "        channels, window_length = window_size\n",
    "        num_patches = (window_length // patch_length)\n",
    "        patch_dim = channels * patch_length\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.patch_conv = nn.Conv1d(in_channels=channels, out_channels=dim, kernel_size=patch_length, stride=patch_length, padding=0, bias=True)\n",
    "\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "        self._init_parameters(patch_dim)\n",
    "\n",
    "    def _init_parameters(self, patch_dim):\n",
    "        bound = 1 / (patch_dim ** .5)\n",
    "        nn.init.uniform_(self.patch_conv.weight, -bound, bound)\n",
    "        nn.init.uniform_(self.patch_conv.bias, -bound, bound)\n",
    "        nn.init.zeros_(self.pos_embedding)\n",
    "        nn.init.zeros_(self.mlp_head[1].weight)\n",
    "        nn.init.zeros_(self.mlp_head[1].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_conv(x).flatten(2).transpose(-2, -1)\n",
    "\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        \n",
    "        \"\"\"if self.use_cls_token:\n",
    "            cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efficient-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir, subject):\n",
    "    \n",
    "    all_other_subjects = ','.join([str(s) for s in range(1, 11) if s != subject])\n",
    "    minmax_picklename = f'./minmax/ds_minmax_sessions=5subjects={all_other_subjects}.pickle'\n",
    "    minmax = load(open(minmax_picklename, 'rb'))\n",
    "    \n",
    "    test_ds = DB6MultiSession(folder=os.path.expanduser(dataset_dir), \n",
    "                              subjects=[subject], sessions=list(range(5, 10)), \n",
    "                              minmax=minmax, n_classes='7+1', steady=True).to(device)\n",
    "    \n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arranged-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_set(dataset_dir, subject):\n",
    "    \n",
    "    all_other_subjects = ','.join([str(s) for s in range(1, 11) if s != subject])\n",
    "    minmax_picklename = f'./minmax/ds_minmax_sessions=5subjects={all_other_subjects}.pickle'\n",
    "    minmax = load(open(minmax_picklename, 'rb'))\n",
    "    \n",
    "    ds = DB6MultiSession(folder=os.path.expanduser(dataset_dir), \n",
    "                              subjects=[subject], sessions=list(range(5)), \n",
    "                              minmax=minmax, n_classes='7+1', steady=True).to(device)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "another-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(subject, training_fold):\n",
    "    net = ViT()\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    net.load_state_dict((torch.load(f\"checkpoints/vit_subject{subject}_fold{training_fold}.pth\")))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increased-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_loss_preds(net, criterion, loader):\n",
    "    y_pred, y_true = [], []\n",
    "    loss = 0\n",
    "    for X_batch, Y_batch in loader:\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        \n",
    "        outputs = net(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss += criterion(outputs, Y_batch).item()\n",
    "\n",
    "        y_pred.append(predicted.cpu())\n",
    "        y_true.append(Y_batch.cpu())\n",
    "        \n",
    "    y_pred, y_true = torch.cat(y_pred), torch.cat(y_true)\n",
    "    loss /= len(loader)\n",
    "    \n",
    "    return loss, (y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "silver-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n"
     ]
    }
   ],
   "source": [
    "test_ds = load_dataset(dataset_dir='../../dataset_DB6', subject=5)\n",
    "ds_loader = DataLoader(test_ds, batch_size=1000, shuffle=False, pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "criminal-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n",
      "minmax [-0.00820696 -0.00955554 -0.00625532 -0.0054779  -0.00636441 -0.00635652\n",
      " -0.0047272  -0.0016807  -0.01117341 -0.00731644 -0.00749952 -0.00527726\n",
      " -0.0054686  -0.00693581] [0.00775334 0.00828333 0.0070026  0.0048873  0.00684452 0.00630389\n",
      " 0.0061128  0.00096831 0.01065748 0.0073393  0.00718339 0.00572001\n",
      " 0.0059097  0.00688035]\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_training_set(dataset_dir='../../dataset_DB6', subject=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_model(subject=5, training_fold=1)\n",
    "_, (y_pred, y_true) = get_loss_preds(net, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()\n",
    "\n",
    "net = load_model(subject=5, training_fold=2)\n",
    "_, (y_pred, y_true) = get_loss_preds(net, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold2 = (y_pred == y_true).float().mean()\n",
    "\n",
    "print(.5 * (accuracy_fold1 + accuracy_fold2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "indian-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.vit = ViT(*args, **kwargs)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # during the convert step, this will be replaced with a\n",
    "        # `quantize_per_tensor` call\n",
    "        x = self.quant(x)\n",
    "        x = self.vit(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject, training_fold = 5, 1\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "model_fp32.vit.load_state_dict((torch.load(f\"checkpoints/vit_subject{subject}_fold{training_fold}.pth\")))\n",
    "\n",
    "# create a quantized model instance\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,  # the original model\n",
    "    {torch.nn.Linear, torch.nn.GELU, torch.nn.Softmax, torch.nn.Conv1d, torch.nn.LayerNorm},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y_pred, y_true) = get_loss_preds_q(model_int8, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_model(subject=5, training_fold=1)\n",
    "_, (y_pred, y_true) = get_loss_preds(net, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "widespread-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject, training_fold = 5, 1\n",
    "model_fp32 = M()\n",
    "model_fp32.vit.load_state_dict((torch.load(f\"checkpoints/vit_subject{subject}_fold{training_fold}.pth\")))\n",
    "#load_model(subject=5, training_fold=1)\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "#model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "qconfig_a_qint8 = torch.quantization.QConfig(\n",
    "    activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8),\n",
    "    weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8),\n",
    ")\n",
    "\n",
    "\n",
    "#model_fp32.vit.qconfig = torch.quantization.default_qconfig\n",
    "model_fp32.vit.transformer.layers[0][0].fn.to_k.qconfig = qconfig_a_qint8\n",
    "model_fp32.vit.transformer.layers[0][0].fn.to_v.qconfig = qconfig_a_qint8\n",
    "model_fp32.qconfig = torch.quantization.default_qconfig\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "#model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "model_fp32_fused = model_fp32\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "acknowledged-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm(a, b):\n",
    "    r = []\n",
    "    for i in range(a.shape[0]):\n",
    "        r.append(nn.functional.linear(a[i], b[i].T))\n",
    "    return torch.stack(r)\n",
    "\n",
    "train_ds_loader = DataLoader(train_ds.split(total_folds=2, val_fold=0)[0], batch_size=1000, shuffle=False, pin_memory=False, drop_last=False)\n",
    "for X_batch, _ in train_ds_loader:\n",
    "    X_batch = X_batch.to(device)\n",
    "    model_fp32_prepared(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "absent-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = torch.quantization.convert(model_fp32_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "organic-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm(a, b):\n",
    "    r = []\n",
    "    print(a.type(), b.type())\n",
    "    for i in range(a.shape[0]):\n",
    "        r.append(nn.quantized.functional.linear(a[i], b[i].T))\n",
    "    return torch.stack(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "civil-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.quantized.QUInt8Tensor torch.quantized.QUInt8Tensor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type QInt8 but found QUInt8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-8655562ab703>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loss_preds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_int8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maccuracy_fold1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-df00784aa82a>\u001b[0m in \u001b[0;36mget_loss_preds\u001b[1;34m(net, criterion, loader)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-bdff22027195>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# `quantize_per_tensor` call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdequant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-2ba5e96a1970>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-2ba5e96a1970>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mff\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-2ba5e96a1970>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-2ba5e96a1970>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mdots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mdots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mdots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdots\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-b87fd8703ad9>\u001b[0m in \u001b[0;36mbmm\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\quantized\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias, scale, zero_point)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mzero_point\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mzero_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_zero_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0m_packed_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_prepack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_packed_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type QInt8 but found QUInt8"
     ]
    }
   ],
   "source": [
    "_, (y_pred, y_true) = get_loss_preds(model_int8, nn.CrossEntropyLoss(), ds_loader)\n",
    "accuracy_fold1 = (y_pred == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "julian-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(7, 100, 12)\n",
    "b = torch.randn(7, 12, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "understanding-homework",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "right-doctor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 100, 100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a @ b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "criminal-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bmm(a, b) != (a @b)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
