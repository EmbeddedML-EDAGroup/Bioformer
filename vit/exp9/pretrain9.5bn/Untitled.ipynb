{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "destroyed-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "        #return self.norm(self.fn(x, **kwargs))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        #nn.init.xavier_uniform_(self.net[0].weight, gain=2 ** .5)\n",
    "        #nn.init.normal_(self.net[0].bias, std=1e-6)\n",
    "        #nn.init.xavier_uniform_(self.net[3].weight, gain=2 ** .5)\n",
    "        #nn.init.normal_(self.net[3].bias, std=1e-6)\n",
    "        #nn.init.kaiming_uniform_(self.net[0].weight, a=5**.5)\n",
    "        #nn.init.kaiming_uniform_(self.net[0].weight, a=5**.5)\n",
    "        bound1 = 1 / (dim ** .5)\n",
    "        bound2 = 1 / (hidden_dim ** .5)\n",
    "        nn.init.uniform_(self.net[0].weight, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[3].weight, -bound2, bound2)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound2, bound2)\n",
    "\n",
    "        #nn.init.xavier_normal_(self.net[0].weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.net[0].bias, std=.1)\n",
    "        #nn.init.xavier_normal_(self.net[3].weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.net[3].bias, std=.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "               \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        #nn.init.xavier_uniform_(self.to_qkv.weight)\n",
    "        bound = 1 / (dim ** .5)\n",
    "        nn.init.uniform_(self.to_qkv.weight, -bound, bound)\n",
    "        #nn.init.xavier_normal_(self.to_qkv.weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.to_qkv.weight, std=1)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "        #nn.init.xavier_uniform_(self.to_out[0].weight)\n",
    "        #nn.init.normal_(self.to_out[0].bias, std=1e-6)\n",
    "        bound = 1 / (inner_dim ** .5)\n",
    "        nn.init.uniform_(self.to_out[0].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_out[0].bias, -bound, bound)\n",
    "\n",
    "        #nn.init.zeros_(self.to_out[0].weight)\n",
    "        #nn.init.zeros_(self.to_out[0].bias)\n",
    "\n",
    "        #nn.init.xavier_normal_(self.to_out[0].weight, gain=1)\n",
    "        #nn.init.normal_(self.to_out[0].bias, std=.05)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = qkv.chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3,\n",
    "                 dim_head = 64, dropout = 0., emb_dropout = 0., use_cls_token=True, \n",
    "                 sessions=\"ignore\", subjects=\"ignore\", training_config=\"ignore\", pretrained=\"ignore\", chunk_idx=\"ignore\", chunk_i=\"ignore\"):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        \n",
    "        self.patch_bm = nn.BatchNorm1d(30) #nn.LayerNorm(dim)\n",
    "        \n",
    "        bound = 1 / (patch_dim ** .5)\n",
    "        nn.init.uniform_(self.to_patch_embedding[1].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_patch_embedding[1].bias, -bound, bound)\n",
    "        #nn.init.xavier_uniform_(self.to_patch_embedding[1].weight)\n",
    "        #nn.init.normal_(self.to_patch_embedding[1].bias, std=1e-6)\n",
    "        #nn.init.xavier_normal_(self.to_patch_embedding[1].weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.to_patch_embedding[1].bias, std=.1)\n",
    "\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "        nn.init.normal_(self.pos_embedding, mean=0, std=.02)\n",
    "        #nn.init.zeros_(self.pos_embedding)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.zeros_(self.cls_token)\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        #nn.init.zeros_(self.mlp_head[1].weight)\n",
    "        #nn.init.zeros_(self.mlp_head[1].bias)\n",
    "        bound = 1 / (dim ** .5)\n",
    "        nn.init.uniform_(self.mlp_head[1].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.mlp_head[1].bias, -bound, bound)\n",
    "        #nn.init.xavier_normal_(self.mlp_head[1].weight, gain=1)\n",
    "        #nn.init.normal_(self.mlp_head[1].weight, std=1)\n",
    "        #nn.init.normal_(self.mlp_head[1].bias, std=1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        x = self.patch_bm(x)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        \n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "parliamentary-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(**{\n",
    "    \"image_size\": (1, 300),\n",
    "    \"patch_size\": (1, 10),\n",
    "    \"channels\": 14,\n",
    "    \"num_classes\": 8,\n",
    "\n",
    "    \"dim\": 64,\n",
    "    \"mlp_dim\": 128,\n",
    "    \"dim_head\": 32,\n",
    "    \"heads\": 8,\n",
    "    \"depth\": 1,\n",
    "\n",
    "    \"dropout\": .2,\n",
    "    \"emb_dropout\": 0,\n",
    "\n",
    "    \"pool\": \"cls\",\n",
    "    \"use_cls_token\": True,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "earned-salmon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.9372e-01,  8.6514e-01,  9.7646e-01, -8.9616e-01, -3.5573e-01,\n",
       "         -8.8875e-01, -3.2140e-01,  4.6889e-01],\n",
       "        [-3.2742e-01,  1.1492e+00,  1.9913e-01, -2.7730e-01, -3.7519e-01,\n",
       "          7.8473e-01,  5.3398e-02,  4.7396e-01],\n",
       "        [ 6.7230e-01,  7.6946e-01, -1.7881e-01, -4.3213e-01,  3.7925e-01,\n",
       "         -4.5986e-01, -8.4310e-01,  1.7550e-01],\n",
       "        [ 5.4021e-02, -4.4312e-01,  8.2933e-01,  5.1700e-02,  6.2461e-01,\n",
       "          9.5278e-01, -2.5473e-01, -4.4725e-01],\n",
       "        [-2.2568e-02, -1.0745e-02,  1.3519e+00, -2.5202e-02,  2.9431e-01,\n",
       "          1.0316e+00, -8.7900e-02, -3.5861e-01],\n",
       "        [ 1.7806e-01, -1.1416e+00,  3.5923e-01,  1.9370e-02,  4.3004e-02,\n",
       "          2.3341e-01,  8.1667e-02,  7.4519e-01],\n",
       "        [ 1.0565e-01,  2.0965e+00,  6.2932e-01, -9.2423e-01, -3.6290e-01,\n",
       "         -1.4173e-01, -7.8540e-02,  9.1168e-02],\n",
       "        [-5.7537e-01,  2.2057e-01, -2.2350e-01, -5.4568e-01,  1.1313e+00,\n",
       "          3.0798e-01, -5.0200e-02,  6.0989e-01],\n",
       "        [ 2.2036e-01,  1.2801e-01, -5.1369e-01,  9.0983e-01, -6.3721e-01,\n",
       "          1.3037e-01,  3.6913e-01,  7.3246e-01],\n",
       "        [-1.2626e-01, -7.7249e-01, -1.1973e-01,  6.6726e-01,  5.7232e-01,\n",
       "          1.0577e+00, -1.6387e-02, -8.9825e-01],\n",
       "        [ 6.8396e-01,  3.0307e-01, -1.3151e-01,  2.9861e-01,  8.3044e-02,\n",
       "          4.7135e-01, -5.4151e-02,  2.8151e-01],\n",
       "        [ 4.6762e-01,  2.4443e-01,  8.8621e-01,  3.0393e-02, -1.0684e+00,\n",
       "         -2.2339e-01, -1.8052e-01,  2.5589e-01],\n",
       "        [-6.9101e-01,  1.6848e-01, -2.4016e-01,  5.0307e-01,  4.7109e-01,\n",
       "          8.6463e-02, -5.1738e-02, -6.5556e-01],\n",
       "        [ 5.4143e-01,  4.7372e-01, -3.6086e-01, -6.9857e-02,  9.8672e-02,\n",
       "          4.4856e-01,  1.3209e+00,  2.4925e-02],\n",
       "        [ 1.7823e-01,  7.4437e-01,  3.1302e-01,  6.8726e-01, -1.6561e-01,\n",
       "          7.3103e-01, -1.0721e-01, -1.0170e+00],\n",
       "        [ 9.5667e-01,  1.0324e+00, -3.4296e-01,  4.4011e-01, -3.6669e-01,\n",
       "          3.7534e-01, -1.7745e-01, -1.0354e-01],\n",
       "        [ 3.1760e-01,  4.1144e-02, -1.1705e+00, -4.5346e-02, -3.1077e-01,\n",
       "          7.4596e-01,  8.8262e-01, -2.3139e-01],\n",
       "        [-6.8006e-01,  1.1305e-01,  5.4775e-01,  1.0248e-01, -6.6788e-01,\n",
       "          3.6198e-01, -1.0109e+00, -1.7567e-01],\n",
       "        [ 4.7938e-01, -2.3736e-01, -7.4358e-04, -4.0543e-01, -1.3306e+00,\n",
       "         -2.5488e-02, -3.1669e-01,  3.1691e-02],\n",
       "        [ 5.1848e-01,  1.2104e-01,  7.6334e-02, -9.2225e-02, -6.0178e-01,\n",
       "          1.2797e-01,  7.4244e-01,  1.3606e+00],\n",
       "        [ 5.4916e-01,  5.2513e-01, -9.2131e-01, -1.1468e+00, -8.7907e-01,\n",
       "         -5.3801e-01,  2.0295e-01,  1.3624e+00],\n",
       "        [ 1.2182e-01,  1.3097e-01,  4.6616e-02,  1.0229e+00,  7.4329e-01,\n",
       "         -2.9558e-02,  4.8169e-02, -3.2439e-02],\n",
       "        [ 2.2898e-01,  9.7896e-01,  6.7958e-01, -3.2594e-01, -2.2607e-01,\n",
       "          6.4597e-01,  1.0640e+00, -3.7642e-02],\n",
       "        [ 1.7596e-01, -5.4693e-01, -5.1330e-01, -9.5298e-01,  9.7225e-01,\n",
       "         -1.2703e-01,  6.5370e-01,  1.2626e-01],\n",
       "        [ 6.8225e-01,  1.2267e-01,  1.0904e+00, -3.8302e-01, -9.1196e-01,\n",
       "          1.1160e-01,  3.4609e-01, -4.6281e-01],\n",
       "        [ 7.0115e-01,  1.5572e-01, -8.0447e-01,  1.4395e-01,  9.8547e-03,\n",
       "          4.0391e-01,  2.3218e-01, -1.6574e-01],\n",
       "        [-2.5151e-01, -3.0019e-01,  4.2647e-01, -5.9516e-01,  2.5397e-01,\n",
       "          3.2554e-01, -5.9826e-01, -3.1180e-01],\n",
       "        [ 8.0479e-01,  8.1137e-01,  6.8679e-01, -4.5352e-01,  2.5125e-01,\n",
       "         -9.0492e-02, -7.3338e-02,  2.3510e-01],\n",
       "        [ 8.5409e-01,  6.2466e-01,  1.0245e+00,  3.6848e-01, -9.9842e-04,\n",
       "          1.7996e-01, -6.1924e-01,  6.1262e-02],\n",
       "        [ 7.4932e-01,  5.1909e-01, -1.7847e-01,  2.1551e-01, -2.1325e-01,\n",
       "          6.6568e-01, -4.9173e-01,  1.0302e-01],\n",
       "        [-2.7498e-01,  1.1397e+00, -1.8317e-01, -4.7032e-01, -4.5430e-01,\n",
       "          6.4677e-01,  5.8551e-01, -4.6283e-02],\n",
       "        [ 7.4630e-01,  3.2746e-01, -9.7365e-01, -1.7805e-01,  6.5342e-01,\n",
       "          6.2765e-01,  7.4897e-01,  4.1201e-01],\n",
       "        [ 8.0356e-01,  7.3364e-01, -8.9706e-02, -2.6750e-01, -7.3799e-01,\n",
       "         -3.8824e-01,  7.2925e-01,  1.1921e-01],\n",
       "        [ 7.6030e-01,  1.2118e+00,  1.5737e-01, -4.5524e-01,  3.8005e-01,\n",
       "          9.3348e-02, -1.6022e-02,  8.2019e-01],\n",
       "        [ 4.8507e-01,  4.6772e-01,  1.7278e-01, -4.0934e-01, -6.7531e-01,\n",
       "         -6.6633e-01, -6.5431e-03,  1.4559e-01],\n",
       "        [ 7.1409e-01, -2.0766e-01, -1.2330e+00, -1.3151e-01, -2.7165e-02,\n",
       "         -5.9724e-01, -7.0777e-01,  1.4153e-01],\n",
       "        [ 3.0845e-01, -1.7846e-01, -1.0709e+00, -3.6738e-01,  4.9731e-02,\n",
       "          6.8810e-01,  3.6883e-01,  1.9926e-01],\n",
       "        [ 1.0908e+00,  1.1959e+00, -1.3463e-01,  4.5617e-02, -5.3898e-01,\n",
       "         -7.7509e-01, -8.0062e-01,  4.5145e-01],\n",
       "        [-1.2157e-01, -7.3188e-02, -1.0589e-03,  3.8726e-01,  7.9100e-01,\n",
       "          7.6971e-01,  2.8350e-01,  5.2139e-01],\n",
       "        [ 1.8674e-01,  9.0214e-01, -2.8934e-01, -9.0631e-01, -5.6433e-01,\n",
       "          3.8219e-01,  1.1240e+00,  6.3122e-01],\n",
       "        [ 9.8514e-01,  8.2851e-01,  1.2422e-01, -1.0376e+00, -5.0979e-01,\n",
       "          1.2701e-01,  4.1999e-01,  3.1424e-01],\n",
       "        [ 9.0964e-02,  7.5702e-01,  1.4194e-01, -1.2972e+00, -4.9658e-01,\n",
       "         -6.0605e-01,  3.2404e-01, -5.3623e-03],\n",
       "        [ 5.8295e-02,  1.1857e-01,  9.0354e-01, -5.3015e-01,  4.5276e-01,\n",
       "         -3.1187e-01,  4.8274e-01,  5.3412e-01],\n",
       "        [ 7.0093e-02,  1.2246e+00,  6.6695e-01, -1.9375e-01,  2.6066e-01,\n",
       "         -1.1457e-01, -6.0777e-01, -5.5374e-01],\n",
       "        [-6.4036e-01, -1.1560e+00, -3.6230e-02,  2.2966e-01,  3.9756e-01,\n",
       "          4.9271e-01,  1.8860e-01,  6.0760e-01],\n",
       "        [ 7.1023e-02,  2.7394e-01, -4.0901e-01,  3.2743e-01,  1.8199e-01,\n",
       "         -3.4354e-01, -2.1297e-01, -2.4006e-01],\n",
       "        [-1.5687e-01,  6.8061e-01,  3.7150e-01, -7.9519e-01, -8.9327e-02,\n",
       "          2.0334e-01, -8.0921e-02,  3.2144e-02],\n",
       "        [ 4.7663e-02,  1.2701e+00, -2.9455e-01, -6.3844e-01,  5.1869e-01,\n",
       "         -1.0345e+00, -4.9368e-01,  7.5796e-01],\n",
       "        [-5.9530e-01,  9.7806e-01,  1.1721e+00, -8.8721e-01,  7.9365e-02,\n",
       "          9.7884e-02,  1.1053e+00, -2.4669e-01],\n",
       "        [ 2.6027e-01, -5.5765e-02,  7.3602e-01, -8.6173e-01,  2.7088e-01,\n",
       "          7.7818e-01, -1.4085e-01, -2.5552e-01],\n",
       "        [-2.8279e-01, -1.3510e-01,  4.2498e-02, -1.7116e-01, -7.4676e-01,\n",
       "          8.7311e-01, -4.4444e-01, -6.9560e-01],\n",
       "        [ 2.5351e-01,  1.1766e-01, -1.7033e-01, -7.6442e-01, -2.5853e-01,\n",
       "          3.3707e-01,  1.0534e-01,  1.2340e+00],\n",
       "        [ 7.2589e-01, -8.0366e-01, -6.5965e-01, -3.1346e-01,  2.9555e-01,\n",
       "          7.0838e-01, -7.0902e-01, -7.7425e-02],\n",
       "        [-1.4814e-01, -1.5875e-01,  1.4368e-01, -2.5951e-01,  7.8362e-01,\n",
       "          4.5735e-02,  4.5474e-01, -3.3722e-02],\n",
       "        [-9.5696e-01,  5.5434e-01, -2.0200e-01, -8.6051e-01,  3.5006e-01,\n",
       "          1.3816e+00,  1.0574e+00, -2.0593e-01],\n",
       "        [-1.4838e-02, -5.9490e-01,  8.9927e-01, -1.8391e-01, -4.6132e-01,\n",
       "          1.0454e+00,  1.0223e+00, -8.7287e-01],\n",
       "        [ 6.3613e-01,  7.7013e-01, -3.6500e-02, -8.0804e-01, -6.4684e-02,\n",
       "         -3.8638e-01,  1.1290e-01, -1.4911e-01],\n",
       "        [ 3.3814e-01,  4.9402e-01, -2.8108e-01,  1.0751e-01,  9.3595e-04,\n",
       "          7.7314e-01, -3.0525e-01,  5.6547e-01],\n",
       "        [ 5.0577e-01,  5.2621e-01,  2.3416e-01, -1.2771e+00, -7.1410e-02,\n",
       "         -4.5737e-01, -1.0566e-01,  3.5793e-01],\n",
       "        [ 2.3747e-01, -2.3792e-03,  1.1158e+00, -2.0891e-01, -2.0788e-01,\n",
       "          6.4779e-02,  1.0982e+00,  2.9874e-01],\n",
       "        [-1.9949e-01, -3.0040e-01, -6.2981e-01, -7.7197e-01, -6.8629e-01,\n",
       "          6.4516e-02,  4.5480e-01, -7.3619e-02],\n",
       "        [ 3.4381e-01,  2.2783e-01,  2.6625e-01, -1.9481e-02, -4.2128e-01,\n",
       "          7.0807e-01,  6.8977e-01,  5.0460e-02],\n",
       "        [ 3.2060e-01,  7.0670e-01,  6.3052e-01,  4.5978e-01,  2.7357e-01,\n",
       "         -7.1323e-01, -6.2096e-01,  2.9422e-01],\n",
       "        [ 8.6780e-01,  4.3158e-01, -2.9202e-01,  2.8279e-01, -1.2437e+00,\n",
       "          9.8379e-01,  1.2098e+00,  4.5282e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit(torch.randn(64, 14, 1, 300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
