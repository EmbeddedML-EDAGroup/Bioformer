{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "destroyed-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "        #return self.norm(self.fn(x, **kwargs))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        #nn.init.xavier_uniform_(self.net[0].weight, gain=2 ** .5)\n",
    "        #nn.init.normal_(self.net[0].bias, std=1e-6)\n",
    "        #nn.init.xavier_uniform_(self.net[3].weight, gain=2 ** .5)\n",
    "        #nn.init.normal_(self.net[3].bias, std=1e-6)\n",
    "        #nn.init.kaiming_uniform_(self.net[0].weight, a=5**.5)\n",
    "        #nn.init.kaiming_uniform_(self.net[0].weight, a=5**.5)\n",
    "        bound1 = 1 / (dim ** .5)\n",
    "        bound2 = 1 / (hidden_dim ** .5)\n",
    "        nn.init.uniform_(self.net[0].weight, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound1, bound1)\n",
    "        nn.init.uniform_(self.net[3].weight, -bound2, bound2)\n",
    "        nn.init.uniform_(self.net[0].bias, -bound2, bound2)\n",
    "\n",
    "        #nn.init.xavier_normal_(self.net[0].weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.net[0].bias, std=.1)\n",
    "        #nn.init.xavier_normal_(self.net[3].weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.net[3].bias, std=.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "               \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        #nn.init.xavier_uniform_(self.to_qkv.weight)\n",
    "        bound = 1 / (dim ** .5)\n",
    "        nn.init.uniform_(self.to_qkv.weight, -bound, bound)\n",
    "        #nn.init.xavier_normal_(self.to_qkv.weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.to_qkv.weight, std=1)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "        #nn.init.xavier_uniform_(self.to_out[0].weight)\n",
    "        #nn.init.normal_(self.to_out[0].bias, std=1e-6)\n",
    "        bound = 1 / (inner_dim ** .5)\n",
    "        nn.init.uniform_(self.to_out[0].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_out[0].bias, -bound, bound)\n",
    "\n",
    "        #nn.init.zeros_(self.to_out[0].weight)\n",
    "        #nn.init.zeros_(self.to_out[0].bias)\n",
    "\n",
    "        #nn.init.xavier_normal_(self.to_out[0].weight, gain=1)\n",
    "        #nn.init.normal_(self.to_out[0].bias, std=.05)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = qkv.chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3,\n",
    "                 dim_head = 64, dropout = 0., emb_dropout = 0., use_cls_token=True, \n",
    "                 sessions=\"ignore\", subjects=\"ignore\", training_config=\"ignore\", pretrained=\"ignore\", chunk_idx=\"ignore\", chunk_i=\"ignore\"):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        \n",
    "        self.patch_layer_norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        bound = 1 / (patch_dim ** .5)\n",
    "        nn.init.uniform_(self.to_patch_embedding[1].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.to_patch_embedding[1].bias, -bound, bound)\n",
    "        #nn.init.xavier_uniform_(self.to_patch_embedding[1].weight)\n",
    "        #nn.init.normal_(self.to_patch_embedding[1].bias, std=1e-6)\n",
    "        #nn.init.xavier_normal_(self.to_patch_embedding[1].weight, gain=2**-.5)\n",
    "        #nn.init.normal_(self.to_patch_embedding[1].bias, std=.1)\n",
    "\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "        nn.init.normal_(self.pos_embedding, mean=0, std=.02)\n",
    "        #nn.init.zeros_(self.pos_embedding)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.zeros_(self.cls_token)\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        #nn.init.zeros_(self.mlp_head[1].weight)\n",
    "        #nn.init.zeros_(self.mlp_head[1].bias)\n",
    "        bound = 1 / (dim ** .5)\n",
    "        nn.init.uniform_(self.mlp_head[1].weight, -bound, bound)\n",
    "        nn.init.uniform_(self.mlp_head[1].bias, -bound, bound)\n",
    "        #nn.init.xavier_normal_(self.mlp_head[1].weight, gain=1)\n",
    "        #nn.init.normal_(self.mlp_head[1].weight, std=1)\n",
    "        #nn.init.normal_(self.mlp_head[1].bias, std=1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        x = self.patch_layer_norm(x)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        \n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parliamentary-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(**{\n",
    "    \"image_size\": (1, 300),\n",
    "    \"patch_size\": (1, 10),\n",
    "    \"channels\": 14,\n",
    "    \"num_classes\": 8,\n",
    "\n",
    "    \"dim\": 64,\n",
    "    \"mlp_dim\": 128,\n",
    "    \"dim_head\": 32,\n",
    "    \"heads\": 8,\n",
    "    \"depth\": 1,\n",
    "\n",
    "    \"dropout\": .2,\n",
    "    \"emb_dropout\": 0,\n",
    "\n",
    "    \"pool\": \"cls\",\n",
    "    \"use_cls_token\": True,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "earned-salmon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3563e-01, -1.1381e+00,  1.0509e-02, -3.2930e-02, -3.7114e-01,\n",
       "         -6.2313e-01, -1.3273e+00, -7.1051e-01],\n",
       "        [ 1.0506e-01,  2.9779e-01,  4.1018e-01, -7.8107e-01, -1.6058e-01,\n",
       "         -8.8037e-01, -7.0013e-01, -3.7741e-01],\n",
       "        [-8.5659e-01,  1.2082e-01,  1.4838e+00, -5.1904e-01, -1.1494e+00,\n",
       "         -6.2442e-01, -1.3055e-01, -1.1003e-03],\n",
       "        [-6.8632e-01,  2.4830e-02,  7.6173e-01, -1.5873e-01, -8.0623e-01,\n",
       "         -5.0671e-01,  3.1818e-01, -1.8746e-01],\n",
       "        [-6.8394e-01, -5.6709e-01,  1.2917e+00,  7.0438e-02, -9.2544e-01,\n",
       "         -7.3106e-01, -6.7923e-01, -5.5209e-01],\n",
       "        [-4.3025e-01,  1.3769e-01,  8.6306e-01, -5.4819e-01, -9.7765e-01,\n",
       "         -4.2731e-01,  5.4673e-01, -5.6188e-02],\n",
       "        [-6.4496e-01, -7.4136e-01, -3.2990e-01,  1.5065e-01,  2.2774e-01,\n",
       "         -8.6682e-01, -2.4947e-01, -4.9481e-01],\n",
       "        [ 1.0405e-01,  1.7261e-02, -4.9970e-01,  7.2296e-01, -4.0426e-01,\n",
       "         -2.3404e-01,  3.1308e-01, -1.0378e+00],\n",
       "        [-8.9360e-02,  3.2300e-01,  1.0374e+00,  6.6767e-01, -7.2989e-01,\n",
       "         -6.7961e-01,  2.2491e-01, -9.3224e-01],\n",
       "        [-7.2653e-01, -5.3608e-01,  5.5423e-01, -1.7835e-01, -6.1196e-01,\n",
       "         -1.7387e-01,  4.6871e-02, -4.5637e-01],\n",
       "        [-9.6108e-01, -8.0056e-01,  5.1331e-01, -2.7735e-01, -7.9889e-01,\n",
       "         -2.0061e-01, -5.5227e-01, -8.6905e-01],\n",
       "        [ 2.6180e-01,  3.9492e-01,  5.2757e-01,  2.2486e-02, -8.4521e-01,\n",
       "         -1.6357e-01,  1.1035e+00, -4.1818e-01],\n",
       "        [-8.1846e-01, -9.4156e-02, -4.7018e-01, -4.6781e-01,  3.8687e-01,\n",
       "          5.8546e-01, -2.5686e-01, -3.4603e-01],\n",
       "        [-7.8692e-01,  5.1445e-01,  5.9822e-01,  1.4509e+00,  5.4837e-01,\n",
       "          7.1498e-01,  8.3560e-01,  1.7542e-01],\n",
       "        [-9.4038e-02, -6.7038e-01,  4.7870e-01,  8.3332e-03, -4.6719e-01,\n",
       "         -9.6203e-01, -6.6991e-01, -6.2020e-01],\n",
       "        [-2.5275e-01, -7.1560e-01,  1.6868e-01, -7.8432e-01, -4.9152e-01,\n",
       "         -3.8691e-01,  4.6596e-01, -5.1042e-01],\n",
       "        [-1.0986e+00,  1.3600e-01,  7.0434e-01,  4.2286e-01, -4.6483e-01,\n",
       "         -4.4686e-01, -3.0105e-02, -6.0258e-01],\n",
       "        [-4.4967e-01, -5.1651e-01, -6.3808e-01, -1.0997e+00, -2.5160e-01,\n",
       "         -8.1203e-02, -1.3286e+00, -4.5378e-01],\n",
       "        [-4.8764e-01,  1.4983e-02,  7.6369e-01,  2.8633e-01, -8.7836e-01,\n",
       "         -7.4621e-01,  9.7497e-02, -1.5491e-01],\n",
       "        [-9.4804e-01, -1.8263e-01,  6.2356e-01,  4.8996e-02,  4.2335e-02,\n",
       "         -1.0282e+00, -1.0003e+00, -8.2814e-01],\n",
       "        [-2.7888e-01,  3.2800e-01, -6.8585e-02,  1.6926e-03,  3.0137e-01,\n",
       "         -4.8172e-01, -1.2665e+00, -7.4994e-01],\n",
       "        [-4.8568e-01,  8.1635e-01, -7.7754e-01,  4.2132e-01, -2.8798e-01,\n",
       "          2.7691e-01, -2.5817e-01, -4.5490e-01],\n",
       "        [-8.0098e-01,  1.9333e-01,  1.1164e+00,  1.0292e+00, -7.5362e-01,\n",
       "         -1.1469e+00,  3.6709e-02, -4.6272e-01],\n",
       "        [-4.0488e-01, -7.7606e-01,  9.4244e-01,  4.9746e-01, -2.1945e-01,\n",
       "         -6.1818e-01, -3.4060e-01, -4.4877e-01],\n",
       "        [ 5.6683e-01,  4.1946e-01,  6.0280e-02,  7.1280e-01,  6.1195e-02,\n",
       "         -9.5724e-02, -7.5412e-02, -4.1987e-01],\n",
       "        [-6.2733e-01,  2.4175e-01,  9.7350e-02, -6.8710e-01, -1.4541e-01,\n",
       "          9.0885e-01,  2.4418e-01, -2.5098e-01],\n",
       "        [-4.3281e-01, -4.3525e-01, -7.1926e-02, -3.5944e-01, -9.1119e-01,\n",
       "         -4.5715e-01, -9.2454e-01, -5.1058e-01],\n",
       "        [-4.1357e-01,  1.4466e-01,  5.7224e-01, -3.0464e-01, -5.3238e-01,\n",
       "         -6.1519e-01, -1.3766e-01,  2.2193e-01],\n",
       "        [ 4.6814e-01, -5.4662e-02,  2.9103e-01,  1.2102e-02,  8.0364e-01,\n",
       "          2.0497e-01, -7.7852e-01,  1.3821e-01],\n",
       "        [ 8.0625e-01,  1.4446e-01, -1.6936e-01, -1.3315e+00,  7.4666e-01,\n",
       "          8.8771e-03, -1.0558e+00, -3.3917e-01],\n",
       "        [-6.8975e-01, -4.3472e-01,  6.8351e-01, -3.8083e-01, -3.2528e-01,\n",
       "          3.1313e-01, -8.3205e-01, -2.0441e-01],\n",
       "        [ 1.8963e-01,  1.7807e-01,  5.6951e-01, -3.9667e-02, -8.9234e-01,\n",
       "          1.1469e-01,  7.1563e-02, -1.0390e+00],\n",
       "        [-9.8510e-01, -3.5277e-01,  2.9844e-01,  2.3082e-01, -6.1765e-01,\n",
       "         -3.5916e-01, -1.0920e-01,  2.5539e-01],\n",
       "        [-1.9102e-01,  4.1256e-01,  2.0379e-01,  4.2221e-02,  1.5042e-01,\n",
       "         -5.2644e-01, -3.6787e-01,  1.5670e-01],\n",
       "        [-8.6659e-01, -6.8089e-01,  2.6467e-01,  1.4377e-01, -3.4769e-01,\n",
       "         -3.5583e-01,  2.6879e-01, -1.2074e-02],\n",
       "        [-1.0169e-01, -3.5976e-01,  1.0595e+00,  1.3629e-01, -5.0213e-01,\n",
       "          1.5917e-02,  1.4503e-01, -4.6158e-01],\n",
       "        [-4.3741e-01, -6.2431e-01,  1.6959e-01, -2.1360e-01, -7.1827e-01,\n",
       "         -5.3285e-01, -9.0281e-01, -1.2980e+00],\n",
       "        [-4.4456e-01,  7.2412e-01, -7.3598e-01,  8.6750e-01,  8.0089e-02,\n",
       "         -2.1274e-01, -2.4251e-01, -2.8355e-01],\n",
       "        [-1.2929e-01,  2.7970e-01,  1.6364e-01, -3.9597e-01, -3.2640e-01,\n",
       "         -1.5360e+00, -4.7839e-01, -3.9224e-01],\n",
       "        [ 5.8452e-01, -1.1138e+00,  4.0979e-01, -5.0053e-01, -1.3892e-01,\n",
       "          1.0434e-01,  1.2920e-01, -2.4482e-01],\n",
       "        [-5.9368e-01,  3.5374e-01,  1.6254e-01, -5.0952e-01, -1.2665e-01,\n",
       "          3.1313e-01,  4.4470e-01,  2.0028e-02],\n",
       "        [-6.1871e-03, -5.9022e-01,  1.3456e+00,  1.3476e-02, -6.7915e-01,\n",
       "         -6.9878e-01,  5.2455e-01,  1.0688e-01],\n",
       "        [-2.3382e-01, -7.0262e-01,  8.0503e-01, -1.7473e-01, -8.4009e-01,\n",
       "         -2.7759e-01,  2.5696e-01, -3.3019e-01],\n",
       "        [ 4.6025e-01, -9.2927e-01, -1.3837e-01, -6.4387e-01,  3.1615e-01,\n",
       "         -2.1607e-01, -8.9903e-02,  2.7446e-01],\n",
       "        [ 8.8949e-02,  7.5148e-01, -1.1759e-01,  3.6670e-01,  7.6744e-02,\n",
       "         -4.0391e-01, -1.6666e+00, -6.6741e-01],\n",
       "        [-1.3814e-01, -7.5881e-01, -2.5107e-01, -4.7053e-01,  2.2195e-01,\n",
       "         -3.5612e-01, -6.8623e-01, -2.1143e-01],\n",
       "        [-1.1184e-01,  5.0481e-01,  9.9551e-01, -3.5346e-01, -3.3386e-01,\n",
       "          1.0753e+00,  1.7496e-01, -1.6828e-01],\n",
       "        [ 4.6441e-01,  4.2145e-02, -5.2684e-01,  4.7538e-01,  7.8515e-01,\n",
       "          8.0313e-01, -6.4507e-01, -7.4413e-01],\n",
       "        [-1.5390e-01, -5.1800e-01,  5.0241e-01,  2.8889e-01, -1.1323e+00,\n",
       "         -4.0940e-01,  3.4084e-01,  4.1062e-01],\n",
       "        [-1.6203e-01, -5.2960e-01,  8.8638e-01,  3.3090e-01, -1.7079e-01,\n",
       "         -3.0102e-01, -6.0372e-01, -5.0436e-01],\n",
       "        [-9.2495e-01, -2.7452e-01,  5.6391e-01, -1.2434e-01, -1.1456e+00,\n",
       "         -1.0746e+00, -3.8774e-01,  5.2984e-01],\n",
       "        [ 1.6434e-01,  4.1219e-02,  6.2078e-01, -3.4719e-01,  2.1532e-01,\n",
       "          6.5762e-01,  1.4921e-01, -7.8371e-01],\n",
       "        [-2.5897e-01,  6.3741e-03, -3.2225e-01, -2.4007e-01, -3.2727e-01,\n",
       "          4.1270e-02, -5.2233e-01, -4.2319e-01],\n",
       "        [-2.6255e-01, -2.6508e-01,  7.7190e-02,  6.4349e-02,  4.9822e-02,\n",
       "         -4.3262e-01, -2.3664e-01, -2.2563e-01],\n",
       "        [-7.0859e-02, -1.7746e-01,  7.1311e-01,  1.0653e-01, -9.6109e-02,\n",
       "         -1.1598e+00, -1.2858e+00, -2.6315e-01],\n",
       "        [ 4.1394e-01,  4.9809e-01,  2.5379e-01,  2.5566e-02,  2.0880e-01,\n",
       "         -2.5883e-01, -9.0354e-02,  3.9658e-02],\n",
       "        [-2.2355e-01, -5.2325e-01,  1.2222e+00, -1.9340e-02, -6.4383e-01,\n",
       "         -3.7316e-01, -1.1613e-02,  1.5809e-01],\n",
       "        [ 5.0886e-01,  1.3460e-01, -9.7235e-01,  2.7012e-01,  6.8731e-01,\n",
       "          3.4129e-01, -2.4020e-01, -5.9821e-01],\n",
       "        [ 2.5441e-01, -3.2733e-01,  3.6173e-02,  5.8539e-01, -4.1812e-01,\n",
       "         -3.7370e-01, -2.9181e-02, -1.4754e+00],\n",
       "        [-7.3212e-01, -5.4298e-02,  5.9804e-01,  3.4914e-01, -3.8502e-01,\n",
       "         -1.2013e-02, -6.8037e-01, -1.5393e-02],\n",
       "        [-1.4975e-01,  3.9417e-01,  6.4105e-01, -7.4548e-01, -2.1495e-01,\n",
       "         -3.3793e-01, -1.3342e+00, -7.1371e-01],\n",
       "        [-4.9468e-01,  3.3144e-02,  3.6539e-01,  3.9460e-02, -3.2954e-01,\n",
       "          1.0913e-01,  3.0981e-01, -2.0742e-01],\n",
       "        [-4.0072e-01, -6.4152e-01,  1.0325e+00, -3.9986e-01, -1.1495e+00,\n",
       "         -4.0330e-01, -2.9347e-01,  3.4248e-01],\n",
       "        [ 2.6398e-01, -3.6620e-01, -1.5438e-01,  6.4225e-01,  6.3902e-01,\n",
       "         -7.4333e-01, -1.0259e+00, -1.1719e+00]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit(torch.randn(64, 14, 1, 300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
