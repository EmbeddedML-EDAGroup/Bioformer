{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(n_tokens, dim, dim_head):\n",
    "    macs = 0\n",
    "    \n",
    "    # token -> k, q, v\n",
    "    macs += n_tokens * 3 * dim * dim_head\n",
    "    \n",
    "    # q * k'\n",
    "    # (n_tokens, dim_head) * (dim_head, n_tokens) -> (n_tokens, n_tokens)\n",
    "    macs += n_tokens * dim_head * n_tokens\n",
    "    \n",
    "    # Softmax e diviso sqrt(dim_head) \n",
    "    # ...\n",
    "    \n",
    "    # (q * k') * v\n",
    "    # (n_tokens, n_tokens) * (n_tokens, dim_head) -> (n_tokens, dim_head)\n",
    "    macs += n_tokens * n_tokens * dim_head\n",
    "    \n",
    "    return macs\n",
    "    \n",
    "def attention(n_tokens, dim, dim_head, n_heads):\n",
    "    macs = 0\n",
    "    \n",
    "    macs += n_heads * attention_head(n_tokens, dim, dim_head)\n",
    "    \n",
    "    # Riporta gli z concatenati a dimensione dim\n",
    "    macs += n_tokens * (dim_head * n_heads) * dim if not (n_heads == 1 and dim_head == dim) else 0\n",
    "    \n",
    "    return macs\n",
    "\n",
    "def feed_forward(n_tokens, dim, mlp_dim):\n",
    "    # 2 Linear: dim -> mlp_dim, mlp_dim -> dim\n",
    "    return n_tokens * dim * mlp_dim * 2\n",
    "\n",
    "def transformer(n_tokens, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    return depth * (attention(n_tokens, dim, dim_head, n_heads) + feed_forward(n_tokens, dim, mlp_dim))\n",
    "\n",
    "def vit(patch_size, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "               \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = qkv.chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., use_cls_token=True):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        #self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "        #nn.init.kaiming_uniform_(self.pos_embedding, a=5 ** .5)\n",
    "        nn.init.normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "        #self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.zeros_(self.cls_token)\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # MACs: patch_size * n_patches * dim, es (30 * 14) * 10 * 300\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # FeedForward    Attention       project out\n",
    "        # 300*300*10*2 + 300*(64*3)*10 + ((64)*300*10)\n",
    "        # Attention -> manca softmax e attention vera e propria, c'è solo linear encoding a qkv\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        \n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "    \n",
    "# Ratio of params\n",
    "def vit_aff_ratio(patch_size, dim, dim_head, n_heads, mlp_dim, depth): \n",
    "    n_tokens = 300 // patch_size + 1\n",
    "    \n",
    "    a = (((dim) * dim_head * 3 * n_heads) + ((dim_head * n_heads) * dim) + dim)\n",
    "    ff = dim * mlp_dim * 2 + mlp_dim + dim\n",
    "    \n",
    "    return a / (a + ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer(n_tokens=31, dim, dim_head, n_heads, mlp_dim, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[64, 64]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = nn.LayerNorm(64)\n",
    "[p.nelement() for p in ln.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ViT(image_size=(1, 300), patch_size=(1, 10), num_classes=8, dim=64, depth=1, heads=8, mlp_dim=128, pool = 'cls', channels = 14, dim_head = 32, dropout = 0., emb_dropout = 0., use_cls_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in v.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(configs, results_, additional_columns, extract_model_hparams):\n",
    "        \n",
    "    acccs = []\n",
    "    acccs_steady = []\n",
    "    acccs_val0 = []\n",
    "    acccs_steady_val0 = []\n",
    "    acccs_val1 = []\n",
    "    acccs_steady_val1 = []\n",
    "\n",
    "    acccs_val_val0 = 0\n",
    "    acccs_val_val1 = 0\n",
    "    acccs_train_val0 = 0\n",
    "    acccs_train_val1 = 0\n",
    "    \n",
    "    acccs_steady_persubject = np.array([0] * 10, dtype=float)\n",
    "    preds_steady_bincounts_subject = np.zeros((10, 8), dtype=int)\n",
    "    \n",
    "    for config, r in zip(configs, results_):\n",
    "\n",
    "        accs = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "            accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        accs /= 2\n",
    "\n",
    "        accs_steady = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        accs_steady /= 2\n",
    "\n",
    "        accs_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        accs_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "            accs_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "            accs_steady_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        acccs_train_val0 += r['val-fold_0']['losses_accs'][-1]['train_acc']\n",
    "        acccs_train_val1 += r['val-fold_1']['losses_accs'][-1]['train_acc']\n",
    "\n",
    "        acccs_val_val0 += r['val-fold_0']['losses_accs'][-1]['val_acc']\n",
    "        acccs_val_val1 += r['val-fold_1']['losses_accs'][-1]['val_acc']\n",
    "\n",
    "        acccs.append(accs)\n",
    "        acccs_steady.append(accs_steady)\n",
    "        acccs_val0.append(accs_val0)\n",
    "        acccs_steady_val0.append(accs_steady_val0)\n",
    "        acccs_val1.append(accs_val1)\n",
    "        acccs_steady_val1.append(accs_steady_val1)\n",
    "        \n",
    "    test_sessions = len(r['test_sessions'])\n",
    "\n",
    "    acccs_steady_persubject /= test_sessions * 2 # 5 sessioni per due\n",
    "\n",
    "    acccs = np.array(acccs).mean(axis=0)\n",
    "\n",
    "    acccs_steady_ = np.array(acccs_steady).mean(axis=1)\n",
    "    acccs_steady = np.array(acccs_steady).mean(axis=0)\n",
    "\n",
    "    acccs_val0 = np.array(acccs_val0).mean(axis=0)\n",
    "    acccs_val1 = np.array(acccs_val1).mean(axis=0)\n",
    "    acccs_steady_val0 = np.array(acccs_steady_val0).mean(axis=0)\n",
    "    acccs_steady_val1 = np.array(acccs_steady_val1).mean(axis=0)\n",
    "    acccs_val_val0 /= 10\n",
    "    acccs_val_val1 /= 10\n",
    "    acccs_train_val0 /= 10\n",
    "    acccs_train_val1 /= 10\n",
    "    \n",
    "    model_hparams = extract_model_hparams(config)\n",
    "\n",
    "    return {        \n",
    "        **model_hparams,\n",
    "\n",
    "        **additional_columns,\n",
    "\n",
    "        \"train accuracy steady fold1\":  acccs_train_val0,\n",
    "        \"train accuracy steady fold2\":  acccs_train_val1,\n",
    "        \"train accuracy steady avg2folds\": .5 * (acccs_train_val0 + acccs_train_val1),\n",
    "\n",
    "        \"validation accuracy steady fold1\": acccs_val_val0,\n",
    "        \"validation accuracy steady fold2\": acccs_val_val1,\n",
    "\n",
    "        \"test accuracy fold1\": acccs_val0.mean(),\n",
    "        \"test accuracy fold2\": acccs_val1.mean(),\n",
    "        \"test accuracy avg2folds\": acccs.mean(), \n",
    "        \"test accuracy steady fold1\": acccs_steady_val0.mean(),\n",
    "        \"test accuracy steady fold2\": acccs_steady_val1.mean(), \n",
    "        \"test accuracy steady avg2folds\": acccs_steady.mean(),\n",
    "\n",
    "        \"test accuracy steady avg2folds std across sessions\": acccs_steady.std(),\n",
    "        \"test accuracy steady avg2folds std across subjects\": acccs_steady_.std(),\n",
    "        \n",
    "        **{\n",
    "          f\"test accuracy steady session{s + 1 + test_sessions} avg2folds\": acccs_steady[s] for s in range(test_sessions)\n",
    "        },\n",
    "        \n",
    "        **{\n",
    "            f\"test accuracy steady subj{s} avg2folds\": acccs_steady_persubject[s] for s in range(10)\n",
    "        },\n",
    "        \n",
    "        **{\n",
    "            f\"test preds steady subj{s} avg2folds\": preds_steady_bincounts_subject[s] for s in range(10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_configs(configs, group_exclude_columns):\n",
    "    # https://stackoverflow.com/a/6027615\n",
    "    import collections.abc\n",
    "\n",
    "    def flatten(d, parent_key='', sep='_'):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = parent_key + sep + k if parent_key else k\n",
    "            if isinstance(v, collections.abc.MutableMapping):\n",
    "                items.extend(flatten(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    df = pd.DataFrame([flatten(config) for config in configs])\n",
    "    df['row_id'] = [[a] for a in df.index]\n",
    "    \n",
    "    if 'training_config_lr_scheduler_hparams_milestones' in df:\n",
    "        df['training_config_lr_scheduler_hparams_milestones'] = df['training_config_lr_scheduler_hparams_milestones'].apply(lambda x: ','.join(map(str, x)))\n",
    "    df = df.fillna('null')\n",
    "    \n",
    "    grouped_configs = df.groupby([c for c in df.columns if c not in group_exclude_columns]).agg({'subjects': 'count', 'row_id': 'sum'})\n",
    "    \n",
    "    if (grouped_configs['subjects'] != 10).sum() != 0:\n",
    "        display(grouped_configs)\n",
    "        raise ValueError(\"For every config, it is assumed that you trained on 10 subjects\")\n",
    "    \n",
    "    return list(grouped_configs[\"row_id\"])\n",
    "\n",
    "\n",
    "extract_model_hparams_generator = {\n",
    "    'vit': lambda config: {\n",
    "        \"window_size\": config[\"image_size\"][1],\n",
    "        \"patch_size\": config[\"patch_size\"][1],\n",
    "        \"dim_projection\": config[\"dim\"],\n",
    "        \"dim_ff\": config[\"mlp_dim\"],\n",
    "        \"dim_head\": config[\"dim_head\"],\n",
    "        \"n_heads\": config[\"heads\"],\n",
    "        \"depth\": config[\"depth\"],\n",
    "        \"dropout\": config[\"dropout\"],\n",
    "        \"emb_dropout\": config[\"emb_dropout\"],\n",
    "        \n",
    "        \"MACs\": vit(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \"params\":  sum([param.nelement() for param in ViT(channels=14, image_size=(1, 300), patch_size=config[\"patch_size\"], dim=config[\"dim\"], dim_head=config[\"dim_head\"], heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"], num_classes=8).parameters()]),\n",
    "        \"params_aff_ratio\": vit_aff_ratio(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \n",
    "    },\n",
    "    'temponet': lambda _: {\n",
    "        \"MACs\": 16028672,\n",
    "        \"params\": 461512,\n",
    "    },\n",
    "}\n",
    "\n",
    "def read_results(filename, additional_columns=None, group_exclude_columns=None, model_name='vit'):\n",
    "    additional_columns = {} if additional_columns is None else additional_columns\n",
    "    \n",
    "    group_exclude_columns = set() if group_exclude_columns is None else group_exclude_columns\n",
    "    group_exclude_columns = group_exclude_columns.union({'subjects', 'row_id'})\n",
    "    \n",
    "    configs, results_ = load(open(filename, 'rb'))\n",
    "    \n",
    "    groups_indices = group_configs(configs, group_exclude_columns)\n",
    "    \n",
    "    df_l = []\n",
    "    for idx in groups_indices:\n",
    "        c = [configs[i] for i in idx]\n",
    "        r = [results_[i] for i in idx]\n",
    "        \n",
    "        df_l.append(get_results(c, r, additional_columns, extract_model_hparams_generator[model_name]))   \n",
    "    \n",
    "    return pd.DataFrame(df_l) \n",
    "\n",
    "def get_rows(all_res_vit, group):\n",
    "    m = None\n",
    "    for k in group.keys():\n",
    "        current_m = all_res_vit[k] == group[k]\n",
    "        if m is None:\n",
    "            m = current_m\n",
    "        else:\n",
    "            m &= current_m\n",
    "    return all_res_vit[m].copy()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_pretrain_9 = read_results(\"exp9/grid_5sess10subj_nopretraining_fold2/results_1627417222.pickle\", {\"pretraining\": 'no', \"pretraining_epochs\": 0, \"finetune_epochs\": 200}, group_exclude_columns={'pretrained'}) \\\n",
    "        .append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100/results_1626421922.pickle\", {\"pretraining\": 'all_others_20', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100_fix/results_1627588086.pickle\", {\"pretraining\": 'all_others_20_fix', \"pretraining_epochs\": 100, \"finetune_epochs\": 40}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100_fix_10/results_1627593378.pickle\", {\"pretraining\": 'all_others_20_fix_10', \"pretraining_epochs\": 100, \"finetune_epochs\": 40}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    ".append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100_fix_10bs8/results_1627593378.pickle\", {\"pretraining\": 'all_others_20_fix_10bs8', \"pretraining_epochs\": 100, \"finetune_epochs\": 40}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    ".append(read_results(\"exp9/pretrain9.5/results_1627617535.pickle\", {\"pretraining\": 'all_others_9.5', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/vit_pretraining_9/results_1627311468.pickle\", {\"pretraining\": 'all_others_min20-20', \"pretraining_epochs\": 100, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/vit_pretraining_9/results_1627333294.pickle\", {\"pretraining\": 'all_others_min20-20from40', \"pretraining_epochs\": 40, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/vit_pretraining_9/results_1627341881.pickle\", {\"pretraining\": 'all_others_hd', \"pretraining_epochs\": 100, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>dim_projection</th>\n",
       "      <th>dim_ff</th>\n",
       "      <th>dim_head</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>depth</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dropout</th>\n",
       "      <th>MACs</th>\n",
       "      <th>params</th>\n",
       "      <th>params_aff_ratio</th>\n",
       "      <th>pretraining</th>\n",
       "      <th>pretraining_epochs</th>\n",
       "      <th>finetune_epochs</th>\n",
       "      <th>train accuracy steady fold1</th>\n",
       "      <th>train accuracy steady fold2</th>\n",
       "      <th>train accuracy steady avg2folds</th>\n",
       "      <th>validation accuracy steady fold1</th>\n",
       "      <th>validation accuracy steady fold2</th>\n",
       "      <th>test accuracy fold1</th>\n",
       "      <th>test accuracy fold2</th>\n",
       "      <th>test accuracy avg2folds</th>\n",
       "      <th>test accuracy steady fold1</th>\n",
       "      <th>test accuracy steady fold2</th>\n",
       "      <th>test accuracy steady avg2folds</th>\n",
       "      <th>test accuracy steady avg2folds std across sessions</th>\n",
       "      <th>test accuracy steady avg2folds std across subjects</th>\n",
       "      <th>test accuracy steady session6 avg2folds</th>\n",
       "      <th>test accuracy steady session7 avg2folds</th>\n",
       "      <th>test accuracy steady session8 avg2folds</th>\n",
       "      <th>test accuracy steady session9 avg2folds</th>\n",
       "      <th>test accuracy steady session10 avg2folds</th>\n",
       "      <th>test accuracy steady subj0 avg2folds</th>\n",
       "      <th>test accuracy steady subj1 avg2folds</th>\n",
       "      <th>test accuracy steady subj2 avg2folds</th>\n",
       "      <th>test accuracy steady subj3 avg2folds</th>\n",
       "      <th>test accuracy steady subj4 avg2folds</th>\n",
       "      <th>test accuracy steady subj5 avg2folds</th>\n",
       "      <th>test accuracy steady subj6 avg2folds</th>\n",
       "      <th>test accuracy steady subj7 avg2folds</th>\n",
       "      <th>test accuracy steady subj8 avg2folds</th>\n",
       "      <th>test accuracy steady subj9 avg2folds</th>\n",
       "      <th>test preds steady subj0 avg2folds</th>\n",
       "      <th>test preds steady subj1 avg2folds</th>\n",
       "      <th>test preds steady subj2 avg2folds</th>\n",
       "      <th>test preds steady subj3 avg2folds</th>\n",
       "      <th>test preds steady subj4 avg2folds</th>\n",
       "      <th>test preds steady subj5 avg2folds</th>\n",
       "      <th>test preds steady subj6 avg2folds</th>\n",
       "      <th>test preds steady subj7 avg2folds</th>\n",
       "      <th>test preds steady subj8 avg2folds</th>\n",
       "      <th>test preds steady subj9 avg2folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.974234</td>\n",
       "      <td>0.969996</td>\n",
       "      <td>0.972115</td>\n",
       "      <td>0.691544</td>\n",
       "      <td>0.691096</td>\n",
       "      <td>0.481805</td>\n",
       "      <td>0.477129</td>\n",
       "      <td>0.479467</td>\n",
       "      <td>0.639890</td>\n",
       "      <td>0.628881</td>\n",
       "      <td>0.634386</td>\n",
       "      <td>0.017992</td>\n",
       "      <td>0.079836</td>\n",
       "      <td>0.637813</td>\n",
       "      <td>0.626278</td>\n",
       "      <td>0.665994</td>\n",
       "      <td>0.630395</td>\n",
       "      <td>0.611449</td>\n",
       "      <td>0.741879</td>\n",
       "      <td>0.675315</td>\n",
       "      <td>0.616850</td>\n",
       "      <td>0.667227</td>\n",
       "      <td>0.784742</td>\n",
       "      <td>0.636111</td>\n",
       "      <td>0.575114</td>\n",
       "      <td>0.555989</td>\n",
       "      <td>0.523119</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>[43683, 8300, 7663, 10186, 4802, 8483, 8844, 5...</td>\n",
       "      <td>[37096, 7620, 4609, 6389, 15561, 10865, 8635, ...</td>\n",
       "      <td>[52591, 5152, 9735, 4958, 7867, 6127, 5779, 5193]</td>\n",
       "      <td>[45926, 7769, 5405, 7001, 6477, 10985, 6543, 6...</td>\n",
       "      <td>[52599, 8356, 7453, 7592, 4948, 4856, 7237, 4289]</td>\n",
       "      <td>[44874, 5145, 4933, 5653, 6904, 13364, 10596, ...</td>\n",
       "      <td>[39292, 8151, 9642, 4945, 11295, 7753, 11643, ...</td>\n",
       "      <td>[61414, 10642, 5421, 6018, 4191, 1553, 5291, 3...</td>\n",
       "      <td>[74741, 5096, 6328, 2890, 3584, 1901, 3079, 1449]</td>\n",
       "      <td>[43748, 4575, 11866, 8372, 9800, 8609, 4053, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.792333</td>\n",
       "      <td>0.737745</td>\n",
       "      <td>0.765039</td>\n",
       "      <td>0.655784</td>\n",
       "      <td>0.660054</td>\n",
       "      <td>0.460341</td>\n",
       "      <td>0.444448</td>\n",
       "      <td>0.452395</td>\n",
       "      <td>0.608122</td>\n",
       "      <td>0.593420</td>\n",
       "      <td>0.600771</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.120493</td>\n",
       "      <td>0.605976</td>\n",
       "      <td>0.599211</td>\n",
       "      <td>0.625779</td>\n",
       "      <td>0.578535</td>\n",
       "      <td>0.594354</td>\n",
       "      <td>0.755758</td>\n",
       "      <td>0.392205</td>\n",
       "      <td>0.477707</td>\n",
       "      <td>0.692919</td>\n",
       "      <td>0.783042</td>\n",
       "      <td>0.472039</td>\n",
       "      <td>0.645453</td>\n",
       "      <td>0.617244</td>\n",
       "      <td>0.558191</td>\n",
       "      <td>0.613153</td>\n",
       "      <td>[42382, 9906, 7322, 8294, 4515, 10634, 9355, 4...</td>\n",
       "      <td>[97068, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[83428, 5730, 3614, 1221, 636, 2398, 7, 368]</td>\n",
       "      <td>[44084, 7843, 7732, 6714, 4990, 11060, 7849, 6...</td>\n",
       "      <td>[52022, 9207, 7626, 8803, 4678, 5942, 4941, 4111]</td>\n",
       "      <td>[51263, 2941, 8735, 2120, 4437, 8606, 16523, 2...</td>\n",
       "      <td>[37519, 8339, 11970, 9627, 7565, 7951, 9157, 5...</td>\n",
       "      <td>[53888, 7964, 6869, 5838, 5333, 1970, 7512, 8568]</td>\n",
       "      <td>[73465, 6928, 3997, 3855, 3498, 2091, 3130, 2104]</td>\n",
       "      <td>[40454, 4862, 6168, 8926, 15658, 10874, 5963, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20_fix</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.955310</td>\n",
       "      <td>0.942484</td>\n",
       "      <td>0.948897</td>\n",
       "      <td>0.708360</td>\n",
       "      <td>0.714751</td>\n",
       "      <td>0.499362</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.658357</td>\n",
       "      <td>0.643371</td>\n",
       "      <td>0.650864</td>\n",
       "      <td>0.015441</td>\n",
       "      <td>0.077375</td>\n",
       "      <td>0.657441</td>\n",
       "      <td>0.649427</td>\n",
       "      <td>0.675946</td>\n",
       "      <td>0.630234</td>\n",
       "      <td>0.641269</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0.656614</td>\n",
       "      <td>0.642311</td>\n",
       "      <td>0.678129</td>\n",
       "      <td>0.806790</td>\n",
       "      <td>0.618921</td>\n",
       "      <td>0.641813</td>\n",
       "      <td>0.594206</td>\n",
       "      <td>0.520928</td>\n",
       "      <td>0.595732</td>\n",
       "      <td>[42537, 9700, 7024, 8530, 5099, 11005, 8204, 5...</td>\n",
       "      <td>[35400, 7959, 2862, 4531, 15576, 10961, 9577, ...</td>\n",
       "      <td>[50162, 4469, 9065, 5241, 6647, 9932, 7253, 4633]</td>\n",
       "      <td>[46081, 8482, 5517, 6489, 6605, 10703, 7144, 5...</td>\n",
       "      <td>[52839, 7440, 7690, 8170, 4698, 6871, 5663, 3959]</td>\n",
       "      <td>[42317, 5905, 5788, 6042, 4677, 15283, 11544, ...</td>\n",
       "      <td>[37716, 8323, 10726, 10230, 7541, 8194, 10243,...</td>\n",
       "      <td>[56188, 8111, 6442, 4636, 6892, 2265, 8314, 5094]</td>\n",
       "      <td>[67523, 9115, 5183, 2529, 3712, 4363, 3772, 2871]</td>\n",
       "      <td>[44283, 3920, 4881, 6871, 14326, 12657, 5057, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20_fix_10</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.930601</td>\n",
       "      <td>0.913502</td>\n",
       "      <td>0.922051</td>\n",
       "      <td>0.703747</td>\n",
       "      <td>0.712814</td>\n",
       "      <td>0.500719</td>\n",
       "      <td>0.486503</td>\n",
       "      <td>0.493611</td>\n",
       "      <td>0.659405</td>\n",
       "      <td>0.640987</td>\n",
       "      <td>0.650196</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.076751</td>\n",
       "      <td>0.658096</td>\n",
       "      <td>0.651570</td>\n",
       "      <td>0.677182</td>\n",
       "      <td>0.625867</td>\n",
       "      <td>0.638264</td>\n",
       "      <td>0.749907</td>\n",
       "      <td>0.651094</td>\n",
       "      <td>0.640269</td>\n",
       "      <td>0.681674</td>\n",
       "      <td>0.805669</td>\n",
       "      <td>0.619584</td>\n",
       "      <td>0.639422</td>\n",
       "      <td>0.599455</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>[42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...</td>\n",
       "      <td>[35379, 6848, 3072, 3653, 15247, 11575, 9471, ...</td>\n",
       "      <td>[51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]</td>\n",
       "      <td>[45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...</td>\n",
       "      <td>[52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]</td>\n",
       "      <td>[43045, 5811, 6303, 6441, 4370, 15096, 10855, ...</td>\n",
       "      <td>[37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...</td>\n",
       "      <td>[56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]</td>\n",
       "      <td>[65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...</td>\n",
       "      <td>[43242, 4183, 4748, 6365, 14667, 13749, 5124, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20_fix_10bs8</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.930601</td>\n",
       "      <td>0.913502</td>\n",
       "      <td>0.922051</td>\n",
       "      <td>0.703747</td>\n",
       "      <td>0.712814</td>\n",
       "      <td>0.500719</td>\n",
       "      <td>0.486503</td>\n",
       "      <td>0.493611</td>\n",
       "      <td>0.659405</td>\n",
       "      <td>0.640987</td>\n",
       "      <td>0.650196</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.076751</td>\n",
       "      <td>0.658096</td>\n",
       "      <td>0.651570</td>\n",
       "      <td>0.677182</td>\n",
       "      <td>0.625867</td>\n",
       "      <td>0.638264</td>\n",
       "      <td>0.749907</td>\n",
       "      <td>0.651094</td>\n",
       "      <td>0.640269</td>\n",
       "      <td>0.681674</td>\n",
       "      <td>0.805669</td>\n",
       "      <td>0.619584</td>\n",
       "      <td>0.639422</td>\n",
       "      <td>0.599455</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>[42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...</td>\n",
       "      <td>[35379, 6848, 3072, 3653, 15247, 11575, 9471, ...</td>\n",
       "      <td>[51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]</td>\n",
       "      <td>[45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...</td>\n",
       "      <td>[52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]</td>\n",
       "      <td>[43045, 5811, 6303, 6441, 4370, 15096, 10855, ...</td>\n",
       "      <td>[37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...</td>\n",
       "      <td>[56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]</td>\n",
       "      <td>[65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...</td>\n",
       "      <td>[43242, 4183, 4748, 6365, 14667, 13749, 5124, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_9.5</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.931951</td>\n",
       "      <td>0.914357</td>\n",
       "      <td>0.923154</td>\n",
       "      <td>0.707964</td>\n",
       "      <td>0.716697</td>\n",
       "      <td>0.505896</td>\n",
       "      <td>0.494344</td>\n",
       "      <td>0.500120</td>\n",
       "      <td>0.667250</td>\n",
       "      <td>0.647310</td>\n",
       "      <td>0.657280</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.074118</td>\n",
       "      <td>0.665342</td>\n",
       "      <td>0.657218</td>\n",
       "      <td>0.684159</td>\n",
       "      <td>0.640325</td>\n",
       "      <td>0.639355</td>\n",
       "      <td>0.758244</td>\n",
       "      <td>0.673686</td>\n",
       "      <td>0.650122</td>\n",
       "      <td>0.675123</td>\n",
       "      <td>0.811746</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.640404</td>\n",
       "      <td>0.604953</td>\n",
       "      <td>0.549219</td>\n",
       "      <td>0.605902</td>\n",
       "      <td>[42338, 9750, 8110, 8274, 4124, 10339, 9045, 5...</td>\n",
       "      <td>[35840, 8146, 3298, 4105, 14213, 11276, 9060, ...</td>\n",
       "      <td>[51233, 4384, 7781, 5462, 6944, 10979, 5926, 4...</td>\n",
       "      <td>[45355, 8938, 6188, 7020, 5351, 10843, 7402, 5...</td>\n",
       "      <td>[52878, 7240, 8270, 8382, 4259, 6466, 5014, 4821]</td>\n",
       "      <td>[41994, 5708, 6966, 6063, 5336, 14776, 11007, ...</td>\n",
       "      <td>[37493, 8224, 11889, 9761, 7863, 8306, 9706, 4...</td>\n",
       "      <td>[55129, 8449, 5846, 5249, 6157, 2015, 8532, 6565]</td>\n",
       "      <td>[72810, 5945, 4523, 3268, 4115, 3117, 3415, 1875]</td>\n",
       "      <td>[42707, 4534, 4659, 7412, 15824, 11516, 4982, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_min20-20</td>\n",
       "      <td>100</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.960346</td>\n",
       "      <td>0.950499</td>\n",
       "      <td>0.955423</td>\n",
       "      <td>0.711452</td>\n",
       "      <td>0.719812</td>\n",
       "      <td>0.489861</td>\n",
       "      <td>0.482169</td>\n",
       "      <td>0.486015</td>\n",
       "      <td>0.644417</td>\n",
       "      <td>0.633744</td>\n",
       "      <td>0.639080</td>\n",
       "      <td>0.016364</td>\n",
       "      <td>0.076835</td>\n",
       "      <td>0.653273</td>\n",
       "      <td>0.632209</td>\n",
       "      <td>0.662876</td>\n",
       "      <td>0.619054</td>\n",
       "      <td>0.627990</td>\n",
       "      <td>0.742444</td>\n",
       "      <td>0.582029</td>\n",
       "      <td>0.618263</td>\n",
       "      <td>0.677145</td>\n",
       "      <td>0.803733</td>\n",
       "      <td>0.614512</td>\n",
       "      <td>0.629365</td>\n",
       "      <td>0.589930</td>\n",
       "      <td>0.531256</td>\n",
       "      <td>0.602128</td>\n",
       "      <td>[42763, 9884, 6540, 7986, 5341, 11634, 8277, 4...</td>\n",
       "      <td>[34757, 5072, 4645, 2849, 8946, 12170, 11238, ...</td>\n",
       "      <td>[52517, 3482, 5792, 4520, 8249, 11040, 7358, 4...</td>\n",
       "      <td>[45997, 8902, 6995, 5306, 6338, 11202, 6193, 6...</td>\n",
       "      <td>[53122, 8011, 7682, 8706, 4690, 5983, 5349, 3787]</td>\n",
       "      <td>[43770, 4952, 5148, 7286, 5583, 11404, 13372, ...</td>\n",
       "      <td>[38270, 7926, 11250, 8812, 8968, 8645, 9469, 4...</td>\n",
       "      <td>[54102, 8807, 6210, 4601, 6443, 2529, 9947, 5303]</td>\n",
       "      <td>[70170, 5699, 6323, 2988, 4986, 3545, 3158, 2199]</td>\n",
       "      <td>[43862, 4354, 4846, 7864, 14067, 11695, 5131, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_min20-20from40</td>\n",
       "      <td>40</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.939094</td>\n",
       "      <td>0.927953</td>\n",
       "      <td>0.933524</td>\n",
       "      <td>0.693335</td>\n",
       "      <td>0.697301</td>\n",
       "      <td>0.483659</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>0.478230</td>\n",
       "      <td>0.638465</td>\n",
       "      <td>0.622259</td>\n",
       "      <td>0.630362</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>0.073742</td>\n",
       "      <td>0.639485</td>\n",
       "      <td>0.624374</td>\n",
       "      <td>0.659964</td>\n",
       "      <td>0.605693</td>\n",
       "      <td>0.622295</td>\n",
       "      <td>0.739955</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.604076</td>\n",
       "      <td>0.661725</td>\n",
       "      <td>0.781150</td>\n",
       "      <td>0.612923</td>\n",
       "      <td>0.626735</td>\n",
       "      <td>0.588007</td>\n",
       "      <td>0.524532</td>\n",
       "      <td>0.585203</td>\n",
       "      <td>[42699, 10313, 6503, 8072, 5559, 10826, 8270, ...</td>\n",
       "      <td>[33820, 6053, 3058, 3572, 11857, 10220, 11483,...</td>\n",
       "      <td>[51735, 3903, 6662, 4609, 9566, 9573, 5992, 5362]</td>\n",
       "      <td>[46162, 9521, 6221, 4640, 5340, 9920, 8623, 6587]</td>\n",
       "      <td>[53149, 6896, 9564, 7236, 4932, 5975, 5737, 3841]</td>\n",
       "      <td>[44580, 4537, 5143, 7133, 6190, 11474, 12305, ...</td>\n",
       "      <td>[38615, 8252, 10522, 8485, 8517, 7893, 10872, ...</td>\n",
       "      <td>[57376, 8582, 6284, 4872, 5460, 2182, 8291, 4895]</td>\n",
       "      <td>[69951, 6045, 4176, 3032, 4994, 3835, 3783, 3252]</td>\n",
       "      <td>[44523, 4327, 6397, 7783, 12398, 11383, 5023, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_hd</td>\n",
       "      <td>100</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.903182</td>\n",
       "      <td>0.878725</td>\n",
       "      <td>0.890954</td>\n",
       "      <td>0.693769</td>\n",
       "      <td>0.700088</td>\n",
       "      <td>0.497503</td>\n",
       "      <td>0.485919</td>\n",
       "      <td>0.491711</td>\n",
       "      <td>0.647878</td>\n",
       "      <td>0.631858</td>\n",
       "      <td>0.639868</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.660336</td>\n",
       "      <td>0.630016</td>\n",
       "      <td>0.666708</td>\n",
       "      <td>0.619943</td>\n",
       "      <td>0.622335</td>\n",
       "      <td>0.749187</td>\n",
       "      <td>0.571347</td>\n",
       "      <td>0.633124</td>\n",
       "      <td>0.689213</td>\n",
       "      <td>0.809166</td>\n",
       "      <td>0.621342</td>\n",
       "      <td>0.621781</td>\n",
       "      <td>0.605645</td>\n",
       "      <td>0.496953</td>\n",
       "      <td>0.600921</td>\n",
       "      <td>[41816, 9942, 7648, 9023, 4181, 10607, 8476, 5...</td>\n",
       "      <td>[32376, 6707, 6017, 3522, 8234, 12767, 10004, ...</td>\n",
       "      <td>[48248, 4144, 7882, 5102, 8106, 11834, 7572, 4...</td>\n",
       "      <td>[43316, 8883, 10423, 6545, 5150, 10360, 5358, ...</td>\n",
       "      <td>[52617, 7905, 7206, 7575, 5678, 6796, 5156, 4397]</td>\n",
       "      <td>[40215, 6041, 3975, 5769, 5032, 14118, 13994, ...</td>\n",
       "      <td>[36032, 7341, 11815, 9392, 7073, 11002, 10771,...</td>\n",
       "      <td>[46711, 11186, 9334, 5658, 6321, 3994, 8224, 6...</td>\n",
       "      <td>[59424, 14255, 7353, 4847, 3272, 3603, 4086, 2...</td>\n",
       "      <td>[39423, 5625, 7140, 7692, 12475, 11916, 5798, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  patch_size  dim_projection  dim_ff  dim_head  n_heads  depth  \\\n",
       "0          300          10              64     128        32        8      1   \n",
       "1          300          10              64     128        32        8      1   \n",
       "2          300          10              64     128        32        8      1   \n",
       "3          300          10              64     128        32        8      1   \n",
       "4          300          10              64     128        32        8      1   \n",
       "5          300          10              64     128        32        8      1   \n",
       "6          300          10              64     128        32        8      1   \n",
       "7          300          10              64     128        32        8      1   \n",
       "8          300          10              64     128        32        8      1   \n",
       "\n",
       "   dropout  emb_dropout     MACs  params  params_aff_ratio  \\\n",
       "0      0.2            0  3300864   94152          0.798287   \n",
       "1      0.2            0  3300864   94152          0.798287   \n",
       "2      0.2            0  3300864   94152          0.798287   \n",
       "3      0.2            0  3300864   94152          0.798287   \n",
       "4      0.2            0  3300864   94152          0.798287   \n",
       "5      0.2            0  3300864   94152          0.798287   \n",
       "6      0.2            0  3300864   94152          0.798287   \n",
       "7      0.2            0  3300864   94152          0.798287   \n",
       "8      0.5            0  3300864   94152          0.798287   \n",
       "\n",
       "                 pretraining  pretraining_epochs finetune_epochs  \\\n",
       "0                         no                   0             200   \n",
       "1              all_others_20                 100              20   \n",
       "2          all_others_20_fix                 100              40   \n",
       "3       all_others_20_fix_10                 100              40   \n",
       "4    all_others_20_fix_10bs8                 100              40   \n",
       "5             all_others_9.5                 100              20   \n",
       "6        all_others_min20-20                 100        min20-20   \n",
       "7  all_others_min20-20from40                  40        min20-20   \n",
       "8              all_others_hd                 100        min20-20   \n",
       "\n",
       "   train accuracy steady fold1  train accuracy steady fold2  \\\n",
       "0                     0.974234                     0.969996   \n",
       "1                     0.792333                     0.737745   \n",
       "2                     0.955310                     0.942484   \n",
       "3                     0.930601                     0.913502   \n",
       "4                     0.930601                     0.913502   \n",
       "5                     0.931951                     0.914357   \n",
       "6                     0.960346                     0.950499   \n",
       "7                     0.939094                     0.927953   \n",
       "8                     0.903182                     0.878725   \n",
       "\n",
       "   train accuracy steady avg2folds  validation accuracy steady fold1  \\\n",
       "0                         0.972115                          0.691544   \n",
       "1                         0.765039                          0.655784   \n",
       "2                         0.948897                          0.708360   \n",
       "3                         0.922051                          0.703747   \n",
       "4                         0.922051                          0.703747   \n",
       "5                         0.923154                          0.707964   \n",
       "6                         0.955423                          0.711452   \n",
       "7                         0.933524                          0.693335   \n",
       "8                         0.890954                          0.693769   \n",
       "\n",
       "   validation accuracy steady fold2  test accuracy fold1  test accuracy fold2  \\\n",
       "0                          0.691096             0.481805             0.477129   \n",
       "1                          0.660054             0.460341             0.444448   \n",
       "2                          0.714751             0.499362             0.488291   \n",
       "3                          0.712814             0.500719             0.486503   \n",
       "4                          0.712814             0.500719             0.486503   \n",
       "5                          0.716697             0.505896             0.494344   \n",
       "6                          0.719812             0.489861             0.482169   \n",
       "7                          0.697301             0.483659             0.472801   \n",
       "8                          0.700088             0.497503             0.485919   \n",
       "\n",
       "   test accuracy avg2folds  test accuracy steady fold1  \\\n",
       "0                 0.479467                    0.639890   \n",
       "1                 0.452395                    0.608122   \n",
       "2                 0.493827                    0.658357   \n",
       "3                 0.493611                    0.659405   \n",
       "4                 0.493611                    0.659405   \n",
       "5                 0.500120                    0.667250   \n",
       "6                 0.486015                    0.644417   \n",
       "7                 0.478230                    0.638465   \n",
       "8                 0.491711                    0.647878   \n",
       "\n",
       "   test accuracy steady fold2  test accuracy steady avg2folds  \\\n",
       "0                    0.628881                        0.634386   \n",
       "1                    0.593420                        0.600771   \n",
       "2                    0.643371                        0.650864   \n",
       "3                    0.640987                        0.650196   \n",
       "4                    0.640987                        0.650196   \n",
       "5                    0.647310                        0.657280   \n",
       "6                    0.633744                        0.639080   \n",
       "7                    0.622259                        0.630362   \n",
       "8                    0.631858                        0.639868   \n",
       "\n",
       "   test accuracy steady avg2folds std across sessions  \\\n",
       "0                                           0.017992    \n",
       "1                                           0.015431    \n",
       "2                                           0.015441    \n",
       "3                                           0.017474    \n",
       "4                                           0.017474    \n",
       "5                                           0.016711    \n",
       "6                                           0.016364    \n",
       "7                                           0.018270    \n",
       "8                                           0.019702    \n",
       "\n",
       "   test accuracy steady avg2folds std across subjects  \\\n",
       "0                                           0.079836    \n",
       "1                                           0.120493    \n",
       "2                                           0.077375    \n",
       "3                                           0.076751    \n",
       "4                                           0.076751    \n",
       "5                                           0.074118    \n",
       "6                                           0.076835    \n",
       "7                                           0.073742    \n",
       "8                                           0.084600    \n",
       "\n",
       "   test accuracy steady session6 avg2folds  \\\n",
       "0                                 0.637813   \n",
       "1                                 0.605976   \n",
       "2                                 0.657441   \n",
       "3                                 0.658096   \n",
       "4                                 0.658096   \n",
       "5                                 0.665342   \n",
       "6                                 0.653273   \n",
       "7                                 0.639485   \n",
       "8                                 0.660336   \n",
       "\n",
       "   test accuracy steady session7 avg2folds  \\\n",
       "0                                 0.626278   \n",
       "1                                 0.599211   \n",
       "2                                 0.649427   \n",
       "3                                 0.651570   \n",
       "4                                 0.651570   \n",
       "5                                 0.657218   \n",
       "6                                 0.632209   \n",
       "7                                 0.624374   \n",
       "8                                 0.630016   \n",
       "\n",
       "   test accuracy steady session8 avg2folds  \\\n",
       "0                                 0.665994   \n",
       "1                                 0.625779   \n",
       "2                                 0.675946   \n",
       "3                                 0.677182   \n",
       "4                                 0.677182   \n",
       "5                                 0.684159   \n",
       "6                                 0.662876   \n",
       "7                                 0.659964   \n",
       "8                                 0.666708   \n",
       "\n",
       "   test accuracy steady session9 avg2folds  \\\n",
       "0                                 0.630395   \n",
       "1                                 0.578535   \n",
       "2                                 0.630234   \n",
       "3                                 0.625867   \n",
       "4                                 0.625867   \n",
       "5                                 0.640325   \n",
       "6                                 0.619054   \n",
       "7                                 0.605693   \n",
       "8                                 0.619943   \n",
       "\n",
       "   test accuracy steady session10 avg2folds  \\\n",
       "0                                  0.611449   \n",
       "1                                  0.594354   \n",
       "2                                  0.641269   \n",
       "3                                  0.638264   \n",
       "4                                  0.638264   \n",
       "5                                  0.639355   \n",
       "6                                  0.627990   \n",
       "7                                  0.622295   \n",
       "8                                  0.622335   \n",
       "\n",
       "   test accuracy steady subj0 avg2folds  test accuracy steady subj1 avg2folds  \\\n",
       "0                              0.741879                              0.675315   \n",
       "1                              0.755758                              0.392205   \n",
       "2                              0.753193                              0.656614   \n",
       "3                              0.749907                              0.651094   \n",
       "4                              0.749907                              0.651094   \n",
       "5                              0.758244                              0.673686   \n",
       "6                              0.742444                              0.582029   \n",
       "7                              0.739955                              0.579315   \n",
       "8                              0.749187                              0.571347   \n",
       "\n",
       "   test accuracy steady subj2 avg2folds  test accuracy steady subj3 avg2folds  \\\n",
       "0                              0.616850                              0.667227   \n",
       "1                              0.477707                              0.692919   \n",
       "2                              0.642311                              0.678129   \n",
       "3                              0.640269                              0.681674   \n",
       "4                              0.640269                              0.681674   \n",
       "5                              0.650122                              0.675123   \n",
       "6                              0.618263                              0.677145   \n",
       "7                              0.604076                              0.661725   \n",
       "8                              0.633124                              0.689213   \n",
       "\n",
       "   test accuracy steady subj4 avg2folds  test accuracy steady subj5 avg2folds  \\\n",
       "0                              0.784742                              0.636111   \n",
       "1                              0.783042                              0.472039   \n",
       "2                              0.806790                              0.618921   \n",
       "3                              0.805669                              0.619584   \n",
       "4                              0.805669                              0.619584   \n",
       "5                              0.811746                              0.603400   \n",
       "6                              0.803733                              0.614512   \n",
       "7                              0.781150                              0.612923   \n",
       "8                              0.809166                              0.621342   \n",
       "\n",
       "   test accuracy steady subj6 avg2folds  test accuracy steady subj7 avg2folds  \\\n",
       "0                              0.575114                              0.555989   \n",
       "1                              0.645453                              0.617244   \n",
       "2                              0.641813                              0.594206   \n",
       "3                              0.639422                              0.599455   \n",
       "4                              0.639422                              0.599455   \n",
       "5                              0.640404                              0.604953   \n",
       "6                              0.629365                              0.589930   \n",
       "7                              0.626735                              0.588007   \n",
       "8                              0.621781                              0.605645   \n",
       "\n",
       "   test accuracy steady subj8 avg2folds  test accuracy steady subj9 avg2folds  \\\n",
       "0                              0.523119                              0.567511   \n",
       "1                              0.558191                              0.613153   \n",
       "2                              0.520928                              0.595732   \n",
       "3                              0.519571                              0.595313   \n",
       "4                              0.519571                              0.595313   \n",
       "5                              0.549219                              0.605902   \n",
       "6                              0.531256                              0.602128   \n",
       "7                              0.524532                              0.585203   \n",
       "8                              0.496953                              0.600921   \n",
       "\n",
       "                   test preds steady subj0 avg2folds  \\\n",
       "0  [43683, 8300, 7663, 10186, 4802, 8483, 8844, 5...   \n",
       "1  [42382, 9906, 7322, 8294, 4515, 10634, 9355, 4...   \n",
       "2  [42537, 9700, 7024, 8530, 5099, 11005, 8204, 5...   \n",
       "3  [42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...   \n",
       "4  [42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...   \n",
       "5  [42338, 9750, 8110, 8274, 4124, 10339, 9045, 5...   \n",
       "6  [42763, 9884, 6540, 7986, 5341, 11634, 8277, 4...   \n",
       "7  [42699, 10313, 6503, 8072, 5559, 10826, 8270, ...   \n",
       "8  [41816, 9942, 7648, 9023, 4181, 10607, 8476, 5...   \n",
       "\n",
       "                   test preds steady subj1 avg2folds  \\\n",
       "0  [37096, 7620, 4609, 6389, 15561, 10865, 8635, ...   \n",
       "1                       [97068, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [35400, 7959, 2862, 4531, 15576, 10961, 9577, ...   \n",
       "3  [35379, 6848, 3072, 3653, 15247, 11575, 9471, ...   \n",
       "4  [35379, 6848, 3072, 3653, 15247, 11575, 9471, ...   \n",
       "5  [35840, 8146, 3298, 4105, 14213, 11276, 9060, ...   \n",
       "6  [34757, 5072, 4645, 2849, 8946, 12170, 11238, ...   \n",
       "7  [33820, 6053, 3058, 3572, 11857, 10220, 11483,...   \n",
       "8  [32376, 6707, 6017, 3522, 8234, 12767, 10004, ...   \n",
       "\n",
       "                   test preds steady subj2 avg2folds  \\\n",
       "0  [52591, 5152, 9735, 4958, 7867, 6127, 5779, 5193]   \n",
       "1       [83428, 5730, 3614, 1221, 636, 2398, 7, 368]   \n",
       "2  [50162, 4469, 9065, 5241, 6647, 9932, 7253, 4633]   \n",
       "3  [51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]   \n",
       "4  [51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]   \n",
       "5  [51233, 4384, 7781, 5462, 6944, 10979, 5926, 4...   \n",
       "6  [52517, 3482, 5792, 4520, 8249, 11040, 7358, 4...   \n",
       "7  [51735, 3903, 6662, 4609, 9566, 9573, 5992, 5362]   \n",
       "8  [48248, 4144, 7882, 5102, 8106, 11834, 7572, 4...   \n",
       "\n",
       "                   test preds steady subj3 avg2folds  \\\n",
       "0  [45926, 7769, 5405, 7001, 6477, 10985, 6543, 6...   \n",
       "1  [44084, 7843, 7732, 6714, 4990, 11060, 7849, 6...   \n",
       "2  [46081, 8482, 5517, 6489, 6605, 10703, 7144, 5...   \n",
       "3  [45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...   \n",
       "4  [45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...   \n",
       "5  [45355, 8938, 6188, 7020, 5351, 10843, 7402, 5...   \n",
       "6  [45997, 8902, 6995, 5306, 6338, 11202, 6193, 6...   \n",
       "7  [46162, 9521, 6221, 4640, 5340, 9920, 8623, 6587]   \n",
       "8  [43316, 8883, 10423, 6545, 5150, 10360, 5358, ...   \n",
       "\n",
       "                   test preds steady subj4 avg2folds  \\\n",
       "0  [52599, 8356, 7453, 7592, 4948, 4856, 7237, 4289]   \n",
       "1  [52022, 9207, 7626, 8803, 4678, 5942, 4941, 4111]   \n",
       "2  [52839, 7440, 7690, 8170, 4698, 6871, 5663, 3959]   \n",
       "3  [52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]   \n",
       "4  [52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]   \n",
       "5  [52878, 7240, 8270, 8382, 4259, 6466, 5014, 4821]   \n",
       "6  [53122, 8011, 7682, 8706, 4690, 5983, 5349, 3787]   \n",
       "7  [53149, 6896, 9564, 7236, 4932, 5975, 5737, 3841]   \n",
       "8  [52617, 7905, 7206, 7575, 5678, 6796, 5156, 4397]   \n",
       "\n",
       "                   test preds steady subj5 avg2folds  \\\n",
       "0  [44874, 5145, 4933, 5653, 6904, 13364, 10596, ...   \n",
       "1  [51263, 2941, 8735, 2120, 4437, 8606, 16523, 2...   \n",
       "2  [42317, 5905, 5788, 6042, 4677, 15283, 11544, ...   \n",
       "3  [43045, 5811, 6303, 6441, 4370, 15096, 10855, ...   \n",
       "4  [43045, 5811, 6303, 6441, 4370, 15096, 10855, ...   \n",
       "5  [41994, 5708, 6966, 6063, 5336, 14776, 11007, ...   \n",
       "6  [43770, 4952, 5148, 7286, 5583, 11404, 13372, ...   \n",
       "7  [44580, 4537, 5143, 7133, 6190, 11474, 12305, ...   \n",
       "8  [40215, 6041, 3975, 5769, 5032, 14118, 13994, ...   \n",
       "\n",
       "                   test preds steady subj6 avg2folds  \\\n",
       "0  [39292, 8151, 9642, 4945, 11295, 7753, 11643, ...   \n",
       "1  [37519, 8339, 11970, 9627, 7565, 7951, 9157, 5...   \n",
       "2  [37716, 8323, 10726, 10230, 7541, 8194, 10243,...   \n",
       "3  [37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...   \n",
       "4  [37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...   \n",
       "5  [37493, 8224, 11889, 9761, 7863, 8306, 9706, 4...   \n",
       "6  [38270, 7926, 11250, 8812, 8968, 8645, 9469, 4...   \n",
       "7  [38615, 8252, 10522, 8485, 8517, 7893, 10872, ...   \n",
       "8  [36032, 7341, 11815, 9392, 7073, 11002, 10771,...   \n",
       "\n",
       "                   test preds steady subj7 avg2folds  \\\n",
       "0  [61414, 10642, 5421, 6018, 4191, 1553, 5291, 3...   \n",
       "1  [53888, 7964, 6869, 5838, 5333, 1970, 7512, 8568]   \n",
       "2  [56188, 8111, 6442, 4636, 6892, 2265, 8314, 5094]   \n",
       "3  [56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]   \n",
       "4  [56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]   \n",
       "5  [55129, 8449, 5846, 5249, 6157, 2015, 8532, 6565]   \n",
       "6  [54102, 8807, 6210, 4601, 6443, 2529, 9947, 5303]   \n",
       "7  [57376, 8582, 6284, 4872, 5460, 2182, 8291, 4895]   \n",
       "8  [46711, 11186, 9334, 5658, 6321, 3994, 8224, 6...   \n",
       "\n",
       "                   test preds steady subj8 avg2folds  \\\n",
       "0  [74741, 5096, 6328, 2890, 3584, 1901, 3079, 1449]   \n",
       "1  [73465, 6928, 3997, 3855, 3498, 2091, 3130, 2104]   \n",
       "2  [67523, 9115, 5183, 2529, 3712, 4363, 3772, 2871]   \n",
       "3  [65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...   \n",
       "4  [65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...   \n",
       "5  [72810, 5945, 4523, 3268, 4115, 3117, 3415, 1875]   \n",
       "6  [70170, 5699, 6323, 2988, 4986, 3545, 3158, 2199]   \n",
       "7  [69951, 6045, 4176, 3032, 4994, 3835, 3783, 3252]   \n",
       "8  [59424, 14255, 7353, 4847, 3272, 3603, 4086, 2...   \n",
       "\n",
       "                   test preds steady subj9 avg2folds  \n",
       "0  [43748, 4575, 11866, 8372, 9800, 8609, 4053, 6...  \n",
       "1  [40454, 4862, 6168, 8926, 15658, 10874, 5963, ...  \n",
       "2  [44283, 3920, 4881, 6871, 14326, 12657, 5057, ...  \n",
       "3  [43242, 4183, 4748, 6365, 14667, 13749, 5124, ...  \n",
       "4  [43242, 4183, 4748, 6365, 14667, 13749, 5124, ...  \n",
       "5  [42707, 4534, 4659, 7412, 15824, 11516, 4982, ...  \n",
       "6  [43862, 4354, 4846, 7864, 14067, 11695, 5131, ...  \n",
       "7  [44523, 4327, 6397, 7783, 12398, 11383, 5023, ...  \n",
       "8  [39423, 5625, 7140, 7692, 12475, 11916, 5798, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_pretrain_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_pretrain_9b = read_results(\"exp9/pretrain9.5b/results_1631301862.pickle\", {\"pretraining\": 'all_others_9.5', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'})\n",
    "vit_pretrain_9c = read_results(\"exp9/pretrain9.5c/results_1631304147.pickle\", {\"pretraining\": 'all_others_9.5', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>dim_projection</th>\n",
       "      <th>dim_ff</th>\n",
       "      <th>dim_head</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>depth</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dropout</th>\n",
       "      <th>MACs</th>\n",
       "      <th>params</th>\n",
       "      <th>params_aff_ratio</th>\n",
       "      <th>pretraining</th>\n",
       "      <th>pretraining_epochs</th>\n",
       "      <th>finetune_epochs</th>\n",
       "      <th>train accuracy steady fold1</th>\n",
       "      <th>train accuracy steady fold2</th>\n",
       "      <th>train accuracy steady avg2folds</th>\n",
       "      <th>validation accuracy steady fold1</th>\n",
       "      <th>validation accuracy steady fold2</th>\n",
       "      <th>test accuracy fold1</th>\n",
       "      <th>test accuracy fold2</th>\n",
       "      <th>test accuracy avg2folds</th>\n",
       "      <th>test accuracy steady fold1</th>\n",
       "      <th>test accuracy steady fold2</th>\n",
       "      <th>test accuracy steady avg2folds</th>\n",
       "      <th>test accuracy steady avg2folds std across sessions</th>\n",
       "      <th>test accuracy steady avg2folds std across subjects</th>\n",
       "      <th>test accuracy steady session6 avg2folds</th>\n",
       "      <th>test accuracy steady session7 avg2folds</th>\n",
       "      <th>test accuracy steady session8 avg2folds</th>\n",
       "      <th>test accuracy steady session9 avg2folds</th>\n",
       "      <th>test accuracy steady session10 avg2folds</th>\n",
       "      <th>test accuracy steady subj0 avg2folds</th>\n",
       "      <th>test accuracy steady subj1 avg2folds</th>\n",
       "      <th>test accuracy steady subj2 avg2folds</th>\n",
       "      <th>test accuracy steady subj3 avg2folds</th>\n",
       "      <th>test accuracy steady subj4 avg2folds</th>\n",
       "      <th>test accuracy steady subj5 avg2folds</th>\n",
       "      <th>test accuracy steady subj6 avg2folds</th>\n",
       "      <th>test accuracy steady subj7 avg2folds</th>\n",
       "      <th>test accuracy steady subj8 avg2folds</th>\n",
       "      <th>test accuracy steady subj9 avg2folds</th>\n",
       "      <th>test preds steady subj0 avg2folds</th>\n",
       "      <th>test preds steady subj1 avg2folds</th>\n",
       "      <th>test preds steady subj2 avg2folds</th>\n",
       "      <th>test preds steady subj3 avg2folds</th>\n",
       "      <th>test preds steady subj4 avg2folds</th>\n",
       "      <th>test preds steady subj5 avg2folds</th>\n",
       "      <th>test preds steady subj6 avg2folds</th>\n",
       "      <th>test preds steady subj7 avg2folds</th>\n",
       "      <th>test preds steady subj8 avg2folds</th>\n",
       "      <th>test preds steady subj9 avg2folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>94152</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_9.5</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.932065</td>\n",
       "      <td>0.915645</td>\n",
       "      <td>0.923855</td>\n",
       "      <td>0.708056</td>\n",
       "      <td>0.716389</td>\n",
       "      <td>0.506473</td>\n",
       "      <td>0.494441</td>\n",
       "      <td>0.500457</td>\n",
       "      <td>0.667252</td>\n",
       "      <td>0.647776</td>\n",
       "      <td>0.657514</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.074592</td>\n",
       "      <td>0.665426</td>\n",
       "      <td>0.657406</td>\n",
       "      <td>0.684969</td>\n",
       "      <td>0.641145</td>\n",
       "      <td>0.638624</td>\n",
       "      <td>0.75909</td>\n",
       "      <td>0.674447</td>\n",
       "      <td>0.646438</td>\n",
       "      <td>0.674821</td>\n",
       "      <td>0.81366</td>\n",
       "      <td>0.600882</td>\n",
       "      <td>0.641972</td>\n",
       "      <td>0.606031</td>\n",
       "      <td>0.549132</td>\n",
       "      <td>0.608667</td>\n",
       "      <td>[42153, 9970, 7913, 7835, 4195, 10777, 9506, 4...</td>\n",
       "      <td>[36102, 8318, 3140, 4270, 14219, 10954, 8795, ...</td>\n",
       "      <td>[51602, 4234, 7483, 5509, 7375, 10686, 6052, 4...</td>\n",
       "      <td>[45401, 8771, 6631, 6750, 5203, 11136, 7484, 5...</td>\n",
       "      <td>[52869, 7504, 8083, 8359, 4142, 6619, 4987, 4767]</td>\n",
       "      <td>[41944, 5556, 7338, 6061, 5230, 14235, 11043, ...</td>\n",
       "      <td>[37456, 8539, 11193, 9798, 7646, 8642, 9696, 4...</td>\n",
       "      <td>[54723, 8444, 6060, 5018, 5919, 2229, 9256, 6293]</td>\n",
       "      <td>[73116, 5893, 4508, 3185, 3948, 3068, 3261, 2089]</td>\n",
       "      <td>[42202, 4899, 4911, 7442, 16119, 11169, 5074, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  patch_size  dim_projection  dim_ff  dim_head  n_heads  depth  \\\n",
       "0          300          10              64     128        32        8      1   \n",
       "\n",
       "   dropout  emb_dropout     MACs  params  params_aff_ratio     pretraining  \\\n",
       "0      0.2            0  3300864   94152          0.798287  all_others_9.5   \n",
       "\n",
       "   pretraining_epochs  finetune_epochs  train accuracy steady fold1  \\\n",
       "0                 100               20                     0.932065   \n",
       "\n",
       "   train accuracy steady fold2  train accuracy steady avg2folds  \\\n",
       "0                     0.915645                         0.923855   \n",
       "\n",
       "   validation accuracy steady fold1  validation accuracy steady fold2  \\\n",
       "0                          0.708056                          0.716389   \n",
       "\n",
       "   test accuracy fold1  test accuracy fold2  test accuracy avg2folds  \\\n",
       "0             0.506473             0.494441                 0.500457   \n",
       "\n",
       "   test accuracy steady fold1  test accuracy steady fold2  \\\n",
       "0                    0.667252                    0.647776   \n",
       "\n",
       "   test accuracy steady avg2folds  \\\n",
       "0                        0.657514   \n",
       "\n",
       "   test accuracy steady avg2folds std across sessions  \\\n",
       "0                                           0.016977    \n",
       "\n",
       "   test accuracy steady avg2folds std across subjects  \\\n",
       "0                                           0.074592    \n",
       "\n",
       "   test accuracy steady session6 avg2folds  \\\n",
       "0                                 0.665426   \n",
       "\n",
       "   test accuracy steady session7 avg2folds  \\\n",
       "0                                 0.657406   \n",
       "\n",
       "   test accuracy steady session8 avg2folds  \\\n",
       "0                                 0.684969   \n",
       "\n",
       "   test accuracy steady session9 avg2folds  \\\n",
       "0                                 0.641145   \n",
       "\n",
       "   test accuracy steady session10 avg2folds  \\\n",
       "0                                  0.638624   \n",
       "\n",
       "   test accuracy steady subj0 avg2folds  test accuracy steady subj1 avg2folds  \\\n",
       "0                               0.75909                              0.674447   \n",
       "\n",
       "   test accuracy steady subj2 avg2folds  test accuracy steady subj3 avg2folds  \\\n",
       "0                              0.646438                              0.674821   \n",
       "\n",
       "   test accuracy steady subj4 avg2folds  test accuracy steady subj5 avg2folds  \\\n",
       "0                               0.81366                              0.600882   \n",
       "\n",
       "   test accuracy steady subj6 avg2folds  test accuracy steady subj7 avg2folds  \\\n",
       "0                              0.641972                              0.606031   \n",
       "\n",
       "   test accuracy steady subj8 avg2folds  test accuracy steady subj9 avg2folds  \\\n",
       "0                              0.549132                              0.608667   \n",
       "\n",
       "                   test preds steady subj0 avg2folds  \\\n",
       "0  [42153, 9970, 7913, 7835, 4195, 10777, 9506, 4...   \n",
       "\n",
       "                   test preds steady subj1 avg2folds  \\\n",
       "0  [36102, 8318, 3140, 4270, 14219, 10954, 8795, ...   \n",
       "\n",
       "                   test preds steady subj2 avg2folds  \\\n",
       "0  [51602, 4234, 7483, 5509, 7375, 10686, 6052, 4...   \n",
       "\n",
       "                   test preds steady subj3 avg2folds  \\\n",
       "0  [45401, 8771, 6631, 6750, 5203, 11136, 7484, 5...   \n",
       "\n",
       "                   test preds steady subj4 avg2folds  \\\n",
       "0  [52869, 7504, 8083, 8359, 4142, 6619, 4987, 4767]   \n",
       "\n",
       "                   test preds steady subj5 avg2folds  \\\n",
       "0  [41944, 5556, 7338, 6061, 5230, 14235, 11043, ...   \n",
       "\n",
       "                   test preds steady subj6 avg2folds  \\\n",
       "0  [37456, 8539, 11193, 9798, 7646, 8642, 9696, 4...   \n",
       "\n",
       "                   test preds steady subj7 avg2folds  \\\n",
       "0  [54723, 8444, 6060, 5018, 5919, 2229, 9256, 6293]   \n",
       "\n",
       "                   test preds steady subj8 avg2folds  \\\n",
       "0  [73116, 5893, 4508, 3185, 3948, 3068, 3261, 2089]   \n",
       "\n",
       "                   test preds steady subj9 avg2folds  \n",
       "0  [42202, 4899, 4911, 7442, 16119, 11169, 5074, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_pretrain_9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_vit = \\\n",
    "    read_results(\"exp5/grid_5sess10subj/finetuneFrom100/results_1623845783.pickle\", {\"pretraining\": \"fold1\", \"pretraining_dropout\": .2, \"pretraining_epochs\": 100, \"finetune_epochs\": 20})\\\n",
    "    .append(read_results(\"exp6/grid_5sess10subj_dim64_p10/finetune_100/results_1624706075.pickle\", {\"pretraining_epochs\": 100, \"finetune_epochs\": 20}), ignore_index=True) \\\n",
    "    .append(read_results(\"exp6/grid_5sess10subj_dim64_p30/finetune_100/results_1624804897.pickle\", {\"pretraining_epochs\": 100, \"finetune_epochs\": 20}), ignore_index=True) \\\n",
    "    .append(read_results(\"exp8/grid_5sess10subj_patches/finetune_100/results_1626031724.pickle\", {\"pretraining_epochs\": 100, \"finetune_epochs\": 20}), ignore_index=True) \\\n",
    "    .append(read_results(\"exp9/grid_5sess10subj_patches/finetune_100/results_1626498530.pickle\", {\"pretraining_epochs\": 100, \"finetune_epochs\": 20}), ignore_index=True) \\\n",
    "    .fillna(value={\"pretraining\": \"fold1\", \"pretraining_dropout\": .2, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = get_rows(all_res_vit, \n",
    "               {'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1,\n",
    "                'pretraining_dropout': .2, 'pretraining_epochs': 100, 'pretraining': 'fold1'}) \\\n",
    "        .append(get_rows(all_res_vit, \n",
    "               {'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 2, 'depth': 2,\n",
    "                'pretraining_dropout': .2, 'pretraining_epochs': 100, 'pretraining': 'fold1'}))\n",
    "\n",
    "res_vit_pretrain = res_.sort_values(by=['n_heads', 'patch_size'])\n",
    "\n",
    "loc = res_vit_pretrain[(res_vit_pretrain['patch_size'] == 10) & (res_vit_pretrain['n_heads'] == 8)].index[0]\n",
    "res_vit_pretrain.loc[loc] = get_rows(vit_pretrain_9,  {'pretraining': 'all_others_9.5', 'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1}).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_tcn = read_results(\"exp7/tcn_pretrain/results_1625075271.pickle\", {\"pretraining\": 'fold1'}, model_name='temponet') \\\n",
    ".append(read_results(\"exp7/tcn_pretrain/results_1625166399.pickle\", {\"pretraining\": 'no'}, model_name='temponet'), ignore_index=True) \\\n",
    ".append(read_results(\"exp7/tcn_pretrain/results_1626361771.pickle\", {\"pretraining\": 'fold2'}, model_name='temponet'), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = get_rows(all_res_tcn, \n",
    "               {'pretraining': 'fold1'}) \\\n",
    "        .append(get_rows(all_res_tcn, \n",
    "               {'pretraining': 'no'}))\n",
    "\n",
    "res_tcn = res_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_vit_2 = \\\n",
    "    read_results(\"exp10/no_pretraining_/results_1630304612.pickle\", {\"pretraining\": False}) \\\n",
    "    .append(read_results(\"exp10/no_pretraining_/results_1630304773.pickle\", {\"pretraining\": False}), ignore_index=True) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_untrained = get_rows(all_res_vit_2, \n",
    "               {'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1,\n",
    "                'pretraining': False}) \\\n",
    "        .append(get_rows(all_res_vit_2, \n",
    "               {'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 2, 'depth': 2,\n",
    "                'pretraining': False}))\n",
    "\n",
    "res_vit_nopretrain = res_untrained.sort_values(by=['n_heads', 'patch_size']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = pd.ExcelWriter('results_.xlsx', engine='xlsxwriter')\n",
    "from styleframe import StyleFrame, Styler\n",
    "writer = StyleFrame.ExcelWriter('results_complete.xlsx')\n",
    "\n",
    "def write_excel(df, name):\n",
    "    sf = StyleFrame(df, Styler(border_type=None, fill_pattern_type=None))\n",
    "    sf.to_excel(writer, sheet_name=name, best_fit=list(df.columns))\n",
    "\n",
    "write_excel(res_vit_nopretrain, 'ViT NoPretraining')\n",
    "write_excel(res_vit_pretrain, 'ViT Pretraining')\n",
    "write_excel(res_tcn, 'TEMPONET')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "from styleframe import StyleFrame, Styler\n",
    "writer = StyleFrame.ExcelWriter('results_mini.xlsx')\n",
    "\n",
    "cols = ['validation accuracy steady fold1'] + ['test accuracy steady avg2folds'] + [f'test accuracy steady session{s} avg2folds' for s in [6, 7, 8, 9, 10]]\n",
    "\n",
    "res_vit_nopretrain['model'] = 'ViT'\n",
    "res_vit_pretrain['model'] = 'ViT'\n",
    "res_tcn['model'] = 'TEMPONet'\n",
    "\n",
    "res_mac_params = res_vit_nopretrain[['model', 'patch_size', 'n_heads', 'MACs', 'params']].append(res_tcn[['model', 'MACs', 'params']].head(1))\n",
    "\n",
    "res_vit_pretrain_mini = res_vit_pretrain[['model', 'pretraining', 'patch_size', 'n_heads'] + cols].rename(columns={'validation accuracy steady fold1': 'validation accuracy steady'})\n",
    "res_vit_pretrain_mini['pretraining'] = True\n",
    "res_vit_nopretrain_mini = res_vit_nopretrain[['model', 'pretraining', 'patch_size', 'n_heads'] + cols].rename(columns={'validation accuracy steady fold1': 'validation accuracy steady'})\n",
    "res_tcn_mini = res_tcn[['model', 'pretraining'] + cols].copy().rename(columns={'validation accuracy steady fold1': 'validation accuracy steady'})\n",
    "res_tcn_mini['pretraining'] = res_tcn_mini['pretraining'].str.replace('fold1', 'True')\n",
    "\n",
    "res_accs = res_vit_nopretrain_mini.append(res_vit_pretrain_mini).append(res_tcn_mini)\n",
    "res_accs.to_csv('a.csv', index=False)\n",
    "res_accs = pd.read_csv('a.csv')\n",
    "write_excel(res_accs, 'Accuracies')\n",
    "\n",
    "res_mac_params.to_csv('a.csv', index=False)\n",
    "res_mac_params = pd.read_csv('a.csv')\n",
    "write_excel(res_mac_params, 'MACsParams')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_ = res_vit_pretrain.copy()\n",
    "res_untrained = res_vit_nopretrain.copy()\n",
    "tcn = all_res_tcn.loc[all_res_tcn[all_res_tcn['pretraining'] == 'fold1'].index[0]]\n",
    "tcn_macs, tcn_params, tcn_test_accuracy_steady_avg2folds = tcn['MACs'], tcn['params'], tcn['test accuracy steady avg2folds']\n",
    "tcn_nop = all_res_tcn.loc[all_res_tcn[all_res_tcn['pretraining'] == 'no'].index[0]]\n",
    "tcn_macs, tcn_params, tcn_test_accuracy_steady_avg2folds_nop = tcn['MACs'], tcn['params'], tcn_nop['test accuracy steady avg2folds']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, depth in enumerate([1, 2]):\n",
    "    r = res_[res_['depth'] == depth]\n",
    "    plt.plot([str(x) for x in r['patch_size']], list(r['test accuracy steady avg2folds']), marker='^', label=f\"ViT heads={8 if depth == 1 else 2}, depth={depth} w/ pretraining\", color=f\"C{i}\", linestyle='-')\n",
    "    r = res_untrained[res_untrained['depth'] == depth]\n",
    "    plt.plot([str(x) for x in r['patch_size']], list(r['test accuracy steady avg2folds']), marker='o', label=f\"ViT heads={8 if depth == 1 else 2}, depth={depth}\", color=f\"C{i}\", linestyle='--')\n",
    "\n",
    "plt.hlines(tcn_test_accuracy_steady_avg2folds, 0, 4, color='C2', label='TEMPONet w/ pretraining')\n",
    "plt.hlines(tcn_test_accuracy_steady_avg2folds_nop, 0, 4, linestyle='--', color='C2', label='TEMPONet')\n",
    "\n",
    "plt.grid('on')\n",
    "plt.xlabel('patch size')\n",
    "#plt.suptitle('ViT 2 head')\n",
    "plt.title('5 session steady accuracy')\n",
    "#plt.legend(title=\"depth\", loc=\"upper right\", bbox_to_anchor=(1.3, 1.02))\n",
    "plt.legend(title=\"Model\", loc=\"upper left\", bbox_to_anchor=(1, 1.025))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "pretraining = get_rows(vit_pretrain_9,  {'pretraining': 'all_others_9.5', 'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1})\n",
    "pretraining_subjects = np.array(pretraining[[f'test accuracy steady subj{s} avg2folds' for s in range(10)]].iloc[0])\n",
    "nopretraining = get_rows(vit_pretrain_9,  {'pretraining': 'no', 'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1})\n",
    "nopretraining_subjects = np.array(nopretraining[[f'test accuracy steady subj{s} avg2folds' for s in range(10)]].iloc[0])\n",
    "\n",
    "fig1 = pd.DataFrame({'subject': np.arange(1, 11) ,'accuracy': nopretraining_subjects, 'accuracy pretraining': pretraining_subjects})\n",
    "fig2 = res_accs[['model', 'pretraining', 'patch_size', 'n_heads', 'test accuracy steady avg2folds']]\n",
    "fig3 = res_mac_params\n",
    "fig4 = res_accs[(res_accs['model'] == 'ViT') & (res_accs['patch_size'] == 10) | (res_accs['model'] == 'TEMPONet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "from styleframe import StyleFrame, Styler\n",
    "writer = StyleFrame.ExcelWriter('results2.xlsx')\n",
    "\n",
    "write_excel(fig1, 'Pretraining Subjects')\n",
    "write_excel(fig2, 'Patch dim Acc')\n",
    "write_excel(fig3, 'MACs Params')\n",
    "write_excel(fig4, 'Testing sessions')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "# fix wrong pretrain\n",
    "pretraining = get_rows(res_vit_pretrain,  {'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1})\n",
    "pretraining_subjects = np.array(pretraining[[f'test accuracy steady subj{s} avg2folds' for s in range(10)]].iloc[0])\n",
    "nopretraining = get_rows(res_vit_nopretrain,  {'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1})\n",
    "nopretraining_subjects = np.array(nopretraining[[f'test accuracy steady subj{s} avg2folds' for s in range(10)]].iloc[0])\n",
    "\n",
    "fig1 = pd.DataFrame({'subject': np.arange(1, 11) ,'accuracy': nopretraining_subjects, 'accuracy pretraining': pretraining_subjects})\n",
    "fig2 = res_accs[['model', 'pretraining', 'patch_size', 'n_heads', 'test accuracy steady avg2folds']]\n",
    "fig3 = res_mac_params\n",
    "fig4 = res_accs[(res_accs['model'] == 'ViT') & (res_accs['patch_size'] == 10) | (res_accs['model'] == 'TEMPONet')]\n",
    "\n",
    "from styleframe import StyleFrame, Styler\n",
    "writer = StyleFrame.ExcelWriter('results2.xlsx')\n",
    "\n",
    "write_excel(fig1, 'Pretraining Subjects')\n",
    "write_excel(fig2, 'Patch dim Acc')\n",
    "write_excel(fig3, 'MACs Params')\n",
    "write_excel(fig4, 'Testing sessions')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\francesco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning:`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "accs = read_results(\"exp10/check_pretrain9.5/results_1631106374.pickle\", group_exclude_columns={'pretrained'})\n",
    "accs.iloc[0].to_excel('aa.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
