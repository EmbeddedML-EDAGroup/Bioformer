{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(n_tokens, dim, dim_head):\n",
    "    macs = 0\n",
    "    \n",
    "    # token -> k, q, v\n",
    "    macs += n_tokens * 3 * dim * dim_head\n",
    "    \n",
    "    # q * k'\n",
    "    # (n_tokens, dim_head) * (dim_head, n_tokens) -> (n_tokens, n_tokens)\n",
    "    macs += n_tokens * dim_head * n_tokens\n",
    "    \n",
    "    # Softmax e diviso sqrt(dim_head) \n",
    "    # ...\n",
    "    \n",
    "    # (q * k') * v\n",
    "    # (n_tokens, n_tokens) * (n_tokens, dim_head) -> (n_tokens, dim_head)\n",
    "    macs += n_tokens * n_tokens * dim_head\n",
    "    \n",
    "    return macs\n",
    "    \n",
    "def attention(n_tokens, dim, dim_head, n_heads):\n",
    "    macs = 0\n",
    "    \n",
    "    macs += n_heads * attention_head(n_tokens, dim, dim_head)\n",
    "    \n",
    "    # Riporta gli z concatenati a dimensione dim\n",
    "    macs += n_tokens * (dim_head * n_heads) * dim if not (n_heads == 1 and dim_head == dim) else 0\n",
    "    \n",
    "    return macs\n",
    "\n",
    "def feed_forward(n_tokens, dim, mlp_dim):\n",
    "    # 2 Linear: dim -> mlp_dim, mlp_dim -> dim\n",
    "    return n_tokens * dim * mlp_dim * 2\n",
    "\n",
    "def transformer(n_tokens, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    return depth * (attention(n_tokens, dim, dim_head, n_heads) + feed_forward(n_tokens, dim, mlp_dim))\n",
    "\n",
    "def vit(patch_size, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "               \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = qkv.chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., use_cls_token=True):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        #self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "        #nn.init.kaiming_uniform_(self.pos_embedding, a=5 ** .5)\n",
    "        nn.init.normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "        #self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.zeros_(self.cls_token)\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # MACs: patch_size * n_patches * dim, es (30 * 14) * 10 * 300\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # FeedForward    Attention       project out\n",
    "        # 300*300*10*2 + 300*(64*3)*10 + ((64)*300*10)\n",
    "        # Attention -> manca softmax e attention vera e propria, c'è solo linear encoding a qkv\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        \n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "    \n",
    "# Ratio of params\n",
    "def vit_aff_ratio(patch_size, dim, dim_head, n_heads, mlp_dim, depth): \n",
    "    n_tokens = 300 // patch_size + 1\n",
    "    \n",
    "    a = (((dim) * dim_head * 3 * n_heads) + ((dim_head * n_heads) * dim) + dim)\n",
    "    ff = dim * mlp_dim * 2 + mlp_dim + dim\n",
    "    \n",
    "    return a / (a + ff)\n",
    "\n",
    "def get_results(configs, results_, additional_columns, extract_model_hparams):\n",
    "        \n",
    "    acccs = []\n",
    "    acccs_steady = []\n",
    "    acccs_val0 = []\n",
    "    acccs_steady_val0 = []\n",
    "    acccs_val1 = []\n",
    "    acccs_steady_val1 = []\n",
    "\n",
    "    acccs_val_val0 = 0\n",
    "    acccs_val_val1 = 0\n",
    "    acccs_train_val0 = 0\n",
    "    acccs_train_val1 = 0\n",
    "    \n",
    "    acccs_steady_persubject = np.array([0] * 10, dtype=float)\n",
    "    acccs_steady_persubject_f1 = np.array([0] * 10, dtype=float)\n",
    "    acccs_steady_persubject_f2 = np.array([0] * 10, dtype=float)\n",
    "    preds_steady_bincounts_subject = np.zeros((10, 8), dtype=int)\n",
    "\n",
    "    for config, r in zip(configs, results_):\n",
    "\n",
    "        accs = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "            accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        accs /= 2\n",
    "\n",
    "        accs_steady = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        accs_steady /= 2\n",
    "\n",
    "        accs_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject_f1[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        accs_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "            accs_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "            accs_steady_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject_f2[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        acccs_train_val0 += r['val-fold_0']['losses_accs'][-1]['train_acc']\n",
    "        acccs_train_val1 += r['val-fold_1']['losses_accs'][-1]['train_acc']\n",
    "\n",
    "        acccs_val_val0 += r['val-fold_0']['losses_accs'][-1]['val_acc']\n",
    "        acccs_val_val1 += r['val-fold_1']['losses_accs'][-1]['val_acc']\n",
    "\n",
    "        acccs.append(accs)\n",
    "        acccs_steady.append(accs_steady)\n",
    "        acccs_val0.append(accs_val0)\n",
    "        acccs_steady_val0.append(accs_steady_val0)\n",
    "        acccs_val1.append(accs_val1)\n",
    "        acccs_steady_val1.append(accs_steady_val1)\n",
    "\n",
    "    acccs_steady_persubject /= 10 # 5 sessioni per due\n",
    "    acccs_steady_persubject_f1 /= 5\n",
    "    acccs_steady_persubject_f2 /= 5\n",
    "\n",
    "    acccs = np.array(acccs).mean(axis=0)\n",
    "\n",
    "    acccs_steady_ = np.array(acccs_steady).mean(axis=1)\n",
    "    acccs_steady = np.array(acccs_steady).mean(axis=0)\n",
    "\n",
    "    acccs_val0 = np.array(acccs_val0).mean(axis=0)\n",
    "    acccs_val1 = np.array(acccs_val1).mean(axis=0)\n",
    "    acccs_steady_val0 = np.array(acccs_steady_val0).mean(axis=0)\n",
    "    acccs_steady_val1 = np.array(acccs_steady_val1).mean(axis=0)\n",
    "    acccs_val_val0 /= 10\n",
    "    acccs_val_val1 /= 10\n",
    "    acccs_train_val0 /= 10\n",
    "    acccs_train_val1 /= 10\n",
    "    \n",
    "    model_hparams = extract_model_hparams(config)\n",
    "\n",
    "    return {        \n",
    "        **model_hparams,\n",
    "\n",
    "        **additional_columns,\n",
    "\n",
    "        \"train accuracy steady fold1\":  acccs_train_val0,\n",
    "        \"train accuracy steady fold2\":  acccs_train_val1,\n",
    "        \"train accuracy steady avg2folds\": .5 * (acccs_train_val0 + acccs_train_val1),\n",
    "\n",
    "        \"validation accuracy steady fold1\": acccs_val_val0,\n",
    "        \"validation accuracy steady fold2\": acccs_val_val1,\n",
    "\n",
    "        \"test accuracy fold1\": acccs_val0.mean(),\n",
    "        \"test accuracy fold2\": acccs_val1.mean(),\n",
    "        \"test accuracy avg2folds\": acccs.mean(), \n",
    "        \"test accuracy steady fold1\": acccs_steady_val0.mean(),\n",
    "        \"test accuracy steady fold2\": acccs_steady_val1.mean(), \n",
    "        \"test accuracy steady avg2folds\": acccs_steady.mean(),\n",
    "\n",
    "        \"test accuracy steady avg2folds std across sessions\": acccs_steady.std(),\n",
    "        \"test accuracy steady avg2folds std across subjects\": acccs_steady_.std(),\n",
    "        \n",
    "        **{ f\"test accuracy steady subj{s} fold1\": acccs_steady_persubject_f1[s] for s in range(10)},\n",
    "        **{ f\"test accuracy steady subj{s} fold2\": acccs_steady_persubject_f2[s] for s in range(10)},\n",
    "        **{ f\"test accuracy steady subj{s} avg2folds\": acccs_steady_persubject[s] for s in range(10)},\n",
    "        \n",
    "        **{\n",
    "            f\"test preds steady subj{s} avg2folds\": preds_steady_bincounts_subject[s] for s in range(10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_configs(configs, group_exclude_columns):\n",
    "    # https://stackoverflow.com/a/6027615\n",
    "    import collections.abc\n",
    "\n",
    "    def flatten(d, parent_key='', sep='_'):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = parent_key + sep + k if parent_key else k\n",
    "            if isinstance(v, collections.abc.MutableMapping):\n",
    "                items.extend(flatten(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    df = pd.DataFrame([flatten(config) for config in configs])\n",
    "    df['row_id'] = [[a] for a in df.index]\n",
    "    \n",
    "    if 'training_config_lr_scheduler_hparams_milestones' in df:\n",
    "        df['training_config_lr_scheduler_hparams_milestones'] = df['training_config_lr_scheduler_hparams_milestones'].apply(lambda x: ','.join(map(str, x)))\n",
    "    df = df.fillna('null')\n",
    "    \n",
    "    grouped_configs = df.groupby([c for c in df.columns if c not in group_exclude_columns]).agg({'subjects': 'count', 'row_id': 'sum'})\n",
    "    \n",
    "    if (grouped_configs['subjects'] != 10).sum() != 0:\n",
    "        display(grouped_configs)\n",
    "        raise ValueError(\"For every config, it is assumed that you trained on 10 subjects\")\n",
    "    \n",
    "    return list(grouped_configs[\"row_id\"])\n",
    "\n",
    "\n",
    "extract_model_hparams_generator = {\n",
    "    'vit': lambda config: {\n",
    "        \"window_size\": config[\"image_size\"][1],\n",
    "        \"patch_size\": config[\"patch_size\"][1],\n",
    "        \"dim_projection\": config[\"dim\"],\n",
    "        \"dim_ff\": config[\"mlp_dim\"],\n",
    "        \"dim_head\": config[\"dim_head\"],\n",
    "        \"n_heads\": config[\"heads\"],\n",
    "        \"depth\": config[\"depth\"],\n",
    "        \"dropout\": config[\"dropout\"],\n",
    "        \"emb_dropout\": config[\"emb_dropout\"],\n",
    "        \n",
    "        \"MACs\": vit(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \"params\":  sum([param.nelement() for param in ViT(image_size=(1, 300), patch_size=config[\"patch_size\"], dim=config[\"dim\"], dim_head=config[\"dim_head\"], heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"], num_classes=8).parameters()]),\n",
    "        \"params_aff_ratio\": vit_aff_ratio(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \n",
    "    },\n",
    "    'temponet': lambda _: {\n",
    "        \"MACs\": 16028672,\n",
    "        \"params\": 461512,\n",
    "    },\n",
    "}\n",
    "\n",
    "def read_results(filename, additional_columns=None, group_exclude_columns=None, model_name='vit'):\n",
    "    additional_columns = {} if additional_columns is None else additional_columns\n",
    "    \n",
    "    group_exclude_columns = set() if group_exclude_columns is None else group_exclude_columns\n",
    "    group_exclude_columns = group_exclude_columns.union({'subjects', 'row_id'})\n",
    "    \n",
    "    configs, results_ = load(open(filename, 'rb'))\n",
    "    \n",
    "    groups_indices = group_configs(configs, group_exclude_columns)\n",
    "    \n",
    "    df_l = []\n",
    "    for idx in groups_indices:\n",
    "        c = [configs[i] for i in idx]\n",
    "        r = [results_[i] for i in idx]\n",
    "        \n",
    "        df_l.append(get_results(c, r, additional_columns, extract_model_hparams_generator[model_name]))   \n",
    "    \n",
    "    return pd.DataFrame(df_l) \n",
    "\n",
    "def get_rows(all_res_vit, group):\n",
    "    m = None\n",
    "    for k in group.keys():\n",
    "        current_m = all_res_vit[k] == group[k]\n",
    "        if m is None:\n",
    "            m = current_m\n",
    "        else:\n",
    "            m &= current_m\n",
    "    return all_res_vit[m].copy()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit_pretrain_9 = read_results(\"exp9/grid_5sess10subj_nopretraining/finetune_0/results_1626391555.pickle\", {\"pretraining\": 'no', \"pretraining_epochs\": 0, \"finetune_epochs\": 200}, group_exclude_columns={'pretrained'}) \\\n",
    "vit_pretrain_9 = read_results(\"exp9/grid_5sess10subj_nopretraining_fold2/results_1627417222.pickle\", {\"pretraining\": 'no', \"pretraining_epochs\": 0, \"finetune_epochs\": 200}, group_exclude_columns={'pretrained'}) \\\n",
    "        .append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100/results_1626421922.pickle\", {\"pretraining\": 'all_others_20', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100_fix/results_1627588086.pickle\", {\"pretraining\": 'all_others_20_fix', \"pretraining_epochs\": 100, \"finetune_epochs\": 40}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100_fix_10/results_1627593378.pickle\", {\"pretraining\": 'all_others_20_fix_10', \"pretraining_epochs\": 100, \"finetune_epochs\": 40}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    ".append(read_results(\"exp9/grid_5sess10subj_pretraining/finetune_100_fix_10bs8/results_1627593378.pickle\", {\"pretraining\": 'all_others_20_fix_10bs8', \"pretraining_epochs\": 100, \"finetune_epochs\": 40}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    ".append(read_results(\"exp9/pretrain9.5/results_1627617535.pickle\", {\"pretraining\": 'all_others_9.5', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    ".append(read_results(\"exp9/pretrain9.5_hd/results_1628515671.pickle\", {\"pretraining\": 'all_others_9.5_hd', \"pretraining_epochs\": 100, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/vit_pretraining_9/results_1627311468.pickle\", {\"pretraining\": 'all_others_min20-20', \"pretraining_epochs\": 100, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/vit_pretraining_9/results_1627333294.pickle\", {\"pretraining\": 'all_others_min20-20from40', \"pretraining_epochs\": 40, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/vit_pretraining_9/results_1627341881.pickle\", {\"pretraining\": 'all_others_hd', \"pretraining_epochs\": 100, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>dim_projection</th>\n",
       "      <th>dim_ff</th>\n",
       "      <th>dim_head</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>depth</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dropout</th>\n",
       "      <th>MACs</th>\n",
       "      <th>params</th>\n",
       "      <th>params_aff_ratio</th>\n",
       "      <th>pretraining</th>\n",
       "      <th>pretraining_epochs</th>\n",
       "      <th>finetune_epochs</th>\n",
       "      <th>train accuracy steady fold1</th>\n",
       "      <th>train accuracy steady fold2</th>\n",
       "      <th>train accuracy steady avg2folds</th>\n",
       "      <th>validation accuracy steady fold1</th>\n",
       "      <th>validation accuracy steady fold2</th>\n",
       "      <th>test accuracy fold1</th>\n",
       "      <th>test accuracy fold2</th>\n",
       "      <th>test accuracy avg2folds</th>\n",
       "      <th>test accuracy steady fold1</th>\n",
       "      <th>test accuracy steady fold2</th>\n",
       "      <th>test accuracy steady avg2folds</th>\n",
       "      <th>test accuracy steady avg2folds std across sessions</th>\n",
       "      <th>test accuracy steady avg2folds std across subjects</th>\n",
       "      <th>test accuracy steady subj0 fold1</th>\n",
       "      <th>test accuracy steady subj1 fold1</th>\n",
       "      <th>test accuracy steady subj2 fold1</th>\n",
       "      <th>test accuracy steady subj3 fold1</th>\n",
       "      <th>test accuracy steady subj4 fold1</th>\n",
       "      <th>test accuracy steady subj5 fold1</th>\n",
       "      <th>test accuracy steady subj6 fold1</th>\n",
       "      <th>test accuracy steady subj7 fold1</th>\n",
       "      <th>test accuracy steady subj8 fold1</th>\n",
       "      <th>test accuracy steady subj9 fold1</th>\n",
       "      <th>test accuracy steady subj0 fold2</th>\n",
       "      <th>test accuracy steady subj1 fold2</th>\n",
       "      <th>test accuracy steady subj2 fold2</th>\n",
       "      <th>test accuracy steady subj3 fold2</th>\n",
       "      <th>test accuracy steady subj4 fold2</th>\n",
       "      <th>test accuracy steady subj5 fold2</th>\n",
       "      <th>test accuracy steady subj6 fold2</th>\n",
       "      <th>test accuracy steady subj7 fold2</th>\n",
       "      <th>test accuracy steady subj8 fold2</th>\n",
       "      <th>test accuracy steady subj9 fold2</th>\n",
       "      <th>test accuracy steady subj0 avg2folds</th>\n",
       "      <th>test accuracy steady subj1 avg2folds</th>\n",
       "      <th>test accuracy steady subj2 avg2folds</th>\n",
       "      <th>test accuracy steady subj3 avg2folds</th>\n",
       "      <th>test accuracy steady subj4 avg2folds</th>\n",
       "      <th>test accuracy steady subj5 avg2folds</th>\n",
       "      <th>test accuracy steady subj6 avg2folds</th>\n",
       "      <th>test accuracy steady subj7 avg2folds</th>\n",
       "      <th>test accuracy steady subj8 avg2folds</th>\n",
       "      <th>test accuracy steady subj9 avg2folds</th>\n",
       "      <th>test preds steady subj0 avg2folds</th>\n",
       "      <th>test preds steady subj1 avg2folds</th>\n",
       "      <th>test preds steady subj2 avg2folds</th>\n",
       "      <th>test preds steady subj3 avg2folds</th>\n",
       "      <th>test preds steady subj4 avg2folds</th>\n",
       "      <th>test preds steady subj5 avg2folds</th>\n",
       "      <th>test preds steady subj6 avg2folds</th>\n",
       "      <th>test preds steady subj7 avg2folds</th>\n",
       "      <th>test preds steady subj8 avg2folds</th>\n",
       "      <th>test preds steady subj9 avg2folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.974234</td>\n",
       "      <td>0.969996</td>\n",
       "      <td>0.972115</td>\n",
       "      <td>0.691544</td>\n",
       "      <td>0.691096</td>\n",
       "      <td>0.481805</td>\n",
       "      <td>0.477129</td>\n",
       "      <td>0.479467</td>\n",
       "      <td>0.639890</td>\n",
       "      <td>0.628881</td>\n",
       "      <td>0.634386</td>\n",
       "      <td>0.017992</td>\n",
       "      <td>0.079836</td>\n",
       "      <td>0.736907</td>\n",
       "      <td>0.696560</td>\n",
       "      <td>0.629511</td>\n",
       "      <td>0.671779</td>\n",
       "      <td>0.788001</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.580213</td>\n",
       "      <td>0.567286</td>\n",
       "      <td>0.514041</td>\n",
       "      <td>0.574422</td>\n",
       "      <td>0.746852</td>\n",
       "      <td>0.654069</td>\n",
       "      <td>0.604190</td>\n",
       "      <td>0.662675</td>\n",
       "      <td>0.781484</td>\n",
       "      <td>0.632039</td>\n",
       "      <td>0.570015</td>\n",
       "      <td>0.544692</td>\n",
       "      <td>0.532196</td>\n",
       "      <td>0.560601</td>\n",
       "      <td>0.741879</td>\n",
       "      <td>0.675315</td>\n",
       "      <td>0.616850</td>\n",
       "      <td>0.667227</td>\n",
       "      <td>0.784742</td>\n",
       "      <td>0.636111</td>\n",
       "      <td>0.575114</td>\n",
       "      <td>0.555989</td>\n",
       "      <td>0.523119</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>[43683, 8300, 7663, 10186, 4802, 8483, 8844, 5...</td>\n",
       "      <td>[37096, 7620, 4609, 6389, 15561, 10865, 8635, ...</td>\n",
       "      <td>[52591, 5152, 9735, 4958, 7867, 6127, 5779, 5193]</td>\n",
       "      <td>[45926, 7769, 5405, 7001, 6477, 10985, 6543, 6...</td>\n",
       "      <td>[52599, 8356, 7453, 7592, 4948, 4856, 7237, 4289]</td>\n",
       "      <td>[44874, 5145, 4933, 5653, 6904, 13364, 10596, ...</td>\n",
       "      <td>[39292, 8151, 9642, 4945, 11295, 7753, 11643, ...</td>\n",
       "      <td>[61414, 10642, 5421, 6018, 4191, 1553, 5291, 3...</td>\n",
       "      <td>[74741, 5096, 6328, 2890, 3584, 1901, 3079, 1449]</td>\n",
       "      <td>[43748, 4575, 11866, 8372, 9800, 8609, 4053, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.792333</td>\n",
       "      <td>0.737745</td>\n",
       "      <td>0.765039</td>\n",
       "      <td>0.655784</td>\n",
       "      <td>0.660054</td>\n",
       "      <td>0.460341</td>\n",
       "      <td>0.444448</td>\n",
       "      <td>0.452395</td>\n",
       "      <td>0.608122</td>\n",
       "      <td>0.593420</td>\n",
       "      <td>0.600771</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.120493</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.392205</td>\n",
       "      <td>0.504509</td>\n",
       "      <td>0.697832</td>\n",
       "      <td>0.788357</td>\n",
       "      <td>0.501278</td>\n",
       "      <td>0.645255</td>\n",
       "      <td>0.626278</td>\n",
       "      <td>0.553240</td>\n",
       "      <td>0.615790</td>\n",
       "      <td>0.755040</td>\n",
       "      <td>0.392205</td>\n",
       "      <td>0.450906</td>\n",
       "      <td>0.688006</td>\n",
       "      <td>0.777727</td>\n",
       "      <td>0.442801</td>\n",
       "      <td>0.645650</td>\n",
       "      <td>0.608209</td>\n",
       "      <td>0.563142</td>\n",
       "      <td>0.610516</td>\n",
       "      <td>0.755758</td>\n",
       "      <td>0.392205</td>\n",
       "      <td>0.477707</td>\n",
       "      <td>0.692919</td>\n",
       "      <td>0.783042</td>\n",
       "      <td>0.472039</td>\n",
       "      <td>0.645453</td>\n",
       "      <td>0.617244</td>\n",
       "      <td>0.558191</td>\n",
       "      <td>0.613153</td>\n",
       "      <td>[42382, 9906, 7322, 8294, 4515, 10634, 9355, 4...</td>\n",
       "      <td>[97068, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[83428, 5730, 3614, 1221, 636, 2398, 7, 368]</td>\n",
       "      <td>[44084, 7843, 7732, 6714, 4990, 11060, 7849, 6...</td>\n",
       "      <td>[52022, 9207, 7626, 8803, 4678, 5942, 4941, 4111]</td>\n",
       "      <td>[51263, 2941, 8735, 2120, 4437, 8606, 16523, 2...</td>\n",
       "      <td>[37519, 8339, 11970, 9627, 7565, 7951, 9157, 5...</td>\n",
       "      <td>[53888, 7964, 6869, 5838, 5333, 1970, 7512, 8568]</td>\n",
       "      <td>[73465, 6928, 3997, 3855, 3498, 2091, 3130, 2104]</td>\n",
       "      <td>[40454, 4862, 6168, 8926, 15658, 10874, 5963, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20_fix</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.955310</td>\n",
       "      <td>0.942484</td>\n",
       "      <td>0.948897</td>\n",
       "      <td>0.708360</td>\n",
       "      <td>0.714751</td>\n",
       "      <td>0.499362</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.658357</td>\n",
       "      <td>0.643371</td>\n",
       "      <td>0.650864</td>\n",
       "      <td>0.015441</td>\n",
       "      <td>0.077375</td>\n",
       "      <td>0.764687</td>\n",
       "      <td>0.689336</td>\n",
       "      <td>0.656671</td>\n",
       "      <td>0.680088</td>\n",
       "      <td>0.815507</td>\n",
       "      <td>0.629179</td>\n",
       "      <td>0.645338</td>\n",
       "      <td>0.605415</td>\n",
       "      <td>0.493640</td>\n",
       "      <td>0.603708</td>\n",
       "      <td>0.741699</td>\n",
       "      <td>0.623893</td>\n",
       "      <td>0.627952</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>0.798073</td>\n",
       "      <td>0.608663</td>\n",
       "      <td>0.638288</td>\n",
       "      <td>0.582998</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.587755</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0.656614</td>\n",
       "      <td>0.642311</td>\n",
       "      <td>0.678129</td>\n",
       "      <td>0.806790</td>\n",
       "      <td>0.618921</td>\n",
       "      <td>0.641813</td>\n",
       "      <td>0.594206</td>\n",
       "      <td>0.520928</td>\n",
       "      <td>0.595732</td>\n",
       "      <td>[42537, 9700, 7024, 8530, 5099, 11005, 8204, 5...</td>\n",
       "      <td>[35400, 7959, 2862, 4531, 15576, 10961, 9577, ...</td>\n",
       "      <td>[50162, 4469, 9065, 5241, 6647, 9932, 7253, 4633]</td>\n",
       "      <td>[46081, 8482, 5517, 6489, 6605, 10703, 7144, 5...</td>\n",
       "      <td>[52839, 7440, 7690, 8170, 4698, 6871, 5663, 3959]</td>\n",
       "      <td>[42317, 5905, 5788, 6042, 4677, 15283, 11544, ...</td>\n",
       "      <td>[37716, 8323, 10726, 10230, 7541, 8194, 10243,...</td>\n",
       "      <td>[56188, 8111, 6442, 4636, 6892, 2265, 8314, 5094]</td>\n",
       "      <td>[67523, 9115, 5183, 2529, 3712, 4363, 3772, 2871]</td>\n",
       "      <td>[44283, 3920, 4881, 6871, 14326, 12657, 5057, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20_fix_10</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.930601</td>\n",
       "      <td>0.913502</td>\n",
       "      <td>0.922051</td>\n",
       "      <td>0.703747</td>\n",
       "      <td>0.712814</td>\n",
       "      <td>0.500719</td>\n",
       "      <td>0.486503</td>\n",
       "      <td>0.493611</td>\n",
       "      <td>0.659405</td>\n",
       "      <td>0.640987</td>\n",
       "      <td>0.650196</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.076751</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>0.679730</td>\n",
       "      <td>0.666113</td>\n",
       "      <td>0.688631</td>\n",
       "      <td>0.816523</td>\n",
       "      <td>0.624783</td>\n",
       "      <td>0.645669</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.486907</td>\n",
       "      <td>0.606161</td>\n",
       "      <td>0.735913</td>\n",
       "      <td>0.622459</td>\n",
       "      <td>0.614426</td>\n",
       "      <td>0.674718</td>\n",
       "      <td>0.794815</td>\n",
       "      <td>0.614385</td>\n",
       "      <td>0.633174</td>\n",
       "      <td>0.583274</td>\n",
       "      <td>0.552234</td>\n",
       "      <td>0.584466</td>\n",
       "      <td>0.749907</td>\n",
       "      <td>0.651094</td>\n",
       "      <td>0.640269</td>\n",
       "      <td>0.681674</td>\n",
       "      <td>0.805669</td>\n",
       "      <td>0.619584</td>\n",
       "      <td>0.639422</td>\n",
       "      <td>0.599455</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>[42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...</td>\n",
       "      <td>[35379, 6848, 3072, 3653, 15247, 11575, 9471, ...</td>\n",
       "      <td>[51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]</td>\n",
       "      <td>[45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...</td>\n",
       "      <td>[52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]</td>\n",
       "      <td>[43045, 5811, 6303, 6441, 4370, 15096, 10855, ...</td>\n",
       "      <td>[37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...</td>\n",
       "      <td>[56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]</td>\n",
       "      <td>[65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...</td>\n",
       "      <td>[43242, 4183, 4748, 6365, 14667, 13749, 5124, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_20_fix_10bs8</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.930601</td>\n",
       "      <td>0.913502</td>\n",
       "      <td>0.922051</td>\n",
       "      <td>0.703747</td>\n",
       "      <td>0.712814</td>\n",
       "      <td>0.500719</td>\n",
       "      <td>0.486503</td>\n",
       "      <td>0.493611</td>\n",
       "      <td>0.659405</td>\n",
       "      <td>0.640987</td>\n",
       "      <td>0.650196</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.076751</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>0.679730</td>\n",
       "      <td>0.666113</td>\n",
       "      <td>0.688631</td>\n",
       "      <td>0.816523</td>\n",
       "      <td>0.624783</td>\n",
       "      <td>0.645669</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.486907</td>\n",
       "      <td>0.606161</td>\n",
       "      <td>0.735913</td>\n",
       "      <td>0.622459</td>\n",
       "      <td>0.614426</td>\n",
       "      <td>0.674718</td>\n",
       "      <td>0.794815</td>\n",
       "      <td>0.614385</td>\n",
       "      <td>0.633174</td>\n",
       "      <td>0.583274</td>\n",
       "      <td>0.552234</td>\n",
       "      <td>0.584466</td>\n",
       "      <td>0.749907</td>\n",
       "      <td>0.651094</td>\n",
       "      <td>0.640269</td>\n",
       "      <td>0.681674</td>\n",
       "      <td>0.805669</td>\n",
       "      <td>0.619584</td>\n",
       "      <td>0.639422</td>\n",
       "      <td>0.599455</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>[42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...</td>\n",
       "      <td>[35379, 6848, 3072, 3653, 15247, 11575, 9471, ...</td>\n",
       "      <td>[51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]</td>\n",
       "      <td>[45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...</td>\n",
       "      <td>[52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]</td>\n",
       "      <td>[43045, 5811, 6303, 6441, 4370, 15096, 10855, ...</td>\n",
       "      <td>[37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...</td>\n",
       "      <td>[56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]</td>\n",
       "      <td>[65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...</td>\n",
       "      <td>[43242, 4183, 4748, 6365, 14667, 13749, 5124, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_9.5</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.931951</td>\n",
       "      <td>0.914357</td>\n",
       "      <td>0.923154</td>\n",
       "      <td>0.707964</td>\n",
       "      <td>0.716697</td>\n",
       "      <td>0.505896</td>\n",
       "      <td>0.494344</td>\n",
       "      <td>0.500120</td>\n",
       "      <td>0.667250</td>\n",
       "      <td>0.647310</td>\n",
       "      <td>0.657280</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.074118</td>\n",
       "      <td>0.761989</td>\n",
       "      <td>0.701850</td>\n",
       "      <td>0.667855</td>\n",
       "      <td>0.684414</td>\n",
       "      <td>0.816908</td>\n",
       "      <td>0.609418</td>\n",
       "      <td>0.645556</td>\n",
       "      <td>0.623734</td>\n",
       "      <td>0.545258</td>\n",
       "      <td>0.615514</td>\n",
       "      <td>0.754498</td>\n",
       "      <td>0.645521</td>\n",
       "      <td>0.632389</td>\n",
       "      <td>0.665833</td>\n",
       "      <td>0.806584</td>\n",
       "      <td>0.597381</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>0.586171</td>\n",
       "      <td>0.553179</td>\n",
       "      <td>0.596289</td>\n",
       "      <td>0.758244</td>\n",
       "      <td>0.673686</td>\n",
       "      <td>0.650122</td>\n",
       "      <td>0.675123</td>\n",
       "      <td>0.811746</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.640404</td>\n",
       "      <td>0.604953</td>\n",
       "      <td>0.549219</td>\n",
       "      <td>0.605902</td>\n",
       "      <td>[42338, 9750, 8110, 8274, 4124, 10339, 9045, 5...</td>\n",
       "      <td>[35840, 8146, 3298, 4105, 14213, 11276, 9060, ...</td>\n",
       "      <td>[51233, 4384, 7781, 5462, 6944, 10979, 5926, 4...</td>\n",
       "      <td>[45355, 8938, 6188, 7020, 5351, 10843, 7402, 5...</td>\n",
       "      <td>[52878, 7240, 8270, 8382, 4259, 6466, 5014, 4821]</td>\n",
       "      <td>[41994, 5708, 6966, 6063, 5336, 14776, 11007, ...</td>\n",
       "      <td>[37493, 8224, 11889, 9761, 7863, 8306, 9706, 4...</td>\n",
       "      <td>[55129, 8449, 5846, 5249, 6157, 2015, 8532, 6565]</td>\n",
       "      <td>[72810, 5945, 4523, 3268, 4115, 3117, 3415, 1875]</td>\n",
       "      <td>[42707, 4534, 4659, 7412, 15824, 11516, 4982, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_9.5_hd</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.896446</td>\n",
       "      <td>0.873192</td>\n",
       "      <td>0.884819</td>\n",
       "      <td>0.686342</td>\n",
       "      <td>0.699925</td>\n",
       "      <td>0.500337</td>\n",
       "      <td>0.487183</td>\n",
       "      <td>0.493760</td>\n",
       "      <td>0.658028</td>\n",
       "      <td>0.638721</td>\n",
       "      <td>0.648375</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>0.074101</td>\n",
       "      <td>0.750657</td>\n",
       "      <td>0.673564</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.679461</td>\n",
       "      <td>0.814140</td>\n",
       "      <td>0.617562</td>\n",
       "      <td>0.649049</td>\n",
       "      <td>0.613809</td>\n",
       "      <td>0.523773</td>\n",
       "      <td>0.605864</td>\n",
       "      <td>0.737159</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.613754</td>\n",
       "      <td>0.652122</td>\n",
       "      <td>0.806063</td>\n",
       "      <td>0.620553</td>\n",
       "      <td>0.631080</td>\n",
       "      <td>0.591007</td>\n",
       "      <td>0.548044</td>\n",
       "      <td>0.574731</td>\n",
       "      <td>0.743908</td>\n",
       "      <td>0.643132</td>\n",
       "      <td>0.633080</td>\n",
       "      <td>0.665792</td>\n",
       "      <td>0.810101</td>\n",
       "      <td>0.619058</td>\n",
       "      <td>0.640065</td>\n",
       "      <td>0.602408</td>\n",
       "      <td>0.535908</td>\n",
       "      <td>0.590298</td>\n",
       "      <td>[42740, 9586, 8540, 7955, 3682, 10972, 8790, 4...</td>\n",
       "      <td>[35422, 6131, 2203, 3514, 14683, 11990, 10532,...</td>\n",
       "      <td>[50305, 5060, 5951, 5653, 8967, 10627, 5580, 5...</td>\n",
       "      <td>[44805, 7726, 4660, 3805, 6079, 14332, 8838, 6...</td>\n",
       "      <td>[52454, 6913, 8193, 6476, 5585, 6832, 5691, 5186]</td>\n",
       "      <td>[43513, 5866, 6583, 6224, 5029, 12336, 12767, ...</td>\n",
       "      <td>[38518, 7955, 10339, 8922, 8214, 7670, 11086, ...</td>\n",
       "      <td>[54105, 8388, 9494, 4944, 6205, 1470, 6946, 6390]</td>\n",
       "      <td>[70010, 7791, 4756, 3682, 3633, 3343, 3205, 2648]</td>\n",
       "      <td>[42112, 4435, 6936, 8710, 10835, 12487, 5142, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_min20-20</td>\n",
       "      <td>100</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.960346</td>\n",
       "      <td>0.950499</td>\n",
       "      <td>0.955423</td>\n",
       "      <td>0.711452</td>\n",
       "      <td>0.719812</td>\n",
       "      <td>0.489861</td>\n",
       "      <td>0.482169</td>\n",
       "      <td>0.486015</td>\n",
       "      <td>0.644417</td>\n",
       "      <td>0.633744</td>\n",
       "      <td>0.639080</td>\n",
       "      <td>0.016364</td>\n",
       "      <td>0.076835</td>\n",
       "      <td>0.752237</td>\n",
       "      <td>0.602903</td>\n",
       "      <td>0.633066</td>\n",
       "      <td>0.681501</td>\n",
       "      <td>0.803236</td>\n",
       "      <td>0.625959</td>\n",
       "      <td>0.626234</td>\n",
       "      <td>0.598717</td>\n",
       "      <td>0.513240</td>\n",
       "      <td>0.607080</td>\n",
       "      <td>0.732650</td>\n",
       "      <td>0.561155</td>\n",
       "      <td>0.603461</td>\n",
       "      <td>0.672789</td>\n",
       "      <td>0.804229</td>\n",
       "      <td>0.603066</td>\n",
       "      <td>0.632496</td>\n",
       "      <td>0.581143</td>\n",
       "      <td>0.549272</td>\n",
       "      <td>0.597175</td>\n",
       "      <td>0.742444</td>\n",
       "      <td>0.582029</td>\n",
       "      <td>0.618263</td>\n",
       "      <td>0.677145</td>\n",
       "      <td>0.803733</td>\n",
       "      <td>0.614512</td>\n",
       "      <td>0.629365</td>\n",
       "      <td>0.589930</td>\n",
       "      <td>0.531256</td>\n",
       "      <td>0.602128</td>\n",
       "      <td>[42763, 9884, 6540, 7986, 5341, 11634, 8277, 4...</td>\n",
       "      <td>[34757, 5072, 4645, 2849, 8946, 12170, 11238, ...</td>\n",
       "      <td>[52517, 3482, 5792, 4520, 8249, 11040, 7358, 4...</td>\n",
       "      <td>[45997, 8902, 6995, 5306, 6338, 11202, 6193, 6...</td>\n",
       "      <td>[53122, 8011, 7682, 8706, 4690, 5983, 5349, 3787]</td>\n",
       "      <td>[43770, 4952, 5148, 7286, 5583, 11404, 13372, ...</td>\n",
       "      <td>[38270, 7926, 11250, 8812, 8968, 8645, 9469, 4...</td>\n",
       "      <td>[54102, 8807, 6210, 4601, 6443, 2529, 9947, 5303]</td>\n",
       "      <td>[70170, 5699, 6323, 2988, 4986, 3545, 3158, 2199]</td>\n",
       "      <td>[43862, 4354, 4846, 7864, 14067, 11695, 5131, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_min20-20from40</td>\n",
       "      <td>40</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.939094</td>\n",
       "      <td>0.927953</td>\n",
       "      <td>0.933524</td>\n",
       "      <td>0.693335</td>\n",
       "      <td>0.697301</td>\n",
       "      <td>0.483659</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>0.478230</td>\n",
       "      <td>0.638465</td>\n",
       "      <td>0.622259</td>\n",
       "      <td>0.630362</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>0.073742</td>\n",
       "      <td>0.747301</td>\n",
       "      <td>0.601740</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.672813</td>\n",
       "      <td>0.787248</td>\n",
       "      <td>0.618401</td>\n",
       "      <td>0.632908</td>\n",
       "      <td>0.608418</td>\n",
       "      <td>0.501350</td>\n",
       "      <td>0.593253</td>\n",
       "      <td>0.732608</td>\n",
       "      <td>0.556889</td>\n",
       "      <td>0.586934</td>\n",
       "      <td>0.650636</td>\n",
       "      <td>0.775051</td>\n",
       "      <td>0.607445</td>\n",
       "      <td>0.620563</td>\n",
       "      <td>0.567597</td>\n",
       "      <td>0.547715</td>\n",
       "      <td>0.577154</td>\n",
       "      <td>0.739955</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.604076</td>\n",
       "      <td>0.661725</td>\n",
       "      <td>0.781150</td>\n",
       "      <td>0.612923</td>\n",
       "      <td>0.626735</td>\n",
       "      <td>0.588007</td>\n",
       "      <td>0.524532</td>\n",
       "      <td>0.585203</td>\n",
       "      <td>[42699, 10313, 6503, 8072, 5559, 10826, 8270, ...</td>\n",
       "      <td>[33820, 6053, 3058, 3572, 11857, 10220, 11483,...</td>\n",
       "      <td>[51735, 3903, 6662, 4609, 9566, 9573, 5992, 5362]</td>\n",
       "      <td>[46162, 9521, 6221, 4640, 5340, 9920, 8623, 6587]</td>\n",
       "      <td>[53149, 6896, 9564, 7236, 4932, 5975, 5737, 3841]</td>\n",
       "      <td>[44580, 4537, 5143, 7133, 6190, 11474, 12305, ...</td>\n",
       "      <td>[38615, 8252, 10522, 8485, 8517, 7893, 10872, ...</td>\n",
       "      <td>[57376, 8582, 6284, 4872, 5460, 2182, 8291, 4895]</td>\n",
       "      <td>[69951, 6045, 4176, 3032, 4994, 3835, 3783, 3252]</td>\n",
       "      <td>[44523, 4327, 6397, 7783, 12398, 11383, 5023, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3300864</td>\n",
       "      <td>87112</td>\n",
       "      <td>0.798287</td>\n",
       "      <td>all_others_hd</td>\n",
       "      <td>100</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.903182</td>\n",
       "      <td>0.878725</td>\n",
       "      <td>0.890954</td>\n",
       "      <td>0.693769</td>\n",
       "      <td>0.700088</td>\n",
       "      <td>0.497503</td>\n",
       "      <td>0.485919</td>\n",
       "      <td>0.491711</td>\n",
       "      <td>0.647878</td>\n",
       "      <td>0.631858</td>\n",
       "      <td>0.639868</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.762331</td>\n",
       "      <td>0.586218</td>\n",
       "      <td>0.665903</td>\n",
       "      <td>0.694925</td>\n",
       "      <td>0.819441</td>\n",
       "      <td>0.622791</td>\n",
       "      <td>0.622481</td>\n",
       "      <td>0.621162</td>\n",
       "      <td>0.464880</td>\n",
       "      <td>0.618643</td>\n",
       "      <td>0.736043</td>\n",
       "      <td>0.556476</td>\n",
       "      <td>0.600345</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.798890</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>0.621080</td>\n",
       "      <td>0.590127</td>\n",
       "      <td>0.529026</td>\n",
       "      <td>0.583198</td>\n",
       "      <td>0.749187</td>\n",
       "      <td>0.571347</td>\n",
       "      <td>0.633124</td>\n",
       "      <td>0.689213</td>\n",
       "      <td>0.809166</td>\n",
       "      <td>0.621342</td>\n",
       "      <td>0.621781</td>\n",
       "      <td>0.605645</td>\n",
       "      <td>0.496953</td>\n",
       "      <td>0.600921</td>\n",
       "      <td>[41816, 9942, 7648, 9023, 4181, 10607, 8476, 5...</td>\n",
       "      <td>[32376, 6707, 6017, 3522, 8234, 12767, 10004, ...</td>\n",
       "      <td>[48248, 4144, 7882, 5102, 8106, 11834, 7572, 4...</td>\n",
       "      <td>[43316, 8883, 10423, 6545, 5150, 10360, 5358, ...</td>\n",
       "      <td>[52617, 7905, 7206, 7575, 5678, 6796, 5156, 4397]</td>\n",
       "      <td>[40215, 6041, 3975, 5769, 5032, 14118, 13994, ...</td>\n",
       "      <td>[36032, 7341, 11815, 9392, 7073, 11002, 10771,...</td>\n",
       "      <td>[46711, 11186, 9334, 5658, 6321, 3994, 8224, 6...</td>\n",
       "      <td>[59424, 14255, 7353, 4847, 3272, 3603, 4086, 2...</td>\n",
       "      <td>[39423, 5625, 7140, 7692, 12475, 11916, 5798, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  patch_size  dim_projection  dim_ff  dim_head  n_heads  depth  \\\n",
       "0          300          10              64     128        32        8      1   \n",
       "1          300          10              64     128        32        8      1   \n",
       "2          300          10              64     128        32        8      1   \n",
       "3          300          10              64     128        32        8      1   \n",
       "4          300          10              64     128        32        8      1   \n",
       "5          300          10              64     128        32        8      1   \n",
       "6          300          10              64     128        32        8      1   \n",
       "7          300          10              64     128        32        8      1   \n",
       "8          300          10              64     128        32        8      1   \n",
       "9          300          10              64     128        32        8      1   \n",
       "\n",
       "   dropout  emb_dropout     MACs  params  params_aff_ratio  \\\n",
       "0      0.2            0  3300864   87112          0.798287   \n",
       "1      0.2            0  3300864   87112          0.798287   \n",
       "2      0.2            0  3300864   87112          0.798287   \n",
       "3      0.2            0  3300864   87112          0.798287   \n",
       "4      0.2            0  3300864   87112          0.798287   \n",
       "5      0.2            0  3300864   87112          0.798287   \n",
       "6      0.2            0  3300864   87112          0.798287   \n",
       "7      0.2            0  3300864   87112          0.798287   \n",
       "8      0.2            0  3300864   87112          0.798287   \n",
       "9      0.5            0  3300864   87112          0.798287   \n",
       "\n",
       "                 pretraining  pretraining_epochs finetune_epochs  \\\n",
       "0                         no                   0             200   \n",
       "1              all_others_20                 100              20   \n",
       "2          all_others_20_fix                 100              40   \n",
       "3       all_others_20_fix_10                 100              40   \n",
       "4    all_others_20_fix_10bs8                 100              40   \n",
       "5             all_others_9.5                 100              20   \n",
       "6          all_others_9.5_hd                 100              20   \n",
       "7        all_others_min20-20                 100        min20-20   \n",
       "8  all_others_min20-20from40                  40        min20-20   \n",
       "9              all_others_hd                 100        min20-20   \n",
       "\n",
       "   train accuracy steady fold1  train accuracy steady fold2  \\\n",
       "0                     0.974234                     0.969996   \n",
       "1                     0.792333                     0.737745   \n",
       "2                     0.955310                     0.942484   \n",
       "3                     0.930601                     0.913502   \n",
       "4                     0.930601                     0.913502   \n",
       "5                     0.931951                     0.914357   \n",
       "6                     0.896446                     0.873192   \n",
       "7                     0.960346                     0.950499   \n",
       "8                     0.939094                     0.927953   \n",
       "9                     0.903182                     0.878725   \n",
       "\n",
       "   train accuracy steady avg2folds  validation accuracy steady fold1  \\\n",
       "0                         0.972115                          0.691544   \n",
       "1                         0.765039                          0.655784   \n",
       "2                         0.948897                          0.708360   \n",
       "3                         0.922051                          0.703747   \n",
       "4                         0.922051                          0.703747   \n",
       "5                         0.923154                          0.707964   \n",
       "6                         0.884819                          0.686342   \n",
       "7                         0.955423                          0.711452   \n",
       "8                         0.933524                          0.693335   \n",
       "9                         0.890954                          0.693769   \n",
       "\n",
       "   validation accuracy steady fold2  test accuracy fold1  test accuracy fold2  \\\n",
       "0                          0.691096             0.481805             0.477129   \n",
       "1                          0.660054             0.460341             0.444448   \n",
       "2                          0.714751             0.499362             0.488291   \n",
       "3                          0.712814             0.500719             0.486503   \n",
       "4                          0.712814             0.500719             0.486503   \n",
       "5                          0.716697             0.505896             0.494344   \n",
       "6                          0.699925             0.500337             0.487183   \n",
       "7                          0.719812             0.489861             0.482169   \n",
       "8                          0.697301             0.483659             0.472801   \n",
       "9                          0.700088             0.497503             0.485919   \n",
       "\n",
       "   test accuracy avg2folds  test accuracy steady fold1  \\\n",
       "0                 0.479467                    0.639890   \n",
       "1                 0.452395                    0.608122   \n",
       "2                 0.493827                    0.658357   \n",
       "3                 0.493611                    0.659405   \n",
       "4                 0.493611                    0.659405   \n",
       "5                 0.500120                    0.667250   \n",
       "6                 0.493760                    0.658028   \n",
       "7                 0.486015                    0.644417   \n",
       "8                 0.478230                    0.638465   \n",
       "9                 0.491711                    0.647878   \n",
       "\n",
       "   test accuracy steady fold2  test accuracy steady avg2folds  \\\n",
       "0                    0.628881                        0.634386   \n",
       "1                    0.593420                        0.600771   \n",
       "2                    0.643371                        0.650864   \n",
       "3                    0.640987                        0.650196   \n",
       "4                    0.640987                        0.650196   \n",
       "5                    0.647310                        0.657280   \n",
       "6                    0.638721                        0.648375   \n",
       "7                    0.633744                        0.639080   \n",
       "8                    0.622259                        0.630362   \n",
       "9                    0.631858                        0.639868   \n",
       "\n",
       "   test accuracy steady avg2folds std across sessions  \\\n",
       "0                                           0.017992    \n",
       "1                                           0.015431    \n",
       "2                                           0.015441    \n",
       "3                                           0.017474    \n",
       "4                                           0.017474    \n",
       "5                                           0.016711    \n",
       "6                                           0.013775    \n",
       "7                                           0.016364    \n",
       "8                                           0.018270    \n",
       "9                                           0.019702    \n",
       "\n",
       "   test accuracy steady avg2folds std across subjects  \\\n",
       "0                                           0.079836    \n",
       "1                                           0.120493    \n",
       "2                                           0.077375    \n",
       "3                                           0.076751    \n",
       "4                                           0.076751    \n",
       "5                                           0.074118    \n",
       "6                                           0.074101    \n",
       "7                                           0.076835    \n",
       "8                                           0.073742    \n",
       "9                                           0.084600    \n",
       "\n",
       "   test accuracy steady subj0 fold1  test accuracy steady subj1 fold1  \\\n",
       "0                          0.736907                          0.696560   \n",
       "1                          0.756475                          0.392205   \n",
       "2                          0.764687                          0.689336   \n",
       "3                          0.763900                          0.679730   \n",
       "4                          0.763900                          0.679730   \n",
       "5                          0.761989                          0.701850   \n",
       "6                          0.750657                          0.673564   \n",
       "7                          0.752237                          0.602903   \n",
       "8                          0.747301                          0.601740   \n",
       "9                          0.762331                          0.586218   \n",
       "\n",
       "   test accuracy steady subj2 fold1  test accuracy steady subj3 fold1  \\\n",
       "0                          0.629511                          0.671779   \n",
       "1                          0.504509                          0.697832   \n",
       "2                          0.656671                          0.680088   \n",
       "3                          0.666113                          0.688631   \n",
       "4                          0.666113                          0.688631   \n",
       "5                          0.667855                          0.684414   \n",
       "6                          0.652406                          0.679461   \n",
       "7                          0.633066                          0.681501   \n",
       "8                          0.621219                          0.672813   \n",
       "9                          0.665903                          0.694925   \n",
       "\n",
       "   test accuracy steady subj4 fold1  test accuracy steady subj5 fold1  \\\n",
       "0                          0.788001                          0.640184   \n",
       "1                          0.788357                          0.501278   \n",
       "2                          0.815507                          0.629179   \n",
       "3                          0.816523                          0.624783   \n",
       "4                          0.816523                          0.624783   \n",
       "5                          0.816908                          0.609418   \n",
       "6                          0.814140                          0.617562   \n",
       "7                          0.803236                          0.625959   \n",
       "8                          0.787248                          0.618401   \n",
       "9                          0.819441                          0.622791   \n",
       "\n",
       "   test accuracy steady subj6 fold1  test accuracy steady subj7 fold1  \\\n",
       "0                          0.580213                          0.567286   \n",
       "1                          0.645255                          0.626278   \n",
       "2                          0.645338                          0.605415   \n",
       "3                          0.645669                          0.615635   \n",
       "4                          0.645669                          0.615635   \n",
       "5                          0.645556                          0.623734   \n",
       "6                          0.649049                          0.613809   \n",
       "7                          0.626234                          0.598717   \n",
       "8                          0.632908                          0.608418   \n",
       "9                          0.622481                          0.621162   \n",
       "\n",
       "   test accuracy steady subj8 fold1  test accuracy steady subj9 fold1  \\\n",
       "0                          0.514041                          0.574422   \n",
       "1                          0.553240                          0.615790   \n",
       "2                          0.493640                          0.603708   \n",
       "3                          0.486907                          0.606161   \n",
       "4                          0.486907                          0.606161   \n",
       "5                          0.545258                          0.615514   \n",
       "6                          0.523773                          0.605864   \n",
       "7                          0.513240                          0.607080   \n",
       "8                          0.501350                          0.593253   \n",
       "9                          0.464880                          0.618643   \n",
       "\n",
       "   test accuracy steady subj0 fold2  test accuracy steady subj1 fold2  \\\n",
       "0                          0.746852                          0.654069   \n",
       "1                          0.755040                          0.392205   \n",
       "2                          0.741699                          0.623893   \n",
       "3                          0.735913                          0.622459   \n",
       "4                          0.735913                          0.622459   \n",
       "5                          0.754498                          0.645521   \n",
       "6                          0.737159                          0.612700   \n",
       "7                          0.732650                          0.561155   \n",
       "8                          0.732608                          0.556889   \n",
       "9                          0.736043                          0.556476   \n",
       "\n",
       "   test accuracy steady subj2 fold2  test accuracy steady subj3 fold2  \\\n",
       "0                          0.604190                          0.662675   \n",
       "1                          0.450906                          0.688006   \n",
       "2                          0.627952                          0.676171   \n",
       "3                          0.614426                          0.674718   \n",
       "4                          0.614426                          0.674718   \n",
       "5                          0.632389                          0.665833   \n",
       "6                          0.613754                          0.652122   \n",
       "7                          0.603461                          0.672789   \n",
       "8                          0.586934                          0.650636   \n",
       "9                          0.600345                          0.683500   \n",
       "\n",
       "   test accuracy steady subj4 fold2  test accuracy steady subj5 fold2  \\\n",
       "0                          0.781484                          0.632039   \n",
       "1                          0.777727                          0.442801   \n",
       "2                          0.798073                          0.608663   \n",
       "3                          0.794815                          0.614385   \n",
       "4                          0.794815                          0.614385   \n",
       "5                          0.806584                          0.597381   \n",
       "6                          0.806063                          0.620553   \n",
       "7                          0.804229                          0.603066   \n",
       "8                          0.775051                          0.607445   \n",
       "9                          0.798890                          0.619893   \n",
       "\n",
       "   test accuracy steady subj6 fold2  test accuracy steady subj7 fold2  \\\n",
       "0                          0.570015                          0.544692   \n",
       "1                          0.645650                          0.608209   \n",
       "2                          0.638288                          0.582998   \n",
       "3                          0.633174                          0.583274   \n",
       "4                          0.633174                          0.583274   \n",
       "5                          0.635252                          0.586171   \n",
       "6                          0.631080                          0.591007   \n",
       "7                          0.632496                          0.581143   \n",
       "8                          0.620563                          0.567597   \n",
       "9                          0.621080                          0.590127   \n",
       "\n",
       "   test accuracy steady subj8 fold2  test accuracy steady subj9 fold2  \\\n",
       "0                          0.532196                          0.560601   \n",
       "1                          0.563142                          0.610516   \n",
       "2                          0.548216                          0.587755   \n",
       "3                          0.552234                          0.584466   \n",
       "4                          0.552234                          0.584466   \n",
       "5                          0.553179                          0.596289   \n",
       "6                          0.548044                          0.574731   \n",
       "7                          0.549272                          0.597175   \n",
       "8                          0.547715                          0.577154   \n",
       "9                          0.529026                          0.583198   \n",
       "\n",
       "   test accuracy steady subj0 avg2folds  test accuracy steady subj1 avg2folds  \\\n",
       "0                              0.741879                              0.675315   \n",
       "1                              0.755758                              0.392205   \n",
       "2                              0.753193                              0.656614   \n",
       "3                              0.749907                              0.651094   \n",
       "4                              0.749907                              0.651094   \n",
       "5                              0.758244                              0.673686   \n",
       "6                              0.743908                              0.643132   \n",
       "7                              0.742444                              0.582029   \n",
       "8                              0.739955                              0.579315   \n",
       "9                              0.749187                              0.571347   \n",
       "\n",
       "   test accuracy steady subj2 avg2folds  test accuracy steady subj3 avg2folds  \\\n",
       "0                              0.616850                              0.667227   \n",
       "1                              0.477707                              0.692919   \n",
       "2                              0.642311                              0.678129   \n",
       "3                              0.640269                              0.681674   \n",
       "4                              0.640269                              0.681674   \n",
       "5                              0.650122                              0.675123   \n",
       "6                              0.633080                              0.665792   \n",
       "7                              0.618263                              0.677145   \n",
       "8                              0.604076                              0.661725   \n",
       "9                              0.633124                              0.689213   \n",
       "\n",
       "   test accuracy steady subj4 avg2folds  test accuracy steady subj5 avg2folds  \\\n",
       "0                              0.784742                              0.636111   \n",
       "1                              0.783042                              0.472039   \n",
       "2                              0.806790                              0.618921   \n",
       "3                              0.805669                              0.619584   \n",
       "4                              0.805669                              0.619584   \n",
       "5                              0.811746                              0.603400   \n",
       "6                              0.810101                              0.619058   \n",
       "7                              0.803733                              0.614512   \n",
       "8                              0.781150                              0.612923   \n",
       "9                              0.809166                              0.621342   \n",
       "\n",
       "   test accuracy steady subj6 avg2folds  test accuracy steady subj7 avg2folds  \\\n",
       "0                              0.575114                              0.555989   \n",
       "1                              0.645453                              0.617244   \n",
       "2                              0.641813                              0.594206   \n",
       "3                              0.639422                              0.599455   \n",
       "4                              0.639422                              0.599455   \n",
       "5                              0.640404                              0.604953   \n",
       "6                              0.640065                              0.602408   \n",
       "7                              0.629365                              0.589930   \n",
       "8                              0.626735                              0.588007   \n",
       "9                              0.621781                              0.605645   \n",
       "\n",
       "   test accuracy steady subj8 avg2folds  test accuracy steady subj9 avg2folds  \\\n",
       "0                              0.523119                              0.567511   \n",
       "1                              0.558191                              0.613153   \n",
       "2                              0.520928                              0.595732   \n",
       "3                              0.519571                              0.595313   \n",
       "4                              0.519571                              0.595313   \n",
       "5                              0.549219                              0.605902   \n",
       "6                              0.535908                              0.590298   \n",
       "7                              0.531256                              0.602128   \n",
       "8                              0.524532                              0.585203   \n",
       "9                              0.496953                              0.600921   \n",
       "\n",
       "                   test preds steady subj0 avg2folds  \\\n",
       "0  [43683, 8300, 7663, 10186, 4802, 8483, 8844, 5...   \n",
       "1  [42382, 9906, 7322, 8294, 4515, 10634, 9355, 4...   \n",
       "2  [42537, 9700, 7024, 8530, 5099, 11005, 8204, 5...   \n",
       "3  [42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...   \n",
       "4  [42298, 9829, 7104, 8347, 4583, 10745, 8869, 5...   \n",
       "5  [42338, 9750, 8110, 8274, 4124, 10339, 9045, 5...   \n",
       "6  [42740, 9586, 8540, 7955, 3682, 10972, 8790, 4...   \n",
       "7  [42763, 9884, 6540, 7986, 5341, 11634, 8277, 4...   \n",
       "8  [42699, 10313, 6503, 8072, 5559, 10826, 8270, ...   \n",
       "9  [41816, 9942, 7648, 9023, 4181, 10607, 8476, 5...   \n",
       "\n",
       "                   test preds steady subj1 avg2folds  \\\n",
       "0  [37096, 7620, 4609, 6389, 15561, 10865, 8635, ...   \n",
       "1                       [97068, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [35400, 7959, 2862, 4531, 15576, 10961, 9577, ...   \n",
       "3  [35379, 6848, 3072, 3653, 15247, 11575, 9471, ...   \n",
       "4  [35379, 6848, 3072, 3653, 15247, 11575, 9471, ...   \n",
       "5  [35840, 8146, 3298, 4105, 14213, 11276, 9060, ...   \n",
       "6  [35422, 6131, 2203, 3514, 14683, 11990, 10532,...   \n",
       "7  [34757, 5072, 4645, 2849, 8946, 12170, 11238, ...   \n",
       "8  [33820, 6053, 3058, 3572, 11857, 10220, 11483,...   \n",
       "9  [32376, 6707, 6017, 3522, 8234, 12767, 10004, ...   \n",
       "\n",
       "                   test preds steady subj2 avg2folds  \\\n",
       "0  [52591, 5152, 9735, 4958, 7867, 6127, 5779, 5193]   \n",
       "1       [83428, 5730, 3614, 1221, 636, 2398, 7, 368]   \n",
       "2  [50162, 4469, 9065, 5241, 6647, 9932, 7253, 4633]   \n",
       "3  [51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]   \n",
       "4  [51435, 4834, 9396, 5177, 5935, 8791, 6994, 4840]   \n",
       "5  [51233, 4384, 7781, 5462, 6944, 10979, 5926, 4...   \n",
       "6  [50305, 5060, 5951, 5653, 8967, 10627, 5580, 5...   \n",
       "7  [52517, 3482, 5792, 4520, 8249, 11040, 7358, 4...   \n",
       "8  [51735, 3903, 6662, 4609, 9566, 9573, 5992, 5362]   \n",
       "9  [48248, 4144, 7882, 5102, 8106, 11834, 7572, 4...   \n",
       "\n",
       "                   test preds steady subj3 avg2folds  \\\n",
       "0  [45926, 7769, 5405, 7001, 6477, 10985, 6543, 6...   \n",
       "1  [44084, 7843, 7732, 6714, 4990, 11060, 7849, 6...   \n",
       "2  [46081, 8482, 5517, 6489, 6605, 10703, 7144, 5...   \n",
       "3  [45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...   \n",
       "4  [45625, 8062, 6290, 6209, 6017, 10933, 7403, 6...   \n",
       "5  [45355, 8938, 6188, 7020, 5351, 10843, 7402, 5...   \n",
       "6  [44805, 7726, 4660, 3805, 6079, 14332, 8838, 6...   \n",
       "7  [45997, 8902, 6995, 5306, 6338, 11202, 6193, 6...   \n",
       "8  [46162, 9521, 6221, 4640, 5340, 9920, 8623, 6587]   \n",
       "9  [43316, 8883, 10423, 6545, 5150, 10360, 5358, ...   \n",
       "\n",
       "                   test preds steady subj4 avg2folds  \\\n",
       "0  [52599, 8356, 7453, 7592, 4948, 4856, 7237, 4289]   \n",
       "1  [52022, 9207, 7626, 8803, 4678, 5942, 4941, 4111]   \n",
       "2  [52839, 7440, 7690, 8170, 4698, 6871, 5663, 3959]   \n",
       "3  [52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]   \n",
       "4  [52646, 7120, 8288, 7997, 4450, 7162, 5717, 3950]   \n",
       "5  [52878, 7240, 8270, 8382, 4259, 6466, 5014, 4821]   \n",
       "6  [52454, 6913, 8193, 6476, 5585, 6832, 5691, 5186]   \n",
       "7  [53122, 8011, 7682, 8706, 4690, 5983, 5349, 3787]   \n",
       "8  [53149, 6896, 9564, 7236, 4932, 5975, 5737, 3841]   \n",
       "9  [52617, 7905, 7206, 7575, 5678, 6796, 5156, 4397]   \n",
       "\n",
       "                   test preds steady subj5 avg2folds  \\\n",
       "0  [44874, 5145, 4933, 5653, 6904, 13364, 10596, ...   \n",
       "1  [51263, 2941, 8735, 2120, 4437, 8606, 16523, 2...   \n",
       "2  [42317, 5905, 5788, 6042, 4677, 15283, 11544, ...   \n",
       "3  [43045, 5811, 6303, 6441, 4370, 15096, 10855, ...   \n",
       "4  [43045, 5811, 6303, 6441, 4370, 15096, 10855, ...   \n",
       "5  [41994, 5708, 6966, 6063, 5336, 14776, 11007, ...   \n",
       "6  [43513, 5866, 6583, 6224, 5029, 12336, 12767, ...   \n",
       "7  [43770, 4952, 5148, 7286, 5583, 11404, 13372, ...   \n",
       "8  [44580, 4537, 5143, 7133, 6190, 11474, 12305, ...   \n",
       "9  [40215, 6041, 3975, 5769, 5032, 14118, 13994, ...   \n",
       "\n",
       "                   test preds steady subj6 avg2folds  \\\n",
       "0  [39292, 8151, 9642, 4945, 11295, 7753, 11643, ...   \n",
       "1  [37519, 8339, 11970, 9627, 7565, 7951, 9157, 5...   \n",
       "2  [37716, 8323, 10726, 10230, 7541, 8194, 10243,...   \n",
       "3  [37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...   \n",
       "4  [37948, 8759, 10671, 9572, 8945, 7896, 9163, 4...   \n",
       "5  [37493, 8224, 11889, 9761, 7863, 8306, 9706, 4...   \n",
       "6  [38518, 7955, 10339, 8922, 8214, 7670, 11086, ...   \n",
       "7  [38270, 7926, 11250, 8812, 8968, 8645, 9469, 4...   \n",
       "8  [38615, 8252, 10522, 8485, 8517, 7893, 10872, ...   \n",
       "9  [36032, 7341, 11815, 9392, 7073, 11002, 10771,...   \n",
       "\n",
       "                   test preds steady subj7 avg2folds  \\\n",
       "0  [61414, 10642, 5421, 6018, 4191, 1553, 5291, 3...   \n",
       "1  [53888, 7964, 6869, 5838, 5333, 1970, 7512, 8568]   \n",
       "2  [56188, 8111, 6442, 4636, 6892, 2265, 8314, 5094]   \n",
       "3  [56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]   \n",
       "4  [56030, 8211, 6594, 5599, 6357, 2189, 7848, 5114]   \n",
       "5  [55129, 8449, 5846, 5249, 6157, 2015, 8532, 6565]   \n",
       "6  [54105, 8388, 9494, 4944, 6205, 1470, 6946, 6390]   \n",
       "7  [54102, 8807, 6210, 4601, 6443, 2529, 9947, 5303]   \n",
       "8  [57376, 8582, 6284, 4872, 5460, 2182, 8291, 4895]   \n",
       "9  [46711, 11186, 9334, 5658, 6321, 3994, 8224, 6...   \n",
       "\n",
       "                   test preds steady subj8 avg2folds  \\\n",
       "0  [74741, 5096, 6328, 2890, 3584, 1901, 3079, 1449]   \n",
       "1  [73465, 6928, 3997, 3855, 3498, 2091, 3130, 2104]   \n",
       "2  [67523, 9115, 5183, 2529, 3712, 4363, 3772, 2871]   \n",
       "3  [65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...   \n",
       "4  [65867, 10403, 4756, 3431, 3769, 4336, 3609, 2...   \n",
       "5  [72810, 5945, 4523, 3268, 4115, 3117, 3415, 1875]   \n",
       "6  [70010, 7791, 4756, 3682, 3633, 3343, 3205, 2648]   \n",
       "7  [70170, 5699, 6323, 2988, 4986, 3545, 3158, 2199]   \n",
       "8  [69951, 6045, 4176, 3032, 4994, 3835, 3783, 3252]   \n",
       "9  [59424, 14255, 7353, 4847, 3272, 3603, 4086, 2...   \n",
       "\n",
       "                   test preds steady subj9 avg2folds  \n",
       "0  [43748, 4575, 11866, 8372, 9800, 8609, 4053, 6...  \n",
       "1  [40454, 4862, 6168, 8926, 15658, 10874, 5963, ...  \n",
       "2  [44283, 3920, 4881, 6871, 14326, 12657, 5057, ...  \n",
       "3  [43242, 4183, 4748, 6365, 14667, 13749, 5124, ...  \n",
       "4  [43242, 4183, 4748, 6365, 14667, 13749, 5124, ...  \n",
       "5  [42707, 4534, 4659, 7412, 15824, 11516, 4982, ...  \n",
       "6  [42112, 4435, 6936, 8710, 10835, 12487, 5142, ...  \n",
       "7  [43862, 4354, 4846, 7864, 14067, 11695, 5131, ...  \n",
       "8  [44523, 4327, 6397, 7783, 12398, 11383, 5023, ...  \n",
       "9  [39423, 5625, 7140, 7692, 12475, 11916, 5798, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_pretrain_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temponet_pretrain_9 = read_results(\"exp7/tcn_pretrain/results_1625151773.pickle\", {\"pretraining\": 'no', \"pretraining_epochs\": 0, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}, model_name='temponet') \\\n",
    "        .append(read_results(\"exp9/temponet_pretraining_9/finetune_100/results_1627311430.pickle\", {\"pretraining\": 'all_others_20', \"pretraining_epochs\": 20, \"finetune_epochs\": 20}, group_exclude_columns={'pretrained'}, model_name='temponet'), ignore_index=True) \\\n",
    "        .append(read_results(\"exp9/temponet_pretraining_9/finetune_100_/results_1627329435.pickle\", {\"pretraining\": 'all_others_min20-20', \"pretraining_epochs\": 20, \"finetune_epochs\": \"min20-20\"}, group_exclude_columns={'pretrained'}, model_name='temponet'), ignore_index=True) \\\n",
    "        .append(read_results(\"exp7/tcn_pretrain/results_1625075271.pickle\", {\"pretraining\": 'fold1'}, model_name='temponet'), ignore_index=True) \\\n",
    "        .append(read_results(\"exp7/tcn_pretrain/results_1626361771.pickle\", {\"pretraining\": 'fold2'}, model_name='temponet'), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MACs</th>\n",
       "      <th>params</th>\n",
       "      <th>pretraining</th>\n",
       "      <th>pretraining_epochs</th>\n",
       "      <th>finetune_epochs</th>\n",
       "      <th>train accuracy steady fold1</th>\n",
       "      <th>train accuracy steady fold2</th>\n",
       "      <th>train accuracy steady avg2folds</th>\n",
       "      <th>validation accuracy steady fold1</th>\n",
       "      <th>validation accuracy steady fold2</th>\n",
       "      <th>test accuracy fold1</th>\n",
       "      <th>test accuracy fold2</th>\n",
       "      <th>test accuracy avg2folds</th>\n",
       "      <th>test accuracy steady fold1</th>\n",
       "      <th>test accuracy steady fold2</th>\n",
       "      <th>test accuracy steady avg2folds</th>\n",
       "      <th>test accuracy steady avg2folds std across sessions</th>\n",
       "      <th>test accuracy steady avg2folds std across subjects</th>\n",
       "      <th>test accuracy steady subj0 fold1</th>\n",
       "      <th>test accuracy steady subj1 fold1</th>\n",
       "      <th>test accuracy steady subj2 fold1</th>\n",
       "      <th>test accuracy steady subj3 fold1</th>\n",
       "      <th>test accuracy steady subj4 fold1</th>\n",
       "      <th>test accuracy steady subj5 fold1</th>\n",
       "      <th>test accuracy steady subj6 fold1</th>\n",
       "      <th>test accuracy steady subj7 fold1</th>\n",
       "      <th>test accuracy steady subj8 fold1</th>\n",
       "      <th>test accuracy steady subj9 fold1</th>\n",
       "      <th>test accuracy steady subj0 fold2</th>\n",
       "      <th>test accuracy steady subj1 fold2</th>\n",
       "      <th>test accuracy steady subj2 fold2</th>\n",
       "      <th>test accuracy steady subj3 fold2</th>\n",
       "      <th>test accuracy steady subj4 fold2</th>\n",
       "      <th>test accuracy steady subj5 fold2</th>\n",
       "      <th>test accuracy steady subj6 fold2</th>\n",
       "      <th>test accuracy steady subj7 fold2</th>\n",
       "      <th>test accuracy steady subj8 fold2</th>\n",
       "      <th>test accuracy steady subj9 fold2</th>\n",
       "      <th>test accuracy steady subj0 avg2folds</th>\n",
       "      <th>test accuracy steady subj1 avg2folds</th>\n",
       "      <th>test accuracy steady subj2 avg2folds</th>\n",
       "      <th>test accuracy steady subj3 avg2folds</th>\n",
       "      <th>test accuracy steady subj4 avg2folds</th>\n",
       "      <th>test accuracy steady subj5 avg2folds</th>\n",
       "      <th>test accuracy steady subj6 avg2folds</th>\n",
       "      <th>test accuracy steady subj7 avg2folds</th>\n",
       "      <th>test accuracy steady subj8 avg2folds</th>\n",
       "      <th>test accuracy steady subj9 avg2folds</th>\n",
       "      <th>test preds steady subj0 avg2folds</th>\n",
       "      <th>test preds steady subj1 avg2folds</th>\n",
       "      <th>test preds steady subj2 avg2folds</th>\n",
       "      <th>test preds steady subj3 avg2folds</th>\n",
       "      <th>test preds steady subj4 avg2folds</th>\n",
       "      <th>test preds steady subj5 avg2folds</th>\n",
       "      <th>test preds steady subj6 avg2folds</th>\n",
       "      <th>test preds steady subj7 avg2folds</th>\n",
       "      <th>test preds steady subj8 avg2folds</th>\n",
       "      <th>test preds steady subj9 avg2folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16028672</td>\n",
       "      <td>461512</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.968119</td>\n",
       "      <td>0.958547</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.716316</td>\n",
       "      <td>0.716999</td>\n",
       "      <td>0.504648</td>\n",
       "      <td>0.492617</td>\n",
       "      <td>0.498632</td>\n",
       "      <td>0.663358</td>\n",
       "      <td>0.646175</td>\n",
       "      <td>0.654767</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>0.090923</td>\n",
       "      <td>0.770526</td>\n",
       "      <td>0.739558</td>\n",
       "      <td>0.660760</td>\n",
       "      <td>0.683550</td>\n",
       "      <td>0.815239</td>\n",
       "      <td>0.695168</td>\n",
       "      <td>0.612760</td>\n",
       "      <td>0.585477</td>\n",
       "      <td>0.456376</td>\n",
       "      <td>0.614164</td>\n",
       "      <td>0.780715</td>\n",
       "      <td>0.658043</td>\n",
       "      <td>0.636953</td>\n",
       "      <td>0.677378</td>\n",
       "      <td>0.806595</td>\n",
       "      <td>0.648190</td>\n",
       "      <td>0.597844</td>\n",
       "      <td>0.558705</td>\n",
       "      <td>0.517730</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.775620</td>\n",
       "      <td>0.698801</td>\n",
       "      <td>0.648857</td>\n",
       "      <td>0.680464</td>\n",
       "      <td>0.810917</td>\n",
       "      <td>0.671679</td>\n",
       "      <td>0.605302</td>\n",
       "      <td>0.572091</td>\n",
       "      <td>0.487053</td>\n",
       "      <td>0.596882</td>\n",
       "      <td>[41318, 9189, 8166, 9809, 5262, 10167, 8183, 5...</td>\n",
       "      <td>[35938, 6736, 4054, 8315, 14667, 13311, 6616, ...</td>\n",
       "      <td>[52412, 5515, 9404, 6059, 5992, 7132, 7545, 3343]</td>\n",
       "      <td>[43907, 6080, 6358, 5767, 6250, 12930, 7669, 8...</td>\n",
       "      <td>[52507, 9225, 6841, 8243, 4319, 4976, 6943, 4276]</td>\n",
       "      <td>[43927, 4629, 5382, 5503, 8591, 11586, 9715, 7...</td>\n",
       "      <td>[41238, 7555, 7907, 4505, 10329, 7195, 13820, ...</td>\n",
       "      <td>[50177, 14734, 9339, 5481, 5929, 3312, 5313, 3...</td>\n",
       "      <td>[60935, 11040, 5879, 4635, 3148, 5437, 4932, 3...</td>\n",
       "      <td>[41073, 4776, 8771, 8421, 10535, 13310, 5036, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16028672</td>\n",
       "      <td>461512</td>\n",
       "      <td>all_others_20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969161</td>\n",
       "      <td>0.936360</td>\n",
       "      <td>0.952760</td>\n",
       "      <td>0.729164</td>\n",
       "      <td>0.776814</td>\n",
       "      <td>0.515240</td>\n",
       "      <td>0.506377</td>\n",
       "      <td>0.510809</td>\n",
       "      <td>0.680713</td>\n",
       "      <td>0.666393</td>\n",
       "      <td>0.673553</td>\n",
       "      <td>0.018927</td>\n",
       "      <td>0.074464</td>\n",
       "      <td>0.780090</td>\n",
       "      <td>0.684403</td>\n",
       "      <td>0.656979</td>\n",
       "      <td>0.682321</td>\n",
       "      <td>0.824407</td>\n",
       "      <td>0.694907</td>\n",
       "      <td>0.660565</td>\n",
       "      <td>0.631600</td>\n",
       "      <td>0.551140</td>\n",
       "      <td>0.640719</td>\n",
       "      <td>0.751675</td>\n",
       "      <td>0.612510</td>\n",
       "      <td>0.635771</td>\n",
       "      <td>0.685828</td>\n",
       "      <td>0.852327</td>\n",
       "      <td>0.682555</td>\n",
       "      <td>0.652758</td>\n",
       "      <td>0.590504</td>\n",
       "      <td>0.575037</td>\n",
       "      <td>0.624963</td>\n",
       "      <td>0.765882</td>\n",
       "      <td>0.648457</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.684074</td>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.688731</td>\n",
       "      <td>0.656661</td>\n",
       "      <td>0.611052</td>\n",
       "      <td>0.563089</td>\n",
       "      <td>0.632841</td>\n",
       "      <td>[42478, 9401, 6709, 8601, 5036, 11911, 7860, 5...</td>\n",
       "      <td>[36772, 6440, 1152, 2149, 12637, 10751, 11682,...</td>\n",
       "      <td>[54657, 4608, 5486, 4963, 6891, 7367, 8347, 5083]</td>\n",
       "      <td>[46584, 4125, 3725, 5947, 7004, 14735, 7608, 7...</td>\n",
       "      <td>[51987, 6844, 5936, 9836, 4849, 6425, 5300, 6153]</td>\n",
       "      <td>[43109, 5944, 3569, 6561, 6262, 12863, 11283, ...</td>\n",
       "      <td>[40364, 7385, 12633, 8298, 6588, 7956, 10480, ...</td>\n",
       "      <td>[55187, 7565, 6659, 4363, 7789, 2099, 7542, 6738]</td>\n",
       "      <td>[73854, 5954, 5725, 3231, 2976, 2867, 2765, 1696]</td>\n",
       "      <td>[42456, 4584, 4505, 8121, 15070, 13056, 5290, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16028672</td>\n",
       "      <td>461512</td>\n",
       "      <td>all_others_min20-20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>min20-20</td>\n",
       "      <td>0.988129</td>\n",
       "      <td>0.968234</td>\n",
       "      <td>0.978182</td>\n",
       "      <td>0.730251</td>\n",
       "      <td>0.772786</td>\n",
       "      <td>0.517049</td>\n",
       "      <td>0.497397</td>\n",
       "      <td>0.507223</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.654421</td>\n",
       "      <td>0.668233</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.076368</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.703593</td>\n",
       "      <td>0.653335</td>\n",
       "      <td>0.676019</td>\n",
       "      <td>0.827613</td>\n",
       "      <td>0.702164</td>\n",
       "      <td>0.657379</td>\n",
       "      <td>0.634680</td>\n",
       "      <td>0.550544</td>\n",
       "      <td>0.639256</td>\n",
       "      <td>0.754428</td>\n",
       "      <td>0.579233</td>\n",
       "      <td>0.622231</td>\n",
       "      <td>0.679806</td>\n",
       "      <td>0.845196</td>\n",
       "      <td>0.667251</td>\n",
       "      <td>0.646822</td>\n",
       "      <td>0.563391</td>\n",
       "      <td>0.565999</td>\n",
       "      <td>0.619858</td>\n",
       "      <td>0.765145</td>\n",
       "      <td>0.641413</td>\n",
       "      <td>0.637783</td>\n",
       "      <td>0.677912</td>\n",
       "      <td>0.836405</td>\n",
       "      <td>0.684708</td>\n",
       "      <td>0.652101</td>\n",
       "      <td>0.599035</td>\n",
       "      <td>0.558272</td>\n",
       "      <td>0.629557</td>\n",
       "      <td>[42501, 9599, 6919, 8178, 5329, 12059, 7283, 5...</td>\n",
       "      <td>[37288, 6162, 1317, 1868, 12776, 9548, 12970, ...</td>\n",
       "      <td>[54338, 4553, 5447, 4571, 6561, 7800, 9158, 4974]</td>\n",
       "      <td>[46627, 3959, 3552, 5800, 6408, 14465, 9017, 7...</td>\n",
       "      <td>[52197, 6782, 6572, 9399, 4661, 6285, 5805, 5629]</td>\n",
       "      <td>[43427, 6211, 4223, 6580, 5429, 13467, 10901, ...</td>\n",
       "      <td>[40357, 7267, 11969, 9018, 6715, 8443, 9762, 3...</td>\n",
       "      <td>[56361, 7662, 5680, 3704, 7903, 2143, 6538, 7951]</td>\n",
       "      <td>[72645, 6148, 6041, 3017, 3463, 3107, 2707, 1940]</td>\n",
       "      <td>[42043, 4007, 4128, 7985, 16062, 12964, 5651, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16028672</td>\n",
       "      <td>461512</td>\n",
       "      <td>fold1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972680</td>\n",
       "      <td>0.936642</td>\n",
       "      <td>0.954661</td>\n",
       "      <td>0.729058</td>\n",
       "      <td>0.782328</td>\n",
       "      <td>0.514245</td>\n",
       "      <td>0.498983</td>\n",
       "      <td>0.506614</td>\n",
       "      <td>0.677961</td>\n",
       "      <td>0.658059</td>\n",
       "      <td>0.668010</td>\n",
       "      <td>0.021699</td>\n",
       "      <td>0.077702</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.669525</td>\n",
       "      <td>0.651408</td>\n",
       "      <td>0.688720</td>\n",
       "      <td>0.834035</td>\n",
       "      <td>0.678448</td>\n",
       "      <td>0.659191</td>\n",
       "      <td>0.636359</td>\n",
       "      <td>0.545116</td>\n",
       "      <td>0.645818</td>\n",
       "      <td>0.752697</td>\n",
       "      <td>0.552860</td>\n",
       "      <td>0.632558</td>\n",
       "      <td>0.685281</td>\n",
       "      <td>0.853315</td>\n",
       "      <td>0.660651</td>\n",
       "      <td>0.664624</td>\n",
       "      <td>0.593809</td>\n",
       "      <td>0.570205</td>\n",
       "      <td>0.614592</td>\n",
       "      <td>0.761845</td>\n",
       "      <td>0.611193</td>\n",
       "      <td>0.641983</td>\n",
       "      <td>0.687001</td>\n",
       "      <td>0.843675</td>\n",
       "      <td>0.669550</td>\n",
       "      <td>0.661907</td>\n",
       "      <td>0.615084</td>\n",
       "      <td>0.557660</td>\n",
       "      <td>0.630205</td>\n",
       "      <td>[41761, 9419, 7582, 8340, 4600, 12463, 7532, 5...</td>\n",
       "      <td>[37119, 4278, 669, 2142, 10619, 10303, 13695, ...</td>\n",
       "      <td>[54680, 4673, 4749, 3715, 7377, 10138, 7444, 4...</td>\n",
       "      <td>[45744, 4670, 3706, 4408, 7799, 14860, 8604, 7...</td>\n",
       "      <td>[51992, 6378, 6818, 9223, 4937, 6060, 5890, 6032]</td>\n",
       "      <td>[41153, 6233, 6108, 7453, 4625, 15256, 9187, 6...</td>\n",
       "      <td>[39608, 7525, 11169, 9726, 6539, 6973, 11153, ...</td>\n",
       "      <td>[60333, 6472, 5313, 4297, 6920, 2001, 5794, 6812]</td>\n",
       "      <td>[70701, 8569, 5395, 3763, 2805, 2938, 3135, 1762]</td>\n",
       "      <td>[41551, 4838, 3303, 7261, 17930, 12066, 5486, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16028672</td>\n",
       "      <td>461512</td>\n",
       "      <td>fold2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.948968</td>\n",
       "      <td>0.969305</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.779266</td>\n",
       "      <td>0.732315</td>\n",
       "      <td>0.520106</td>\n",
       "      <td>0.501060</td>\n",
       "      <td>0.510583</td>\n",
       "      <td>0.685966</td>\n",
       "      <td>0.659222</td>\n",
       "      <td>0.672594</td>\n",
       "      <td>0.020318</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>0.783701</td>\n",
       "      <td>0.673458</td>\n",
       "      <td>0.675539</td>\n",
       "      <td>0.698187</td>\n",
       "      <td>0.837082</td>\n",
       "      <td>0.687946</td>\n",
       "      <td>0.676174</td>\n",
       "      <td>0.657496</td>\n",
       "      <td>0.544001</td>\n",
       "      <td>0.626074</td>\n",
       "      <td>0.770467</td>\n",
       "      <td>0.642874</td>\n",
       "      <td>0.641781</td>\n",
       "      <td>0.677714</td>\n",
       "      <td>0.802563</td>\n",
       "      <td>0.654799</td>\n",
       "      <td>0.646847</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.551546</td>\n",
       "      <td>0.591352</td>\n",
       "      <td>0.777084</td>\n",
       "      <td>0.658166</td>\n",
       "      <td>0.658660</td>\n",
       "      <td>0.687951</td>\n",
       "      <td>0.819823</td>\n",
       "      <td>0.671373</td>\n",
       "      <td>0.661510</td>\n",
       "      <td>0.634887</td>\n",
       "      <td>0.547773</td>\n",
       "      <td>0.608713</td>\n",
       "      <td>[41624, 9780, 5801, 8449, 7275, 12831, 6554, 4...</td>\n",
       "      <td>[37817, 4891, 587, 2131, 13207, 13443, 9379, 1...</td>\n",
       "      <td>[51831, 4557, 5239, 5185, 7867, 9730, 8153, 4840]</td>\n",
       "      <td>[44628, 5419, 3706, 5264, 8743, 12586, 8489, 8...</td>\n",
       "      <td>[52592, 6600, 7631, 8541, 4727, 5305, 6335, 5599]</td>\n",
       "      <td>[43232, 6764, 5568, 7903, 5554, 13369, 9646, 4...</td>\n",
       "      <td>[37701, 8040, 9797, 8606, 8705, 8349, 11502, 4...</td>\n",
       "      <td>[53209, 8349, 5806, 5042, 8848, 2572, 9339, 4777]</td>\n",
       "      <td>[70069, 7807, 3504, 3382, 5400, 3586, 3676, 1644]</td>\n",
       "      <td>[45676, 3219, 3911, 8102, 17511, 10488, 3914, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MACs  params          pretraining  pretraining_epochs finetune_epochs  \\\n",
       "0  16028672  461512                   no                 0.0              20   \n",
       "1  16028672  461512        all_others_20                20.0              20   \n",
       "2  16028672  461512  all_others_min20-20                20.0        min20-20   \n",
       "3  16028672  461512                fold1                 NaN             NaN   \n",
       "4  16028672  461512                fold2                 NaN             NaN   \n",
       "\n",
       "   train accuracy steady fold1  train accuracy steady fold2  \\\n",
       "0                     0.968119                     0.958547   \n",
       "1                     0.969161                     0.936360   \n",
       "2                     0.988129                     0.968234   \n",
       "3                     0.972680                     0.936642   \n",
       "4                     0.948968                     0.969305   \n",
       "\n",
       "   train accuracy steady avg2folds  validation accuracy steady fold1  \\\n",
       "0                         0.963333                          0.716316   \n",
       "1                         0.952760                          0.729164   \n",
       "2                         0.978182                          0.730251   \n",
       "3                         0.954661                          0.729058   \n",
       "4                         0.959136                          0.779266   \n",
       "\n",
       "   validation accuracy steady fold2  test accuracy fold1  test accuracy fold2  \\\n",
       "0                          0.716999             0.504648             0.492617   \n",
       "1                          0.776814             0.515240             0.506377   \n",
       "2                          0.772786             0.517049             0.497397   \n",
       "3                          0.782328             0.514245             0.498983   \n",
       "4                          0.732315             0.520106             0.501060   \n",
       "\n",
       "   test accuracy avg2folds  test accuracy steady fold1  \\\n",
       "0                 0.498632                    0.663358   \n",
       "1                 0.510809                    0.680713   \n",
       "2                 0.507223                    0.682045   \n",
       "3                 0.506614                    0.677961   \n",
       "4                 0.510583                    0.685966   \n",
       "\n",
       "   test accuracy steady fold2  test accuracy steady avg2folds  \\\n",
       "0                    0.646175                        0.654767   \n",
       "1                    0.666393                        0.673553   \n",
       "2                    0.654421                        0.668233   \n",
       "3                    0.658059                        0.668010   \n",
       "4                    0.659222                        0.672594   \n",
       "\n",
       "   test accuracy steady avg2folds std across sessions  \\\n",
       "0                                           0.025317    \n",
       "1                                           0.018927    \n",
       "2                                           0.019815    \n",
       "3                                           0.021699    \n",
       "4                                           0.020318    \n",
       "\n",
       "   test accuracy steady avg2folds std across subjects  \\\n",
       "0                                           0.090923    \n",
       "1                                           0.074464    \n",
       "2                                           0.076368    \n",
       "3                                           0.077702    \n",
       "4                                           0.073783    \n",
       "\n",
       "   test accuracy steady subj0 fold1  test accuracy steady subj1 fold1  \\\n",
       "0                          0.770526                          0.739558   \n",
       "1                          0.780090                          0.684403   \n",
       "2                          0.775862                          0.703593   \n",
       "3                          0.770992                          0.669525   \n",
       "4                          0.783701                          0.673458   \n",
       "\n",
       "   test accuracy steady subj2 fold1  test accuracy steady subj3 fold1  \\\n",
       "0                          0.660760                          0.683550   \n",
       "1                          0.656979                          0.682321   \n",
       "2                          0.653335                          0.676019   \n",
       "3                          0.651408                          0.688720   \n",
       "4                          0.675539                          0.698187   \n",
       "\n",
       "   test accuracy steady subj4 fold1  test accuracy steady subj5 fold1  \\\n",
       "0                          0.815239                          0.695168   \n",
       "1                          0.824407                          0.694907   \n",
       "2                          0.827613                          0.702164   \n",
       "3                          0.834035                          0.678448   \n",
       "4                          0.837082                          0.687946   \n",
       "\n",
       "   test accuracy steady subj6 fold1  test accuracy steady subj7 fold1  \\\n",
       "0                          0.612760                          0.585477   \n",
       "1                          0.660565                          0.631600   \n",
       "2                          0.657379                          0.634680   \n",
       "3                          0.659191                          0.636359   \n",
       "4                          0.676174                          0.657496   \n",
       "\n",
       "   test accuracy steady subj8 fold1  test accuracy steady subj9 fold1  \\\n",
       "0                          0.456376                          0.614164   \n",
       "1                          0.551140                          0.640719   \n",
       "2                          0.550544                          0.639256   \n",
       "3                          0.545116                          0.645818   \n",
       "4                          0.544001                          0.626074   \n",
       "\n",
       "   test accuracy steady subj0 fold2  test accuracy steady subj1 fold2  \\\n",
       "0                          0.780715                          0.658043   \n",
       "1                          0.751675                          0.612510   \n",
       "2                          0.754428                          0.579233   \n",
       "3                          0.752697                          0.552860   \n",
       "4                          0.770467                          0.642874   \n",
       "\n",
       "   test accuracy steady subj2 fold2  test accuracy steady subj3 fold2  \\\n",
       "0                          0.636953                          0.677378   \n",
       "1                          0.635771                          0.685828   \n",
       "2                          0.622231                          0.679806   \n",
       "3                          0.632558                          0.685281   \n",
       "4                          0.641781                          0.677714   \n",
       "\n",
       "   test accuracy steady subj4 fold2  test accuracy steady subj5 fold2  \\\n",
       "0                          0.806595                          0.648190   \n",
       "1                          0.852327                          0.682555   \n",
       "2                          0.845196                          0.667251   \n",
       "3                          0.853315                          0.660651   \n",
       "4                          0.802563                          0.654799   \n",
       "\n",
       "   test accuracy steady subj6 fold2  test accuracy steady subj7 fold2  \\\n",
       "0                          0.597844                          0.558705   \n",
       "1                          0.652758                          0.590504   \n",
       "2                          0.646822                          0.563391   \n",
       "3                          0.664624                          0.593809   \n",
       "4                          0.646847                          0.612277   \n",
       "\n",
       "   test accuracy steady subj8 fold2  test accuracy steady subj9 fold2  \\\n",
       "0                          0.517730                          0.579600   \n",
       "1                          0.575037                          0.624963   \n",
       "2                          0.565999                          0.619858   \n",
       "3                          0.570205                          0.614592   \n",
       "4                          0.551546                          0.591352   \n",
       "\n",
       "   test accuracy steady subj0 avg2folds  test accuracy steady subj1 avg2folds  \\\n",
       "0                              0.775620                              0.698801   \n",
       "1                              0.765882                              0.648457   \n",
       "2                              0.765145                              0.641413   \n",
       "3                              0.761845                              0.611193   \n",
       "4                              0.777084                              0.658166   \n",
       "\n",
       "   test accuracy steady subj2 avg2folds  test accuracy steady subj3 avg2folds  \\\n",
       "0                              0.648857                              0.680464   \n",
       "1                              0.646375                              0.684074   \n",
       "2                              0.637783                              0.677912   \n",
       "3                              0.641983                              0.687001   \n",
       "4                              0.658660                              0.687951   \n",
       "\n",
       "   test accuracy steady subj4 avg2folds  test accuracy steady subj5 avg2folds  \\\n",
       "0                              0.810917                              0.671679   \n",
       "1                              0.838367                              0.688731   \n",
       "2                              0.836405                              0.684708   \n",
       "3                              0.843675                              0.669550   \n",
       "4                              0.819823                              0.671373   \n",
       "\n",
       "   test accuracy steady subj6 avg2folds  test accuracy steady subj7 avg2folds  \\\n",
       "0                              0.605302                              0.572091   \n",
       "1                              0.656661                              0.611052   \n",
       "2                              0.652101                              0.599035   \n",
       "3                              0.661907                              0.615084   \n",
       "4                              0.661510                              0.634887   \n",
       "\n",
       "   test accuracy steady subj8 avg2folds  test accuracy steady subj9 avg2folds  \\\n",
       "0                              0.487053                              0.596882   \n",
       "1                              0.563089                              0.632841   \n",
       "2                              0.558272                              0.629557   \n",
       "3                              0.557660                              0.630205   \n",
       "4                              0.547773                              0.608713   \n",
       "\n",
       "                   test preds steady subj0 avg2folds  \\\n",
       "0  [41318, 9189, 8166, 9809, 5262, 10167, 8183, 5...   \n",
       "1  [42478, 9401, 6709, 8601, 5036, 11911, 7860, 5...   \n",
       "2  [42501, 9599, 6919, 8178, 5329, 12059, 7283, 5...   \n",
       "3  [41761, 9419, 7582, 8340, 4600, 12463, 7532, 5...   \n",
       "4  [41624, 9780, 5801, 8449, 7275, 12831, 6554, 4...   \n",
       "\n",
       "                   test preds steady subj1 avg2folds  \\\n",
       "0  [35938, 6736, 4054, 8315, 14667, 13311, 6616, ...   \n",
       "1  [36772, 6440, 1152, 2149, 12637, 10751, 11682,...   \n",
       "2  [37288, 6162, 1317, 1868, 12776, 9548, 12970, ...   \n",
       "3  [37119, 4278, 669, 2142, 10619, 10303, 13695, ...   \n",
       "4  [37817, 4891, 587, 2131, 13207, 13443, 9379, 1...   \n",
       "\n",
       "                   test preds steady subj2 avg2folds  \\\n",
       "0  [52412, 5515, 9404, 6059, 5992, 7132, 7545, 3343]   \n",
       "1  [54657, 4608, 5486, 4963, 6891, 7367, 8347, 5083]   \n",
       "2  [54338, 4553, 5447, 4571, 6561, 7800, 9158, 4974]   \n",
       "3  [54680, 4673, 4749, 3715, 7377, 10138, 7444, 4...   \n",
       "4  [51831, 4557, 5239, 5185, 7867, 9730, 8153, 4840]   \n",
       "\n",
       "                   test preds steady subj3 avg2folds  \\\n",
       "0  [43907, 6080, 6358, 5767, 6250, 12930, 7669, 8...   \n",
       "1  [46584, 4125, 3725, 5947, 7004, 14735, 7608, 7...   \n",
       "2  [46627, 3959, 3552, 5800, 6408, 14465, 9017, 7...   \n",
       "3  [45744, 4670, 3706, 4408, 7799, 14860, 8604, 7...   \n",
       "4  [44628, 5419, 3706, 5264, 8743, 12586, 8489, 8...   \n",
       "\n",
       "                   test preds steady subj4 avg2folds  \\\n",
       "0  [52507, 9225, 6841, 8243, 4319, 4976, 6943, 4276]   \n",
       "1  [51987, 6844, 5936, 9836, 4849, 6425, 5300, 6153]   \n",
       "2  [52197, 6782, 6572, 9399, 4661, 6285, 5805, 5629]   \n",
       "3  [51992, 6378, 6818, 9223, 4937, 6060, 5890, 6032]   \n",
       "4  [52592, 6600, 7631, 8541, 4727, 5305, 6335, 5599]   \n",
       "\n",
       "                   test preds steady subj5 avg2folds  \\\n",
       "0  [43927, 4629, 5382, 5503, 8591, 11586, 9715, 7...   \n",
       "1  [43109, 5944, 3569, 6561, 6262, 12863, 11283, ...   \n",
       "2  [43427, 6211, 4223, 6580, 5429, 13467, 10901, ...   \n",
       "3  [41153, 6233, 6108, 7453, 4625, 15256, 9187, 6...   \n",
       "4  [43232, 6764, 5568, 7903, 5554, 13369, 9646, 4...   \n",
       "\n",
       "                   test preds steady subj6 avg2folds  \\\n",
       "0  [41238, 7555, 7907, 4505, 10329, 7195, 13820, ...   \n",
       "1  [40364, 7385, 12633, 8298, 6588, 7956, 10480, ...   \n",
       "2  [40357, 7267, 11969, 9018, 6715, 8443, 9762, 3...   \n",
       "3  [39608, 7525, 11169, 9726, 6539, 6973, 11153, ...   \n",
       "4  [37701, 8040, 9797, 8606, 8705, 8349, 11502, 4...   \n",
       "\n",
       "                   test preds steady subj7 avg2folds  \\\n",
       "0  [50177, 14734, 9339, 5481, 5929, 3312, 5313, 3...   \n",
       "1  [55187, 7565, 6659, 4363, 7789, 2099, 7542, 6738]   \n",
       "2  [56361, 7662, 5680, 3704, 7903, 2143, 6538, 7951]   \n",
       "3  [60333, 6472, 5313, 4297, 6920, 2001, 5794, 6812]   \n",
       "4  [53209, 8349, 5806, 5042, 8848, 2572, 9339, 4777]   \n",
       "\n",
       "                   test preds steady subj8 avg2folds  \\\n",
       "0  [60935, 11040, 5879, 4635, 3148, 5437, 4932, 3...   \n",
       "1  [73854, 5954, 5725, 3231, 2976, 2867, 2765, 1696]   \n",
       "2  [72645, 6148, 6041, 3017, 3463, 3107, 2707, 1940]   \n",
       "3  [70701, 8569, 5395, 3763, 2805, 2938, 3135, 1762]   \n",
       "4  [70069, 7807, 3504, 3382, 5400, 3586, 3676, 1644]   \n",
       "\n",
       "                   test preds steady subj9 avg2folds  \n",
       "0  [41073, 4776, 8771, 8421, 10535, 13310, 5036, ...  \n",
       "1  [42456, 4584, 4505, 8121, 15070, 13056, 5290, ...  \n",
       "2  [42043, 4007, 4128, 7985, 16062, 12964, 5651, ...  \n",
       "3  [41551, 4838, 3303, 7261, 17930, 12066, 5486, ...  \n",
       "4  [45676, 3219, 3911, 8102, 17511, 10488, 3914, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temponet_pretrain_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7651447296142578, 0.6414129734039307, 0.6377828598022461, 0.6779122829437256, 0.8364046096801758, 0.6847075462341309, 0.6521005153656005, 0.5990354061126709, 0.5582716941833497, 0.6295568943023682\n",
      "0.7756202220916748, 0.6988006591796875, 0.6488565444946289, 0.6804637908935547, 0.8109169006347656, 0.6716788291931153, 0.6053022384643555, 0.5720909595489502, 0.48705320358276366, 0.5968821048736572\n"
     ]
    }
   ],
   "source": [
    "r = get_rows(temponet_pretrain_9,\n",
    "                       {'pretraining': 'all_others_min20-20', })\n",
    "x = [f'test accuracy steady subj{s} avg2folds' for s in range(10)]\n",
    "\n",
    "print(', '.join(map(str, r.iloc[0][x])))\n",
    "\n",
    "\n",
    "r = get_rows(temponet_pretrain_9,\n",
    "                       {'pretraining': 'no', })\n",
    "print(', '.join(map(str, r.iloc[0][x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_vit_2 = \\\n",
    "    read_results(\"exp10/no_pretraining_/results_1630304612.pickle\", {\"pretraining\": False}) \\\n",
    "    .append(read_results(\"exp10/no_pretraining_/results_1630304773.pickle\", {\"pretraining\": False}), ignore_index=True) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7582435131072998, 0.6736855030059814, 0.6501219749450684, 0.6751233100891113, 0.8117460250854492, 0.603399658203125, 0.6404040336608887, 0.6049529552459717, 0.54921875, 0.6059015274047852\n",
      "0.7438871383666992, 0.6690134525299072, 0.5250726699829101, 0.6583911418914795, 0.7826356887817383, 0.6459615230560303, 0.5689082145690918, 0.5522381782531738, 0.5183722972869873, 0.5694052219390869\n"
     ]
    }
   ],
   "source": [
    "r = get_rows(vit_pretrain_9,\n",
    "                       {'pretraining': 'all_others_9.5', 'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1})\n",
    "x = [f'test accuracy steady subj{s} avg2folds' for s in range(10)]\n",
    "\n",
    "print(', '.join(map(str, r.iloc[0][x])))\n",
    "\n",
    "\n",
    "r = get_rows(all_res_vit_2,\n",
    "                       {'pretraining': False, 'patch_size': 10, 'dim_projection': 64, 'dim_ff': 128, 'dim_head': 32, 'n_heads': 8, 'depth': 1})\n",
    "print(', '.join(map(str, r.iloc[0][x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5), ncols=2, sharey=True)\n",
    "ax[1].yaxis.set_tick_params(labelleft=True)\n",
    "\n",
    "p = {'all_others_9.5': 'pretraining', 'no': 'no pretraining'}\n",
    "v = {}\n",
    "\n",
    "for ii in [0, 1]:\n",
    "\n",
    "    for i, pretraining in enumerate(p.keys()):\n",
    "        r = get_rows([vit_pretrain_9, temponet_pretrain_9][ii],\n",
    "                       {'pretraining': pretraining, })\n",
    "\n",
    "        x = [f'test accuracy steady subj{s} avg2folds' for s in range(10)]\n",
    "        y = r.iloc[0][x]\n",
    "\n",
    "        ax[ii].scatter(np.arange(1, 11) + i / 20, y, label=p[pretraining])\n",
    "        \n",
    "        v[pretraining] = r[[f'test accuracy steady subj{s} avg2folds' for s in [0, 3, 4, 5, 6, 7, 8, 9]]].mean(axis=1, numeric_only=True).iloc[0]\n",
    "        ax[ii].hlines(v[pretraining], xmin=.5, xmax=10.5, color=f'C{i}')\n",
    "        \n",
    "        params = r['params'].iloc[0] / 1e3\n",
    "        MACs = r['MACs'].iloc[0] / 1e6\n",
    "\n",
    "    ax[ii].legend()\n",
    "    ax[ii].grid('on')\n",
    "    ax[ii].set_xticks(np.arange(1, 11, 1.0))\n",
    "    ax[ii].set_title(['ViT Patch 10 heads 8 depth 1', 'TEMPONet'][ii] + \"\\nparams={:.2f}k, MACS={:.2f}M\".format(params, MACs))\n",
    "    ax[ii].set_xlim([.5,10.5])\n",
    "    \n",
    "    p = {'all_others_min20-20': 'pretraining', 'no': 'no pretraining'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
