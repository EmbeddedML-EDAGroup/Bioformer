{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(n_tokens, dim, dim_head):\n",
    "    macs = 0\n",
    "    \n",
    "    # token -> k, q, v\n",
    "    macs += n_tokens * 3 * dim * dim_head\n",
    "    \n",
    "    # q * k'\n",
    "    # (n_tokens, dim_head) * (dim_head, n_tokens) -> (n_tokens, n_tokens)\n",
    "    macs += n_tokens * dim_head * n_tokens\n",
    "    \n",
    "    # Softmax e diviso sqrt(dim_head) \n",
    "    # ...\n",
    "    \n",
    "    # (q * k') * v\n",
    "    # (n_tokens, n_tokens) * (n_tokens, dim_head) -> (n_tokens, dim_head)\n",
    "    macs += n_tokens * n_tokens * dim_head\n",
    "    \n",
    "    return macs\n",
    "    \n",
    "def attention(n_tokens, dim, dim_head, n_heads):\n",
    "    macs = 0\n",
    "    \n",
    "    macs += n_heads * attention_head(n_tokens, dim, dim_head)\n",
    "    \n",
    "    # Riporta gli z concatenati a dimensione dim\n",
    "    macs += n_tokens * (dim_head * n_heads) * dim if not (n_heads == 1 and dim_head == dim) else 0\n",
    "    \n",
    "    return macs\n",
    "\n",
    "def conv(n_tokens, dim, n_heads):\n",
    "    return n_tokens * (n_heads * dim * dim)\n",
    "\n",
    "def feed_forward(n_tokens, dim, mlp_dim):\n",
    "    # 2 Linear: dim -> mlp_dim, mlp_dim -> dim\n",
    "    return n_tokens * dim * mlp_dim * 2\n",
    "\n",
    "def transformer(n_tokens, dim, dim_head, n_heads, mlp_dim, depth, n_convs):\n",
    "    assert n_convs < depth\n",
    "    return (depth - n_convs) * (attention(n_tokens, dim, dim_head, n_heads) + feed_forward(n_tokens, dim, mlp_dim)) \\\n",
    "            + (n_convs) * (conv(n_tokens, dim, n_heads) + feed_forward(n_tokens, dim, mlp_dim))\n",
    "\n",
    "def convit(patch_size, dim, dim_head, n_heads, mlp_dim, depth, n_convs):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth, n_convs)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(n_tokens, dim, dim_head):\n",
    "    macs = 0\n",
    "    \n",
    "    # token -> k, q, v\n",
    "    macs += n_tokens * 3 * dim * dim_head\n",
    "    \n",
    "    # q * k'\n",
    "    # (n_tokens, dim_head) * (dim_head, n_tokens) -> (n_tokens, n_tokens)\n",
    "    macs += n_tokens * dim_head * n_tokens\n",
    "    \n",
    "    # Softmax e diviso sqrt(dim_head) \n",
    "    # ...\n",
    "    \n",
    "    # (q * k') * v\n",
    "    # (n_tokens, n_tokens) * (n_tokens, dim_head) -> (n_tokens, dim_head)\n",
    "    macs += n_tokens * n_tokens * dim_head\n",
    "    \n",
    "    return macs\n",
    "    \n",
    "def attention(n_tokens, dim, dim_head, n_heads):\n",
    "    macs = 0\n",
    "    \n",
    "    macs += n_heads * attention_head(n_tokens, dim, dim_head)\n",
    "    \n",
    "    # Riporta gli z concatenati a dimensione dim\n",
    "    macs += n_tokens * (dim_head * n_heads) * dim if not (n_heads == 1 and dim_head == dim) else 0\n",
    "    \n",
    "    return macs\n",
    "\n",
    "def feed_forward(n_tokens, dim, mlp_dim):\n",
    "    # 2 Linear: dim -> mlp_dim, mlp_dim -> dim\n",
    "    return n_tokens * dim * mlp_dim * 2\n",
    "\n",
    "def transformer(n_tokens, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    return depth * (attention(n_tokens, dim, dim_head, n_heads) + feed_forward(n_tokens, dim, mlp_dim))\n",
    "\n",
    "def vit(patch_size, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2015-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the CC-by-NC license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "'''These modules are adapted from those of timm, see\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class ConvAtt(nn.Module):\n",
    "    def __init__(self, dim, kernel_size):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class GPSA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 locality_strength=1., use_local_init=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)       \n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)       \n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.pos_proj = nn.Linear(3, num_heads)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.locality_strength = locality_strength\n",
    "        self.gating_param = nn.Parameter(torch.ones(self.num_heads))\n",
    "        self.apply(self._init_weights)\n",
    "        if use_local_init:\n",
    "            self.local_init(locality_strength=locality_strength)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        if not hasattr(self, 'rel_indices') or self.rel_indices.size(1)!=N:\n",
    "            self.get_rel_indices(N)\n",
    "\n",
    "        attn = self.get_attention(x)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention(self, x):\n",
    "        B, N, C = x.shape        \n",
    "        qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k = qk[0], qk[1]\n",
    "        pos_score = self.rel_indices.expand(B, -1, -1,-1)\n",
    "        pos_score = self.pos_proj(pos_score).permute(0,3,1,2) \n",
    "        patch_score = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        patch_score = patch_score.softmax(dim=-1)\n",
    "        pos_score = pos_score.softmax(dim=-1)\n",
    "\n",
    "        gating = self.gating_param.view(1,-1,1,1)\n",
    "        attn = (1.-torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score\n",
    "        attn /= attn.sum(dim=-1).unsqueeze(-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        return attn\n",
    "\n",
    "    def get_attention_map(self, x, return_map = False):\n",
    "\n",
    "        attn_map = self.get_attention(x).mean(0) # average over batch\n",
    "        distances = self.rel_indices.squeeze()[:,:,-1]**.5\n",
    "        dist = torch.einsum('nm,hnm->h', (distances, attn_map))\n",
    "        dist /= distances.size(0)\n",
    "        if return_map:\n",
    "            return dist, attn_map\n",
    "        else:\n",
    "            return dist\n",
    "    \n",
    "    def local_init(self, locality_strength=1.):\n",
    "        \n",
    "        self.v.weight.data.copy_(torch.eye(self.dim))\n",
    "        locality_distance = 1 #max(1,1/locality_strength**.5)\n",
    "        \n",
    "        kernel_size = int(self.num_heads**.5)\n",
    "        center = (kernel_size-1)/2 if kernel_size%2==0 else kernel_size//2\n",
    "        for h1 in range(kernel_size):\n",
    "            for h2 in range(kernel_size):\n",
    "                position = h1+kernel_size*h2\n",
    "                self.pos_proj.weight.data[position,2] = -1\n",
    "                self.pos_proj.weight.data[position,1] = 2*(h1-center)*locality_distance\n",
    "                self.pos_proj.weight.data[position,0] = 2*(h2-center)*locality_distance\n",
    "        self.pos_proj.weight.data *= locality_strength\n",
    "\n",
    "    def get_rel_indices(self, num_patches):\n",
    "        #img_size = int(num_patches**.5)\n",
    "        #rel_indices   = torch.zeros(1, num_patches, num_patches, 3)\n",
    "        #ind = torch.arange(img_size).view(1,-1) - torch.arange(img_size).view(-1, 1)\n",
    "        #indx = ind.repeat(img_size,img_size)\n",
    "        #indy = ind.repeat_interleave(img_size,dim=0).repeat_interleave(img_size,dim=1)\n",
    "        #indd = indx**2 + indy**2\n",
    "        #rel_indices[:,:,:,2] = indd.unsqueeze(0)\n",
    "        #rel_indices[:,:,:,1] = indy.unsqueeze(0)\n",
    "        #rel_indices[:,:,:,0] = indx.unsqueeze(0)\n",
    "        #device = self.qk.weight.device\n",
    "        #self.rel_indices = rel_indices.to(device)\n",
    "\n",
    "        img_size = (1, 300)\n",
    "        patch_size = (1, 300 // num_patches)\n",
    "\n",
    "        num_patches_x = img_size[1] // patch_size[1]\n",
    "        num_patches_y = img_size[0] // patch_size[0]\n",
    "\n",
    "        rel_indices = torch.zeros(1, num_patches_x * num_patches_y, num_patches_x * num_patches_y, 3)\n",
    "        ind = torch.arange(num_patches_x).view(1,-1) - torch.arange(num_patches_x).view(-1, 1)\n",
    "        indx = ind.repeat(num_patches_y, 1).repeat(1, num_patches_y)\n",
    "        ind = torch.arange(num_patches_y).view(1,-1) - torch.arange(num_patches_y).view(-1, 1)\n",
    "        indy = ind.repeat_interleave(num_patches_y, dim=0).repeat_interleave(num_patches_y, dim=1)\n",
    "        indd = indx**2 + indy**2\n",
    "        rel_indices[:,:,:,2] = indd.unsqueeze(0)\n",
    "        rel_indices[:,:,:,1] = indy.unsqueeze(0)\n",
    "        rel_indices[:,:,:,0] = indx.unsqueeze(0)\n",
    "\n",
    "        device = self.qk.weight.device\n",
    "        self.rel_indices = rel_indices.to(device)\n",
    "\n",
    " \n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_attention_map(self, x, return_map = False):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_map = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_map = attn_map.softmax(dim=-1).mean(0)\n",
    "\n",
    "        img_size = int(N**.5)\n",
    "        ind = torch.arange(img_size).view(1,-1) - torch.arange(img_size).view(-1, 1)\n",
    "        indx = ind.repeat(img_size,img_size)\n",
    "        indy = ind.repeat_interleave(img_size,dim=0).repeat_interleave(img_size,dim=1)\n",
    "        indd = indx**2 + indy**2\n",
    "        distances = indd**.5\n",
    "        distances = distances.to('cuda')\n",
    "\n",
    "        dist = torch.einsum('nm,hnm->h', (distances, attn_map))\n",
    "        dist /= N\n",
    "        \n",
    "        if return_map:\n",
    "            return dist, attn_map\n",
    "        else:\n",
    "            return dist\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads,  mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_conv=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        #self.use_gpsa = use_gpsa\n",
    "        if use_conv:\n",
    "            #self.attn = GPSA(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, **kwargs)\n",
    "            self.attn = ConvAtt(dim, kernel_size=num_heads)\n",
    "        else:\n",
    "            self.attn = MHSA(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, **kwargs)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding, from timm\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.apply(self._init_weights)\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "class HybridEmbed(nn.Module):\n",
    "    \"\"\" CNN Feature Map Embedding, from timm\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "        img_size = to_2tuple(img_size)\n",
    "        self.img_size = img_size\n",
    "        self.backbone = backbone\n",
    "        if feature_size is None:\n",
    "            with torch.no_grad():\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        else:\n",
    "            feature_size = to_2tuple(feature_size)\n",
    "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "        self.num_patches = feature_size[0] * feature_size[1]\n",
    "        self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)[-1]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=48, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, global_pool=None,\n",
    "                 conv_layers=(-1,), locality_strength=1., use_pos_embed=True, **extra):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"ignored params\", extra)\n",
    "\n",
    "        embed_dim *= num_heads\n",
    "        self.num_classes = num_classes\n",
    "        #self.local_up_to_layer = local_up_to_layer\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.locality_strength = locality_strength\n",
    "        self.use_pos_embed = use_pos_embed\n",
    "\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        \"\"\"self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                use_gpsa=True,\n",
    "                locality_strength=locality_strength)\n",
    "            if i<local_up_to_layer else\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                use_gpsa=False)\n",
    "            for i in range(depth)])\"\"\"\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                use_conv=i in conv_layers,\n",
    "                )\n",
    "            for i in range(depth)\n",
    "            ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.head.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for u,blk in enumerate(self.blocks):\n",
    "            if u == 0: #self.local_up_to_layer :\n",
    "                x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored params {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "241160"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in VisionTransformer(**{\n",
    "    \"img_size\": (1, 300),\n",
    "    \"patch_size\": (1, 10),\n",
    "    \"in_chans\": 14,\n",
    "    \"num_classes\": 8, \n",
    "    \n",
    "    \"depth\": 3,\n",
    "    \"num_heads\": 3,\n",
    "    \"embed_dim\": 32,\n",
    "    \"mlp_ratio\": 2.,\n",
    "\n",
    "    \"drop_rate\": 0,\n",
    "    \"conv_layers\": (0, 1),\n",
    "}).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored params {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "315752"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in VisionTransformer(**{\n",
    "    \"img_size\": (1, 300),\n",
    "    \"patch_size\": (1, 10),\n",
    "    \"in_chans\": 14,\n",
    "    \"num_classes\": 8, \n",
    "    \n",
    "    \"depth\": 4,\n",
    "    \"num_heads\": 3,\n",
    "    \"embed_dim\": 32,\n",
    "    \"mlp_ratio\": 2.,\n",
    "\n",
    "    \"drop_rate\": 0,\n",
    "    \"conv_layers\": (0, 1, 2),\n",
    "}).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored params {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "613736"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in VisionTransformer(**{\n",
    "    \"img_size\": (1, 300),\n",
    "    \"patch_size\": (1, 10),\n",
    "    \"in_chans\": 14,\n",
    "    \"num_classes\": 8, \n",
    "    \n",
    "    \"depth\": 8,\n",
    "    \"num_heads\": 3,\n",
    "    \"embed_dim\": 32,\n",
    "    \"mlp_ratio\": 2.,\n",
    "\n",
    "    \"drop_rate\": 0,\n",
    "    \"conv_layers\": (0, 1, 2),\n",
    "}).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2169984"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convit(patch_size=10, dim=64, dim_head=32, n_heads=2, mlp_dim=128, depth=2, n_convs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2546944"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vit(patch_size, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth, n_convs=0)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs\n",
    "\n",
    "vit(patch_size=10, dim=64, dim_head=32, n_heads=2, mlp_dim=128, depth=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652416"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convit(patch_size=10, dim=32, dim_head=32, n_heads=3, mlp_dim=128, depth=3, n_convs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001600"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convit(patch_size=10, dim=32, dim_head=32, n_heads=3, mlp_dim=128, depth=4, n_convs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279168"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convit(patch_size=10, dim=32, dim_head=32, n_heads=3, mlp_dim=128, depth=8, n_convs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
