{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(n_tokens, dim, dim_head):\n",
    "    macs = 0\n",
    "    \n",
    "    # token -> k, q, v\n",
    "    macs += n_tokens * 3 * dim * dim_head\n",
    "    \n",
    "    # q * k'\n",
    "    # (n_tokens, dim_head) * (dim_head, n_tokens) -> (n_tokens, n_tokens)\n",
    "    macs += n_tokens * dim_head * n_tokens\n",
    "    \n",
    "    # Softmax e diviso sqrt(dim_head) \n",
    "    # ...\n",
    "    \n",
    "    # (q * k') * v\n",
    "    # (n_tokens, n_tokens) * (n_tokens, dim_head) -> (n_tokens, dim_head)\n",
    "    macs += n_tokens * n_tokens * dim_head\n",
    "    \n",
    "    return macs\n",
    "    \n",
    "def attention(n_tokens, dim, dim_head, n_heads):\n",
    "    macs = 0\n",
    "    \n",
    "    macs += n_heads * attention_head(n_tokens, dim, dim_head)\n",
    "    \n",
    "    # Riporta gli z concatenati a dimensione dim\n",
    "    macs += n_tokens * (dim_head * n_heads) * dim if not (n_heads == 1 and dim_head == dim) else 0\n",
    "    \n",
    "    return macs\n",
    "\n",
    "def feed_forward(n_tokens, dim, mlp_dim):\n",
    "    # 2 Linear: dim -> mlp_dim, mlp_dim -> dim\n",
    "    return n_tokens * dim * mlp_dim * 2\n",
    "\n",
    "def transformer(n_tokens, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    return depth * (attention(n_tokens, dim, dim_head, n_heads) + feed_forward(n_tokens, dim, mlp_dim))\n",
    "\n",
    "def vit(patch_size, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "               \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = qkv.chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., use_cls_token=True):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        #self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "        #nn.init.kaiming_uniform_(self.pos_embedding, a=5 ** .5)\n",
    "        nn.init.normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "        #self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.zeros_(self.cls_token)\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # MACs: patch_size * n_patches * dim, es (30 * 14) * 10 * 300\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # FeedForward    Attention       project out\n",
    "        # 300*300*10*2 + 300*(64*3)*10 + ((64)*300*10)\n",
    "        # Attention -> manca softmax e attention vera e propria, c'è solo linear encoding a qkv\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        \n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "    \n",
    "# Ratio of params\n",
    "def vit_aff_ratio(patch_size, dim, dim_head, n_heads, mlp_dim, depth): \n",
    "    n_tokens = 300 // patch_size + 1\n",
    "    \n",
    "    a = (((dim) * dim_head * 3 * n_heads) + ((dim_head * n_heads) * dim) + dim)\n",
    "    ff = dim * mlp_dim * 2 + mlp_dim + dim\n",
    "    \n",
    "    return a / (a + ff)\n",
    "\n",
    "def get_results(configs, results_, additional_columns, extract_model_hparams):\n",
    "        \n",
    "    acccs = []\n",
    "    acccs_steady = []\n",
    "    acccs_val0 = []\n",
    "    acccs_steady_val0 = []\n",
    "    acccs_val1 = []\n",
    "    acccs_steady_val1 = []\n",
    "\n",
    "    acccs_val_val0 = 0\n",
    "    acccs_val_val1 = 0\n",
    "    acccs_train_val0 = 0\n",
    "    acccs_train_val1 = 0\n",
    "    \n",
    "    acccs_steady_persubject = np.array([0] * 10, dtype=float)\n",
    "    preds_steady_bincounts_subject = np.zeros((10, 8), dtype=int)\n",
    "    \n",
    "    for config, r in zip(configs, results_):\n",
    "\n",
    "        accs = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "        #    accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #accs /= 2\n",
    "\n",
    "        accs_steady = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "        #    accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #accs_steady /= 2\n",
    "\n",
    "        accs_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        accs_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        #for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "        #    accs_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        #for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "        #    accs_steady_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #    acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #    preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        acccs_train_val0 += r['val-fold_0']['losses_accs'][-1]['train_acc']\n",
    "        #acccs_train_val1 += r['val-fold_1']['losses_accs'][-1]['train_acc']\n",
    "\n",
    "        acccs_val_val0 += r['val-fold_0']['losses_accs'][-1]['val_acc']\n",
    "        #acccs_val_val1 += r['val-fold_1']['losses_accs'][-1]['val_acc']\n",
    "\n",
    "        acccs.append(accs)\n",
    "        acccs_steady.append(accs_steady)\n",
    "        acccs_val0.append(accs_val0)\n",
    "        acccs_steady_val0.append(accs_steady_val0)\n",
    "        acccs_val1.append(accs_val1)\n",
    "        acccs_steady_val1.append(accs_steady_val1)\n",
    "        \n",
    "    test_sessions = len(r['test_sessions'])\n",
    "\n",
    "    acccs_steady_persubject /= test_sessions #* 2 # 5 sessioni per due\n",
    "\n",
    "    acccs = np.array(acccs).mean(axis=0)\n",
    "\n",
    "    acccs_steady_ = np.array(acccs_steady).mean(axis=1)\n",
    "    acccs_steady = np.array(acccs_steady).mean(axis=0)\n",
    "\n",
    "    acccs_val0 = np.array(acccs_val0).mean(axis=0)\n",
    "    acccs_val1 = np.array(acccs_val1).mean(axis=0)\n",
    "    acccs_steady_val0 = np.array(acccs_steady_val0).mean(axis=0)\n",
    "    acccs_steady_val1 = np.array(acccs_steady_val1).mean(axis=0)\n",
    "    acccs_val_val0 /= 10\n",
    "    acccs_val_val1 /= 10\n",
    "    acccs_train_val0 /= 10\n",
    "    acccs_train_val1 /= 10\n",
    "    \n",
    "    model_hparams = extract_model_hparams(config)\n",
    "\n",
    "    return {        \n",
    "        **model_hparams,\n",
    "\n",
    "        **additional_columns,\n",
    "\n",
    "        \"train accuracy steady fold1\":  acccs_train_val0,\n",
    "        \"train accuracy steady fold2\":  acccs_train_val1,\n",
    "        \"train accuracy steady avg2folds\": .5 * (acccs_train_val0 + acccs_train_val1),\n",
    "\n",
    "        \"validation accuracy steady fold1\": acccs_val_val0,\n",
    "        \"validation accuracy steady fold2\": acccs_val_val1,\n",
    "\n",
    "        \"test accuracy fold1\": acccs_val0.mean(),\n",
    "        \"test accuracy fold2\": acccs_val1.mean(),\n",
    "        \"test accuracy avg2folds\": acccs.mean(), \n",
    "        \"test accuracy steady fold1\": acccs_steady_val0.mean(),\n",
    "        \"test accuracy steady fold2\": acccs_steady_val1.mean(), \n",
    "        \"test accuracy steady avg2folds\": acccs_steady.mean(),\n",
    "\n",
    "        \"test accuracy steady avg2folds std across sessions\": acccs_steady.std(),\n",
    "        \"test accuracy steady avg2folds std across subjects\": acccs_steady_.std(),\n",
    "        \n",
    "        **{\n",
    "          f\"test accuracy steady session{s + 1 + test_sessions} avg2folds\": acccs_steady[s] for s in range(test_sessions)\n",
    "        },\n",
    "        \n",
    "        **{\n",
    "            f\"test accuracy steady subj{s} avg2folds\": acccs_steady_persubject[s] for s in range(10)\n",
    "        },\n",
    "        \n",
    "        **{\n",
    "            f\"test preds steady subj{s} avg2folds\": preds_steady_bincounts_subject[s] for s in range(10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_configs(configs, group_exclude_columns):\n",
    "    # https://stackoverflow.com/a/6027615\n",
    "    import collections.abc\n",
    "\n",
    "    def flatten(d, parent_key='', sep='_'):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = parent_key + sep + k if parent_key else k\n",
    "            if isinstance(v, collections.abc.MutableMapping):\n",
    "                items.extend(flatten(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    df = pd.DataFrame([flatten(config) for config in configs])\n",
    "    df['row_id'] = [[a] for a in df.index]\n",
    "    \n",
    "    if 'training_config_lr_scheduler_hparams_milestones' in df:\n",
    "        df['training_config_lr_scheduler_hparams_milestones'] = df['training_config_lr_scheduler_hparams_milestones'].apply(lambda x: ','.join(map(str, x)))\n",
    "    df = df.fillna('null')\n",
    "    \n",
    "    grouped_configs = df.groupby([c for c in df.columns if c not in group_exclude_columns]).agg({'subjects': 'count', 'row_id': 'sum'})\n",
    "    \n",
    "    if (grouped_configs['subjects'] != 10).sum() != 0:\n",
    "        display(grouped_configs)\n",
    "        raise ValueError(\"For every config, it is assumed that you trained on 10 subjects\")\n",
    "    \n",
    "    return list(grouped_configs[\"row_id\"])\n",
    "\n",
    "\n",
    "extract_model_hparams_generator = {\n",
    "    'vit': lambda config: {\n",
    "        \"window_size\": config[\"image_size\"][1],\n",
    "        \"patch_size\": config[\"patch_size\"][1],\n",
    "        \"dim_projection\": config[\"dim\"],\n",
    "        \"dim_ff\": config[\"mlp_dim\"],\n",
    "        \"dim_head\": config[\"dim_head\"],\n",
    "        \"n_heads\": config[\"heads\"],\n",
    "        \"depth\": config[\"depth\"],\n",
    "        \"dropout\": config[\"dropout\"],\n",
    "        \"emb_dropout\": config[\"emb_dropout\"],\n",
    "        \n",
    "        \"MACs\": vit(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \"params\":  sum([param.nelement() for param in ViT(image_size=(1, 300), patch_size=config[\"patch_size\"], dim=config[\"dim\"], dim_head=config[\"dim_head\"], heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"], num_classes=8).parameters()]),\n",
    "        \"params_aff_ratio\": vit_aff_ratio(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \n",
    "    },\n",
    "    'temponet': lambda _: {\n",
    "        \"MACs\": 16028672,\n",
    "        \"params\": 461512,\n",
    "    },\n",
    "    \"convit\": lambda _: {}\n",
    "}\n",
    "\n",
    "def read_results(filename, additional_columns=None, group_exclude_columns=None, model_name='vit'):\n",
    "    additional_columns = {} if additional_columns is None else additional_columns\n",
    "    \n",
    "    group_exclude_columns = set() if group_exclude_columns is None else group_exclude_columns\n",
    "    group_exclude_columns = group_exclude_columns.union({'subjects', 'row_id'})\n",
    "    \n",
    "    configs, results_ = load(open(filename, 'rb'))\n",
    "    \n",
    "    groups_indices = group_configs(configs, group_exclude_columns)\n",
    "    \n",
    "    df_l = []\n",
    "    for idx in groups_indices:\n",
    "        c = [configs[i] for i in idx]\n",
    "        r = [results_[i] for i in idx]\n",
    "        \n",
    "        df_l.append(get_results(c, r, additional_columns, extract_model_hparams_generator[model_name]))   \n",
    "    \n",
    "    return pd.DataFrame(df_l) \n",
    "\n",
    "def get_rows(all_res_vit, group):\n",
    "    m = None\n",
    "    for k in group.keys():\n",
    "        current_m = all_res_vit[k] == group[k]\n",
    "        if m is None:\n",
    "            m = current_m\n",
    "        else:\n",
    "            m &= current_m\n",
    "    return all_res_vit[m].copy()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"pretrain/train_conv/results_1631106425.pickle\",additional_columns={\"conv\": \"all\", \"pretrain\": \"no\"}, model_name='convit').\n",
    "append(read_results(\"pretrain/train_conv/results_1631106425.pickle\",additional_columns={\"conv\": \"all\", \"pretrain\": \"no\"}, model_name='convit') ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6342636161380344"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[f\"test accuracy steady subj{i} avg2folds\" for i in range(10) if i != 2]].iloc[0].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
