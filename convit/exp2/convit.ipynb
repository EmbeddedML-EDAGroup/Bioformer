{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(n_tokens, dim, dim_head):\n",
    "    macs = 0\n",
    "    \n",
    "    # token -> k, q, v\n",
    "    macs += n_tokens * 3 * dim * dim_head\n",
    "    \n",
    "    # q * k'\n",
    "    # (n_tokens, dim_head) * (dim_head, n_tokens) -> (n_tokens, n_tokens)\n",
    "    macs += n_tokens * dim_head * n_tokens\n",
    "    \n",
    "    # Softmax e diviso sqrt(dim_head) \n",
    "    # ...\n",
    "    \n",
    "    # (q * k') * v\n",
    "    # (n_tokens, n_tokens) * (n_tokens, dim_head) -> (n_tokens, dim_head)\n",
    "    macs += n_tokens * n_tokens * dim_head\n",
    "    \n",
    "    return macs\n",
    "    \n",
    "def attention(n_tokens, dim, dim_head, n_heads):\n",
    "    macs = 0\n",
    "    \n",
    "    macs += n_heads * attention_head(n_tokens, dim, dim_head)\n",
    "    \n",
    "    # Riporta gli z concatenati a dimensione dim\n",
    "    macs += n_tokens * (dim_head * n_heads) * dim if not (n_heads == 1 and dim_head == dim) else 0\n",
    "    \n",
    "    return macs\n",
    "\n",
    "def feed_forward(n_tokens, dim, mlp_dim):\n",
    "    # 2 Linear: dim -> mlp_dim, mlp_dim -> dim\n",
    "    return n_tokens * dim * mlp_dim * 2\n",
    "\n",
    "def transformer(n_tokens, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    return depth * (attention(n_tokens, dim, dim_head, n_heads) + feed_forward(n_tokens, dim, mlp_dim))\n",
    "\n",
    "def vit(patch_size, dim, dim_head, n_heads, mlp_dim, depth):\n",
    "    macs = 0\n",
    "    \n",
    "    n_tokens = 300 // patch_size\n",
    "    \n",
    "    # linear embedding\n",
    "    macs += n_tokens * (14 * patch_size) * dim\n",
    "    \n",
    "    # +1 perché c'è cls_token\n",
    "    macs += transformer(n_tokens + 1, dim, dim_head, n_heads, mlp_dim, depth)\n",
    "    \n",
    "    # output\n",
    "    # Da mean o last token a class_scores\n",
    "    macs += dim * 8\n",
    "    \n",
    "    return macs\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "               \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = qkv.chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., use_cls_token=True):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        #self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.use_cls_token = use_cls_token\n",
    "        if self.use_cls_token:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches + 1, dim))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.empty(1, num_patches, dim))\n",
    "        #nn.init.kaiming_uniform_(self.pos_embedding, a=5 ** .5)\n",
    "        nn.init.normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "        #self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.zeros_(self.cls_token)\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # MACs: patch_size * n_patches * dim, es (30 * 14) * 10 * 300\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            x += self.pos_embedding[:, :(n + 1)]\n",
    "        else :\n",
    "            x += self.pos_embedding\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # FeedForward    Attention       project out\n",
    "        # 300*300*10*2 + 300*(64*3)*10 + ((64)*300*10)\n",
    "        # Attention -> manca softmax e attention vera e propria, c'è solo linear encoding a qkv\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        \n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "    \n",
    "# Ratio of params\n",
    "def vit_aff_ratio(patch_size, dim, dim_head, n_heads, mlp_dim, depth): \n",
    "    n_tokens = 300 // patch_size + 1\n",
    "    \n",
    "    a = (((dim) * dim_head * 3 * n_heads) + ((dim_head * n_heads) * dim) + dim)\n",
    "    ff = dim * mlp_dim * 2 + mlp_dim + dim\n",
    "    \n",
    "    return a / (a + ff)\n",
    "\n",
    "def get_results(configs, results_, additional_columns, extract_model_hparams):\n",
    "        \n",
    "    acccs = []\n",
    "    acccs_steady = []\n",
    "    acccs_val0 = []\n",
    "    acccs_steady_val0 = []\n",
    "    acccs_val1 = []\n",
    "    acccs_steady_val1 = []\n",
    "\n",
    "    acccs_val_val0 = 0\n",
    "    acccs_val_val1 = 0\n",
    "    acccs_train_val0 = 0\n",
    "    acccs_train_val1 = 0\n",
    "    \n",
    "    acccs_steady_persubject = np.array([0] * 10, dtype=float)\n",
    "    preds_steady_bincounts_subject = np.zeros((10, 8), dtype=int)\n",
    "    \n",
    "    for config, r in zip(configs, results_):\n",
    "\n",
    "        accs = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "        #    accs[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        #accs /= 2\n",
    "\n",
    "        accs_steady = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "            accs_steady[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "        accs_steady /= 2\n",
    "\n",
    "        accs_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds'], r['val-fold_0']['y_trues'])):\n",
    "            accs_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val0 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_0']['y_preds_steady'], r['val-fold_0']['y_trues_steady'])):\n",
    "            accs_steady_val0[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        accs_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds'], r['val-fold_1']['y_trues'])):\n",
    "            accs_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "\n",
    "        accs_steady_val1 = np.array([0] * len(r['test_sessions']), dtype=float)\n",
    "        for i, (y_pred, y_true) in enumerate(zip(r['val-fold_1']['y_preds_steady'], r['val-fold_1']['y_trues_steady'])):\n",
    "            accs_steady_val1[i] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            acccs_steady_persubject[r['subject'] - 1] += (y_pred == y_true).sum().float() / len(y_true)\n",
    "            preds_steady_bincounts_subject[r['subject'] - 1] += y_pred.bincount(minlength=(y_true.max() + 1)).numpy()\n",
    "            \n",
    "        acccs_train_val0 += r['val-fold_0']['losses_accs'][-1]['train_acc']\n",
    "        acccs_train_val1 += r['val-fold_1']['losses_accs'][-1]['train_acc']\n",
    "\n",
    "        acccs_val_val0 += r['val-fold_0']['losses_accs'][-1]['val_acc']\n",
    "        acccs_val_val1 += r['val-fold_1']['losses_accs'][-1]['val_acc']\n",
    "\n",
    "        acccs.append(accs)\n",
    "        acccs_steady.append(accs_steady)\n",
    "        acccs_val0.append(accs_val0)\n",
    "        acccs_steady_val0.append(accs_steady_val0)\n",
    "        acccs_val1.append(accs_val1)\n",
    "        acccs_steady_val1.append(accs_steady_val1)\n",
    "        \n",
    "    test_sessions = len(r['test_sessions'])\n",
    "\n",
    "    acccs_steady_persubject /= test_sessions * 2 # 5 sessioni per due\n",
    "\n",
    "    acccs = np.array(acccs).mean(axis=0)\n",
    "\n",
    "    acccs_steady_ = np.array(acccs_steady).mean(axis=1)\n",
    "    acccs_steady = np.array(acccs_steady).mean(axis=0)\n",
    "\n",
    "    acccs_val0 = np.array(acccs_val0).mean(axis=0)\n",
    "    acccs_val1 = np.array(acccs_val1).mean(axis=0)\n",
    "    acccs_steady_val0 = np.array(acccs_steady_val0).mean(axis=0)\n",
    "    acccs_steady_val1 = np.array(acccs_steady_val1).mean(axis=0)\n",
    "    acccs_val_val0 /= 10\n",
    "    acccs_val_val1 /= 10\n",
    "    acccs_train_val0 /= 10\n",
    "    acccs_train_val1 /= 10\n",
    "    \n",
    "    model_hparams = extract_model_hparams(config)\n",
    "\n",
    "    return {        \n",
    "        **model_hparams,\n",
    "\n",
    "        **additional_columns,\n",
    "\n",
    "        \"train accuracy steady fold1\":  acccs_train_val0,\n",
    "        \"train accuracy steady fold2\":  acccs_train_val1,\n",
    "        \"train accuracy steady avg2folds\": .5 * (acccs_train_val0 + acccs_train_val1),\n",
    "\n",
    "        \"validation accuracy steady fold1\": acccs_val_val0,\n",
    "        \"validation accuracy steady fold2\": acccs_val_val1,\n",
    "\n",
    "        \"test accuracy fold1\": acccs_val0.mean(),\n",
    "        \"test accuracy fold2\": acccs_val1.mean(),\n",
    "        \"test accuracy avg2folds\": acccs.mean(), \n",
    "        \"test accuracy steady fold1\": acccs_steady_val0.mean(),\n",
    "        \"test accuracy steady fold2\": acccs_steady_val1.mean(), \n",
    "        \"test accuracy steady avg2folds\": acccs_steady.mean(),\n",
    "\n",
    "        \"test accuracy steady avg2folds std across sessions\": acccs_steady.std(),\n",
    "        \"test accuracy steady avg2folds std across subjects\": acccs_steady_.std(),\n",
    "        \n",
    "        **{\n",
    "          f\"test accuracy steady session{s + 1 + test_sessions} avg2folds\": acccs_steady[s] for s in range(test_sessions)\n",
    "        },\n",
    "        \n",
    "        **{\n",
    "            f\"test accuracy steady subj{s} avg2folds\": acccs_steady_persubject[s] for s in range(10)\n",
    "        },\n",
    "        \n",
    "        **{\n",
    "            f\"test preds steady subj{s} avg2folds\": preds_steady_bincounts_subject[s] for s in range(10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_configs(configs, group_exclude_columns):\n",
    "    # https://stackoverflow.com/a/6027615\n",
    "    import collections.abc\n",
    "\n",
    "    def flatten(d, parent_key='', sep='_'):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = parent_key + sep + k if parent_key else k\n",
    "            if isinstance(v, collections.abc.MutableMapping):\n",
    "                items.extend(flatten(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    df = pd.DataFrame([flatten(config) for config in configs])\n",
    "    df['row_id'] = [[a] for a in df.index]\n",
    "    \n",
    "    if 'training_config_lr_scheduler_hparams_milestones' in df:\n",
    "        df['training_config_lr_scheduler_hparams_milestones'] = df['training_config_lr_scheduler_hparams_milestones'].apply(lambda x: ','.join(map(str, x)))\n",
    "    df = df.fillna('null')\n",
    "    \n",
    "    grouped_configs = df.groupby([c for c in df.columns if c not in group_exclude_columns]).agg({'subjects': 'count', 'row_id': 'sum'})\n",
    "    \n",
    "    if (grouped_configs['subjects'] != 10).sum() != 0:\n",
    "        display(grouped_configs)\n",
    "        raise ValueError(\"For every config, it is assumed that you trained on 10 subjects\")\n",
    "    \n",
    "    return list(grouped_configs[\"row_id\"])\n",
    "\n",
    "\n",
    "extract_model_hparams_generator = {\n",
    "    'vit': lambda config: {\n",
    "        \"window_size\": config[\"image_size\"][1],\n",
    "        \"patch_size\": config[\"patch_size\"][1],\n",
    "        \"dim_projection\": config[\"dim\"],\n",
    "        \"dim_ff\": config[\"mlp_dim\"],\n",
    "        \"dim_head\": config[\"dim_head\"],\n",
    "        \"n_heads\": config[\"heads\"],\n",
    "        \"depth\": config[\"depth\"],\n",
    "        \"dropout\": config[\"dropout\"],\n",
    "        \"emb_dropout\": config[\"emb_dropout\"],\n",
    "        \n",
    "        \"MACs\": vit(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \"params\":  sum([param.nelement() for param in ViT(image_size=(1, 300), patch_size=config[\"patch_size\"], dim=config[\"dim\"], dim_head=config[\"dim_head\"], heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"], num_classes=8).parameters()]),\n",
    "        \"params_aff_ratio\": vit_aff_ratio(patch_size=config[\"patch_size\"][1], dim=config[\"dim\"], dim_head=config[\"dim_head\"], n_heads=config[\"heads\"], mlp_dim=config[\"mlp_dim\"], depth=config[\"depth\"]),\n",
    "        \n",
    "    },\n",
    "    'temponet': lambda _: {\n",
    "        \"MACs\": 16028672,\n",
    "        \"params\": 461512,\n",
    "    },\n",
    "    \"convit\": lambda config: {\n",
    "        \"depth\": config[\"depth\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "def read_results(filename, additional_columns=None, group_exclude_columns=None, model_name='vit'):\n",
    "    additional_columns = {} if additional_columns is None else additional_columns\n",
    "    \n",
    "    group_exclude_columns = set() if group_exclude_columns is None else group_exclude_columns\n",
    "    group_exclude_columns = group_exclude_columns.union({'subjects', 'row_id'})\n",
    "    \n",
    "    configs, results_ = load(open(filename, 'rb'))\n",
    "    \n",
    "    groups_indices = group_configs(configs, group_exclude_columns)\n",
    "    \n",
    "    df_l = []\n",
    "    for idx in groups_indices:\n",
    "        c = [configs[i] for i in idx]\n",
    "        r = [results_[i] for i in idx]\n",
    "        \n",
    "        ac = additional_columns.copy()\n",
    "        ac[\"training_config\"] = c[0][\"training_config\"]\n",
    "        df_l.append(get_results(c, r, ac, extract_model_hparams_generator[model_name]))   \n",
    "    \n",
    "    return pd.DataFrame(df_l) \n",
    "\n",
    "def get_rows(all_res_vit, group):\n",
    "    m = None\n",
    "    for k in group.keys():\n",
    "        current_m = all_res_vit[k] == group[k]\n",
    "        if m is None:\n",
    "            m = current_m\n",
    "        else:\n",
    "            m &= current_m\n",
    "    return all_res_vit[m].copy()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"pretrain/train_conv/results_1631106425.pickle\",additional_columns={\"conv\": \"all\", \"pretrain\": \"no\"}, model_name='convit')\\\n",
    ".append(read_results(\"train_h3d8_adamw/train_conv/results_1631096710.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"no\"}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain/pretrain_conv/finetune25/results_1631115451.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes_nonfixed\", \"finetune\": 25}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain/pretrain_conv/finetune50/results_1631130033.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes_nonfixed\", \"finetune\": 50}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain/pretrain_conv/finetune75/results_1631124308.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes_nonfixed\", \"finetune\": 75}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain/pretrain_conv/finetune25_/results_1631175251.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes\", \"finetune\": 25}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain/pretrain_conv/finetune50_/results_1631177094.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes\", \"finetune\": 50}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain/pretrain_conv/finetune75_/results_1631178989.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes\", \"finetune\": 50}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\\n",
    ".append(read_results(\"pretrain_2/pretrain_conv/finetune25/results_1631285553.pickle\", additional_columns={\"conv\": \"single\", \"pretrain\": \"yes\", \"finetune\": 'n'}, group_exclude_columns={'conv_layers'}, model_name='convit'), ignore_index=True)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>conv</th>\n",
       "      <th>pretrain</th>\n",
       "      <th>training_config</th>\n",
       "      <th>train accuracy steady fold1</th>\n",
       "      <th>train accuracy steady fold2</th>\n",
       "      <th>train accuracy steady avg2folds</th>\n",
       "      <th>validation accuracy steady fold1</th>\n",
       "      <th>validation accuracy steady fold2</th>\n",
       "      <th>test accuracy fold1</th>\n",
       "      <th>test accuracy fold2</th>\n",
       "      <th>test accuracy avg2folds</th>\n",
       "      <th>test accuracy steady fold1</th>\n",
       "      <th>test accuracy steady fold2</th>\n",
       "      <th>test accuracy steady avg2folds</th>\n",
       "      <th>test accuracy steady avg2folds std across sessions</th>\n",
       "      <th>test accuracy steady avg2folds std across subjects</th>\n",
       "      <th>test accuracy steady session6 avg2folds</th>\n",
       "      <th>test accuracy steady session7 avg2folds</th>\n",
       "      <th>test accuracy steady session8 avg2folds</th>\n",
       "      <th>test accuracy steady session9 avg2folds</th>\n",
       "      <th>test accuracy steady session10 avg2folds</th>\n",
       "      <th>test accuracy steady subj0 avg2folds</th>\n",
       "      <th>test accuracy steady subj1 avg2folds</th>\n",
       "      <th>test accuracy steady subj2 avg2folds</th>\n",
       "      <th>test accuracy steady subj3 avg2folds</th>\n",
       "      <th>test accuracy steady subj4 avg2folds</th>\n",
       "      <th>test accuracy steady subj5 avg2folds</th>\n",
       "      <th>test accuracy steady subj6 avg2folds</th>\n",
       "      <th>test accuracy steady subj7 avg2folds</th>\n",
       "      <th>test accuracy steady subj8 avg2folds</th>\n",
       "      <th>test accuracy steady subj9 avg2folds</th>\n",
       "      <th>test preds steady subj0 avg2folds</th>\n",
       "      <th>test preds steady subj1 avg2folds</th>\n",
       "      <th>test preds steady subj2 avg2folds</th>\n",
       "      <th>test preds steady subj3 avg2folds</th>\n",
       "      <th>test preds steady subj4 avg2folds</th>\n",
       "      <th>test preds steady subj5 avg2folds</th>\n",
       "      <th>test preds steady subj6 avg2folds</th>\n",
       "      <th>test preds steady subj7 avg2folds</th>\n",
       "      <th>test preds steady subj8 avg2folds</th>\n",
       "      <th>test preds steady subj9 avg2folds</th>\n",
       "      <th>finetune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>all</td>\n",
       "      <td>no</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.832824</td>\n",
       "      <td>0.891702</td>\n",
       "      <td>0.862263</td>\n",
       "      <td>0.628401</td>\n",
       "      <td>0.638954</td>\n",
       "      <td>0.441933</td>\n",
       "      <td>0.449447</td>\n",
       "      <td>0.441933</td>\n",
       "      <td>0.602031</td>\n",
       "      <td>0.605940</td>\n",
       "      <td>0.603985</td>\n",
       "      <td>0.012319</td>\n",
       "      <td>0.118277</td>\n",
       "      <td>0.616156</td>\n",
       "      <td>0.590528</td>\n",
       "      <td>0.620980</td>\n",
       "      <td>0.593185</td>\n",
       "      <td>0.599078</td>\n",
       "      <td>0.753866</td>\n",
       "      <td>0.684087</td>\n",
       "      <td>0.450906</td>\n",
       "      <td>0.670771</td>\n",
       "      <td>0.801496</td>\n",
       "      <td>0.651356</td>\n",
       "      <td>0.541126</td>\n",
       "      <td>0.477420</td>\n",
       "      <td>0.536560</td>\n",
       "      <td>0.472264</td>\n",
       "      <td>[41807, 8911, 7252, 11555, 5250, 8265, 8493, 5719]</td>\n",
       "      <td>[37130, 10021, 4311, 7235, 14533, 10389, 6589, 6860]</td>\n",
       "      <td>[97402, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[45596, 6682, 6356, 7039, 7494, 12349, 5225, 6273]</td>\n",
       "      <td>[51944, 7929, 7942, 7570, 4418, 6030, 6991, 4506]</td>\n",
       "      <td>[41677, 5808, 6131, 5142, 8978, 13131, 9451, 6554]</td>\n",
       "      <td>[36381, 7532, 12002, 7409, 12199, 6717, 10644, 4496]</td>\n",
       "      <td>[97942, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[73835, 5280, 3776, 2604, 3837, 2701, 4316, 2719]</td>\n",
       "      <td>[68396, 2106, 2865, 3539, 6398, 6286, 4247, 3403]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>no</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.882529</td>\n",
       "      <td>0.859839</td>\n",
       "      <td>0.871184</td>\n",
       "      <td>0.647656</td>\n",
       "      <td>0.635783</td>\n",
       "      <td>0.455730</td>\n",
       "      <td>0.448515</td>\n",
       "      <td>0.455730</td>\n",
       "      <td>0.614778</td>\n",
       "      <td>0.605766</td>\n",
       "      <td>0.610272</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>0.106803</td>\n",
       "      <td>0.622817</td>\n",
       "      <td>0.589404</td>\n",
       "      <td>0.636001</td>\n",
       "      <td>0.599015</td>\n",
       "      <td>0.604123</td>\n",
       "      <td>0.746648</td>\n",
       "      <td>0.673362</td>\n",
       "      <td>0.450906</td>\n",
       "      <td>0.668524</td>\n",
       "      <td>0.787117</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>0.558581</td>\n",
       "      <td>0.477420</td>\n",
       "      <td>0.538243</td>\n",
       "      <td>0.547374</td>\n",
       "      <td>[43159, 9577, 8003, 8520, 5472, 8625, 8858, 5038]</td>\n",
       "      <td>[35555, 9297, 6584, 7208, 13775, 12204, 6015, 6430]</td>\n",
       "      <td>[97402, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[43968, 8248, 5617, 8590, 6828, 12203, 6886, 4674]</td>\n",
       "      <td>[52325, 8817, 7123, 7560, 3986, 5596, 8221, 3702]</td>\n",
       "      <td>[44828, 5115, 8246, 5673, 7816, 10145, 10383, 4666]</td>\n",
       "      <td>[35180, 7878, 8591, 6620, 12448, 9308, 12962, 4393]</td>\n",
       "      <td>[97942, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[74923, 3967, 4307, 2553, 4213, 2639, 4008, 2458]</td>\n",
       "      <td>[46626, 4676, 7847, 6280, 10477, 9842, 6589, 4903]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>yes_nonfixed</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.959673</td>\n",
       "      <td>0.998564</td>\n",
       "      <td>0.979119</td>\n",
       "      <td>0.676759</td>\n",
       "      <td>0.709262</td>\n",
       "      <td>0.461770</td>\n",
       "      <td>0.472543</td>\n",
       "      <td>0.461770</td>\n",
       "      <td>0.617109</td>\n",
       "      <td>0.623918</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>0.642582</td>\n",
       "      <td>0.613916</td>\n",
       "      <td>0.643545</td>\n",
       "      <td>0.598913</td>\n",
       "      <td>0.603610</td>\n",
       "      <td>0.743256</td>\n",
       "      <td>0.541221</td>\n",
       "      <td>0.602619</td>\n",
       "      <td>0.656861</td>\n",
       "      <td>0.698363</td>\n",
       "      <td>0.615981</td>\n",
       "      <td>0.615221</td>\n",
       "      <td>0.594285</td>\n",
       "      <td>0.549048</td>\n",
       "      <td>0.588277</td>\n",
       "      <td>[41603, 10600, 6281, 9057, 5419, 10113, 8405, 5774]</td>\n",
       "      <td>[34901, 2659, 906, 1782, 5888, 11619, 12282, 27031]</td>\n",
       "      <td>[55468, 3750, 4904, 4073, 7213, 10869, 5904, 5221]</td>\n",
       "      <td>[47962, 9259, 4465, 3970, 6029, 11772, 8041, 5516]</td>\n",
       "      <td>[53005, 4404, 10682, 2972, 8192, 4930, 3816, 9329]</td>\n",
       "      <td>[44820, 4944, 5629, 6212, 6996, 11084, 12243, 4944]</td>\n",
       "      <td>[38982, 7702, 10332, 7889, 8685, 8117, 11674, 3999]</td>\n",
       "      <td>[54955, 8021, 7953, 4404, 5753, 3206, 7772, 5878]</td>\n",
       "      <td>[73546, 4095, 4041, 2086, 4118, 2839, 4980, 3363]</td>\n",
       "      <td>[45239, 4659, 6754, 5554, 11795, 11614, 5909, 5716]</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>yes_nonfixed</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.998241</td>\n",
       "      <td>0.998068</td>\n",
       "      <td>0.998154</td>\n",
       "      <td>0.708974</td>\n",
       "      <td>0.736615</td>\n",
       "      <td>0.492865</td>\n",
       "      <td>0.476860</td>\n",
       "      <td>0.492865</td>\n",
       "      <td>0.654383</td>\n",
       "      <td>0.629076</td>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.070781</td>\n",
       "      <td>0.663354</td>\n",
       "      <td>0.633657</td>\n",
       "      <td>0.663246</td>\n",
       "      <td>0.621813</td>\n",
       "      <td>0.626578</td>\n",
       "      <td>0.741090</td>\n",
       "      <td>0.608682</td>\n",
       "      <td>0.613583</td>\n",
       "      <td>0.661406</td>\n",
       "      <td>0.802737</td>\n",
       "      <td>0.616337</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>0.597427</td>\n",
       "      <td>0.559118</td>\n",
       "      <td>0.594563</td>\n",
       "      <td>[42732, 9380, 6097, 10432, 5282, 9455, 8193, 5681]</td>\n",
       "      <td>[37894, 3918, 1626, 2037, 8323, 13903, 13684, 15683]</td>\n",
       "      <td>[54454, 3561, 6678, 4240, 7351, 10790, 6140, 4188]</td>\n",
       "      <td>[45895, 8892, 4499, 4612, 6601, 9868, 9783, 6864]</td>\n",
       "      <td>[53051, 7970, 9514, 5260, 4665, 5714, 6734, 4422]</td>\n",
       "      <td>[44650, 5211, 6166, 7183, 6559, 12127, 9700, 5276]</td>\n",
       "      <td>[38431, 7660, 9622, 8804, 9962, 8500, 10432, 3969]</td>\n",
       "      <td>[57191, 8231, 5578, 4329, 5725, 2475, 8116, 6297]</td>\n",
       "      <td>[75288, 4047, 3520, 2274, 4141, 3326, 4238, 2234]</td>\n",
       "      <td>[44094, 4775, 5595, 7104, 12449, 13096, 4691, 5436]</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>yes_nonfixed</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.997923</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0.707988</td>\n",
       "      <td>0.750072</td>\n",
       "      <td>0.488877</td>\n",
       "      <td>0.479657</td>\n",
       "      <td>0.488877</td>\n",
       "      <td>0.651552</td>\n",
       "      <td>0.633752</td>\n",
       "      <td>0.642652</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>0.067983</td>\n",
       "      <td>0.661105</td>\n",
       "      <td>0.634181</td>\n",
       "      <td>0.665290</td>\n",
       "      <td>0.621955</td>\n",
       "      <td>0.630728</td>\n",
       "      <td>0.737263</td>\n",
       "      <td>0.616858</td>\n",
       "      <td>0.606898</td>\n",
       "      <td>0.671402</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.622458</td>\n",
       "      <td>0.630461</td>\n",
       "      <td>0.602135</td>\n",
       "      <td>0.553396</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>[43115, 9285, 6600, 7995, 5412, 10557, 8257, 6031]</td>\n",
       "      <td>[39793, 4466, 1305, 3008, 8029, 11032, 14869, 14566]</td>\n",
       "      <td>[59113, 3787, 5261, 3256, 9532, 8669, 3745, 4039]</td>\n",
       "      <td>[46457, 7633, 5563, 5589, 6069, 10831, 8701, 6171]</td>\n",
       "      <td>[52701, 7551, 8620, 6385, 4664, 5717, 7183, 4509]</td>\n",
       "      <td>[43665, 5496, 6070, 7341, 7203, 11224, 11157, 4716]</td>\n",
       "      <td>[38720, 7628, 11747, 7897, 9376, 7944, 10219, 3849]</td>\n",
       "      <td>[56763, 8292, 5717, 4860, 5750, 2313, 8025, 6222]</td>\n",
       "      <td>[75015, 4012, 4625, 2322, 3900, 3177, 3601, 2416]</td>\n",
       "      <td>[45302, 5342, 4967, 6092, 12818, 12275, 5342, 5102]</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701014</td>\n",
       "      <td>0.764761</td>\n",
       "      <td>0.483799</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.483799</td>\n",
       "      <td>0.643965</td>\n",
       "      <td>0.626183</td>\n",
       "      <td>0.635074</td>\n",
       "      <td>0.018878</td>\n",
       "      <td>0.070415</td>\n",
       "      <td>0.651722</td>\n",
       "      <td>0.626734</td>\n",
       "      <td>0.662666</td>\n",
       "      <td>0.613291</td>\n",
       "      <td>0.620956</td>\n",
       "      <td>0.734223</td>\n",
       "      <td>0.594341</td>\n",
       "      <td>0.596575</td>\n",
       "      <td>0.658075</td>\n",
       "      <td>0.793397</td>\n",
       "      <td>0.613355</td>\n",
       "      <td>0.625503</td>\n",
       "      <td>0.589611</td>\n",
       "      <td>0.554981</td>\n",
       "      <td>0.590679</td>\n",
       "      <td>[43426, 8087, 5520, 9837, 5046, 9789, 8088, 7459]</td>\n",
       "      <td>[39860, 4575, 1939, 3178, 7745, 7514, 14139, 18118]</td>\n",
       "      <td>[55611, 3600, 6652, 2922, 7491, 9971, 5388, 5767]</td>\n",
       "      <td>[46824, 7787, 5527, 6266, 5439, 10751, 8663, 5757]</td>\n",
       "      <td>[52223, 6711, 8547, 6653, 4597, 5466, 7289, 5844]</td>\n",
       "      <td>[44634, 5457, 6599, 6428, 7675, 10613, 10054, 5412]</td>\n",
       "      <td>[39161, 8029, 10414, 9881, 8867, 8154, 8687, 4187]</td>\n",
       "      <td>[54289, 8005, 5533, 5024, 7263, 3152, 7692, 6984]</td>\n",
       "      <td>[72966, 4448, 4262, 2559, 4544, 3116, 3788, 3385]</td>\n",
       "      <td>[43836, 4776, 6043, 7410, 13825, 10651, 5289, 5410]</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.702524</td>\n",
       "      <td>0.807746</td>\n",
       "      <td>0.478013</td>\n",
       "      <td>0.471284</td>\n",
       "      <td>0.478013</td>\n",
       "      <td>0.636780</td>\n",
       "      <td>0.624332</td>\n",
       "      <td>0.630556</td>\n",
       "      <td>0.020094</td>\n",
       "      <td>0.069056</td>\n",
       "      <td>0.647797</td>\n",
       "      <td>0.627941</td>\n",
       "      <td>0.658224</td>\n",
       "      <td>0.604172</td>\n",
       "      <td>0.614647</td>\n",
       "      <td>0.721906</td>\n",
       "      <td>0.600570</td>\n",
       "      <td>0.582704</td>\n",
       "      <td>0.657313</td>\n",
       "      <td>0.787451</td>\n",
       "      <td>0.604389</td>\n",
       "      <td>0.627124</td>\n",
       "      <td>0.582811</td>\n",
       "      <td>0.552383</td>\n",
       "      <td>0.588909</td>\n",
       "      <td>[44290, 7628, 4814, 9366, 4816, 8933, 10043, 7362]</td>\n",
       "      <td>[41895, 3567, 1726, 3068, 7367, 8923, 15480, 15042]</td>\n",
       "      <td>[55801, 2912, 6968, 2807, 7047, 11267, 5424, 5176]</td>\n",
       "      <td>[46969, 7024, 5467, 6470, 6181, 11766, 7567, 5570]</td>\n",
       "      <td>[52604, 6767, 8361, 6425, 4677, 4804, 7843, 5849]</td>\n",
       "      <td>[46128, 5281, 6577, 6521, 8039, 10520, 8288, 5518]</td>\n",
       "      <td>[39324, 8032, 10180, 9062, 9033, 8306, 9025, 4418]</td>\n",
       "      <td>[55071, 6924, 4619, 5158, 7330, 2840, 8064, 7936]</td>\n",
       "      <td>[72851, 5088, 4719, 2535, 4165, 2882, 4261, 2567]</td>\n",
       "      <td>[44943, 4741, 6018, 7261, 13895, 10012, 5111, 5259]</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999773</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>0.702832</td>\n",
       "      <td>0.840944</td>\n",
       "      <td>0.472904</td>\n",
       "      <td>0.473583</td>\n",
       "      <td>0.472904</td>\n",
       "      <td>0.630852</td>\n",
       "      <td>0.626580</td>\n",
       "      <td>0.628716</td>\n",
       "      <td>0.021979</td>\n",
       "      <td>0.069588</td>\n",
       "      <td>0.646831</td>\n",
       "      <td>0.625515</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.599720</td>\n",
       "      <td>0.611913</td>\n",
       "      <td>0.727337</td>\n",
       "      <td>0.597063</td>\n",
       "      <td>0.588965</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>0.783393</td>\n",
       "      <td>0.590392</td>\n",
       "      <td>0.630161</td>\n",
       "      <td>0.581199</td>\n",
       "      <td>0.552252</td>\n",
       "      <td>0.583597</td>\n",
       "      <td>[44030, 8163, 4897, 8939, 4745, 8144, 10883, 7451]</td>\n",
       "      <td>[41544, 4669, 1501, 3609, 6368, 5648, 19503, 14226]</td>\n",
       "      <td>[57643, 3724, 5541, 3079, 7479, 10554, 4307, 5075]</td>\n",
       "      <td>[46061, 6775, 5052, 6628, 6273, 11788, 8376, 6061]</td>\n",
       "      <td>[52326, 6627, 8173, 6890, 4463, 4476, 8202, 6173]</td>\n",
       "      <td>[45202, 5160, 7282, 6341, 8477, 10099, 9139, 5172]</td>\n",
       "      <td>[40511, 7815, 9473, 9209, 8422, 7682, 9360, 4908]</td>\n",
       "      <td>[56393, 6867, 4145, 4588, 6427, 3156, 8034, 8332]</td>\n",
       "      <td>[73831, 4962, 3703, 2337, 4295, 2823, 4136, 2981]</td>\n",
       "      <td>[42834, 5174, 6948, 7438, 13104, 10737, 5819, 5186]</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.667566</td>\n",
       "      <td>0.762099</td>\n",
       "      <td>0.667566</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>0.599935</td>\n",
       "      <td>0.599935</td>\n",
       "      <td>0.599935</td>\n",
       "      <td>0.026289</td>\n",
       "      <td>0.069445</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>0.598142</td>\n",
       "      <td>0.616040</td>\n",
       "      <td>0.558392</td>\n",
       "      <td>0.590136</td>\n",
       "      <td>0.706369</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.550364</td>\n",
       "      <td>0.551571</td>\n",
       "      <td>0.743818</td>\n",
       "      <td>0.568019</td>\n",
       "      <td>0.642014</td>\n",
       "      <td>0.580132</td>\n",
       "      <td>0.521666</td>\n",
       "      <td>0.565136</td>\n",
       "      <td>[43032, 8378, 7302, 7696, 5438, 10330, 8276, 6800]</td>\n",
       "      <td>[40844, 3362, 4898, 3110, 6048, 8036, 16512, 14258]</td>\n",
       "      <td>[67090, 2956, 1696, 1016, 6354, 12694, 2574, 3022]</td>\n",
       "      <td>[41908, 6354, 1982, 12918, 2768, 15794, 9088, 6202]</td>\n",
       "      <td>[51428, 5758, 8040, 8244, 4586, 5878, 8318, 5078]</td>\n",
       "      <td>[43684, 4420, 5670, 6176, 14072, 13966, 4870, 4014]</td>\n",
       "      <td>[38168, 9210, 8626, 7652, 9938, 9642, 8862, 5282]</td>\n",
       "      <td>[51120, 9808, 4566, 5468, 4404, 5906, 8770, 7900]</td>\n",
       "      <td>[65166, 10368, 2506, 4848, 4748, 6562, 3484, 1386]</td>\n",
       "      <td>[36440, 7978, 7538, 13962, 14572, 7888, 4650, 4212]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.996431</td>\n",
       "      <td>0.995991</td>\n",
       "      <td>0.996211</td>\n",
       "      <td>0.709019</td>\n",
       "      <td>0.716973</td>\n",
       "      <td>0.491747</td>\n",
       "      <td>0.473887</td>\n",
       "      <td>0.491747</td>\n",
       "      <td>0.652992</td>\n",
       "      <td>0.626230</td>\n",
       "      <td>0.639611</td>\n",
       "      <td>0.017223</td>\n",
       "      <td>0.073236</td>\n",
       "      <td>0.660144</td>\n",
       "      <td>0.625860</td>\n",
       "      <td>0.661206</td>\n",
       "      <td>0.626749</td>\n",
       "      <td>0.624097</td>\n",
       "      <td>0.742451</td>\n",
       "      <td>0.609140</td>\n",
       "      <td>0.614546</td>\n",
       "      <td>0.662740</td>\n",
       "      <td>0.801409</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.621077</td>\n",
       "      <td>0.585088</td>\n",
       "      <td>0.545317</td>\n",
       "      <td>0.591726</td>\n",
       "      <td>[41349, 9469, 5210, 9590, 5803, 10619, 9143, 6069]</td>\n",
       "      <td>[37060, 4211, 2475, 1805, 10566, 11821, 12975, 16155]</td>\n",
       "      <td>[56629, 4186, 5231, 4385, 8075, 6273, 6299, 6324]</td>\n",
       "      <td>[47119, 8846, 5968, 4902, 6268, 9855, 8812, 5244]</td>\n",
       "      <td>[53032, 7977, 7167, 8271, 3994, 6180, 6706, 4003]</td>\n",
       "      <td>[46397, 5147, 4691, 6809, 7651, 10073, 11881, 4223]</td>\n",
       "      <td>[39878, 8133, 8943, 6423, 10118, 7564, 11789, 4532]</td>\n",
       "      <td>[58284, 9226, 6306, 3983, 5022, 2858, 8093, 4170]</td>\n",
       "      <td>[74421, 5471, 3941, 2283, 4063, 3345, 4055, 1489]</td>\n",
       "      <td>[45078, 4215, 5158, 7160, 11889, 14052, 4732, 4956]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.901621</td>\n",
       "      <td>0.685970</td>\n",
       "      <td>0.793796</td>\n",
       "      <td>0.685970</td>\n",
       "      <td>0.901621</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.613291</td>\n",
       "      <td>0.613291</td>\n",
       "      <td>0.613291</td>\n",
       "      <td>0.022625</td>\n",
       "      <td>0.061504</td>\n",
       "      <td>0.645334</td>\n",
       "      <td>0.614970</td>\n",
       "      <td>0.629416</td>\n",
       "      <td>0.584667</td>\n",
       "      <td>0.592066</td>\n",
       "      <td>0.709041</td>\n",
       "      <td>0.588212</td>\n",
       "      <td>0.557945</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.750749</td>\n",
       "      <td>0.580515</td>\n",
       "      <td>0.613696</td>\n",
       "      <td>0.590188</td>\n",
       "      <td>0.552930</td>\n",
       "      <td>0.586513</td>\n",
       "      <td>[45276, 7468, 6290, 9056, 5316, 9490, 9510, 4846]</td>\n",
       "      <td>[42562, 5104, 5338, 3826, 4726, 7174, 15276, 13062]</td>\n",
       "      <td>[68540, 3578, 4910, 942, 3044, 12096, 3402, 890]</td>\n",
       "      <td>[48908, 7524, 2918, 7512, 4406, 12862, 5976, 6908]</td>\n",
       "      <td>[52700, 5520, 7428, 8946, 5586, 5190, 6936, 5024]</td>\n",
       "      <td>[48278, 5982, 6684, 4460, 10034, 13402, 4310, 3722]</td>\n",
       "      <td>[45246, 7790, 12434, 6740, 5646, 7046, 8728, 3750]</td>\n",
       "      <td>[56820, 6956, 5276, 4440, 4846, 3866, 7830, 7908]</td>\n",
       "      <td>[74936, 4176, 4784, 3334, 3426, 3664, 2756, 1992]</td>\n",
       "      <td>[42438, 8546, 9462, 7578, 14704, 6542, 3638, 4332]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.723029</td>\n",
       "      <td>0.735722</td>\n",
       "      <td>0.497365</td>\n",
       "      <td>0.476320</td>\n",
       "      <td>0.497365</td>\n",
       "      <td>0.660166</td>\n",
       "      <td>0.630630</td>\n",
       "      <td>0.645398</td>\n",
       "      <td>0.016065</td>\n",
       "      <td>0.071867</td>\n",
       "      <td>0.655506</td>\n",
       "      <td>0.637054</td>\n",
       "      <td>0.671994</td>\n",
       "      <td>0.633356</td>\n",
       "      <td>0.629081</td>\n",
       "      <td>0.746291</td>\n",
       "      <td>0.610501</td>\n",
       "      <td>0.618789</td>\n",
       "      <td>0.665040</td>\n",
       "      <td>0.806116</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>0.627375</td>\n",
       "      <td>0.598092</td>\n",
       "      <td>0.552015</td>\n",
       "      <td>0.605326</td>\n",
       "      <td>[42768, 8915, 5681, 8781, 5823, 11728, 8433, 5123]</td>\n",
       "      <td>[36844, 3902, 1468, 2232, 11468, 12234, 13798, 15122]</td>\n",
       "      <td>[53796, 4478, 4587, 4879, 6607, 9344, 7211, 6500]</td>\n",
       "      <td>[47216, 7182, 6307, 6177, 7185, 10310, 7793, 4844]</td>\n",
       "      <td>[53109, 7812, 8266, 7943, 3603, 6168, 6077, 4352]</td>\n",
       "      <td>[47272, 5153, 4917, 6763, 7963, 10022, 10793, 3989]</td>\n",
       "      <td>[39115, 7736, 9892, 7711, 8090, 8379, 12700, 3757]</td>\n",
       "      <td>[57839, 9572, 5084, 4445, 5530, 2836, 8598, 4038]</td>\n",
       "      <td>[74457, 4772, 4305, 2390, 4635, 2889, 3701, 1919]</td>\n",
       "      <td>[45846, 4180, 5189, 7095, 12485, 11424, 5391, 5630]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.953040</td>\n",
       "      <td>0.691374</td>\n",
       "      <td>0.822207</td>\n",
       "      <td>0.691374</td>\n",
       "      <td>0.953040</td>\n",
       "      <td>0.459843</td>\n",
       "      <td>0.459843</td>\n",
       "      <td>0.459843</td>\n",
       "      <td>0.612123</td>\n",
       "      <td>0.612123</td>\n",
       "      <td>0.612123</td>\n",
       "      <td>0.026010</td>\n",
       "      <td>0.060849</td>\n",
       "      <td>0.637352</td>\n",
       "      <td>0.622971</td>\n",
       "      <td>0.637267</td>\n",
       "      <td>0.573689</td>\n",
       "      <td>0.589338</td>\n",
       "      <td>0.709736</td>\n",
       "      <td>0.586230</td>\n",
       "      <td>0.581170</td>\n",
       "      <td>0.599906</td>\n",
       "      <td>0.740620</td>\n",
       "      <td>0.592312</td>\n",
       "      <td>0.627469</td>\n",
       "      <td>0.578533</td>\n",
       "      <td>0.538976</td>\n",
       "      <td>0.566282</td>\n",
       "      <td>[44844, 6784, 3978, 10430, 6180, 8998, 10810, 5228]</td>\n",
       "      <td>[42326, 5950, 5062, 5870, 4482, 6176, 14960, 12242]</td>\n",
       "      <td>[61836, 2212, 4056, 2434, 7178, 10384, 7304, 1998]</td>\n",
       "      <td>[45874, 8146, 5394, 7372, 5254, 11914, 5652, 7408]</td>\n",
       "      <td>[51798, 5426, 4964, 8540, 5622, 5736, 8210, 7034]</td>\n",
       "      <td>[47532, 5336, 6984, 5246, 10372, 11996, 5834, 3572]</td>\n",
       "      <td>[39886, 8680, 10852, 8656, 6620, 9854, 8820, 4012]</td>\n",
       "      <td>[53948, 7610, 3666, 3852, 6784, 5568, 7046, 9468]</td>\n",
       "      <td>[73482, 5908, 3010, 3600, 2856, 5824, 2790, 1598]</td>\n",
       "      <td>[39034, 9392, 8460, 9852, 12194, 9552, 4064, 4692]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999630</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>0.742529</td>\n",
       "      <td>0.499154</td>\n",
       "      <td>0.476909</td>\n",
       "      <td>0.499154</td>\n",
       "      <td>0.662457</td>\n",
       "      <td>0.631552</td>\n",
       "      <td>0.647005</td>\n",
       "      <td>0.016744</td>\n",
       "      <td>0.070378</td>\n",
       "      <td>0.668169</td>\n",
       "      <td>0.638057</td>\n",
       "      <td>0.666245</td>\n",
       "      <td>0.629173</td>\n",
       "      <td>0.633378</td>\n",
       "      <td>0.750426</td>\n",
       "      <td>0.612469</td>\n",
       "      <td>0.610064</td>\n",
       "      <td>0.671816</td>\n",
       "      <td>0.798918</td>\n",
       "      <td>0.623259</td>\n",
       "      <td>0.635534</td>\n",
       "      <td>0.599882</td>\n",
       "      <td>0.554783</td>\n",
       "      <td>0.612895</td>\n",
       "      <td>[42581, 9477, 5652, 8762, 5915, 11209, 8413, 5243]</td>\n",
       "      <td>[37310, 3475, 1478, 1919, 10970, 12726, 13156, 16034]</td>\n",
       "      <td>[56681, 3802, 3779, 3764, 7380, 7321, 8420, 6255]</td>\n",
       "      <td>[47842, 8296, 5295, 6366, 6466, 10196, 7675, 4878]</td>\n",
       "      <td>[53236, 8404, 7571, 8628, 3582, 6026, 5828, 4055]</td>\n",
       "      <td>[46323, 4716, 4715, 6521, 8965, 10314, 11395, 3923]</td>\n",
       "      <td>[38583, 7840, 9710, 7760, 8381, 9161, 12250, 3695]</td>\n",
       "      <td>[57281, 9302, 5210, 4879, 5957, 2791, 8536, 3986]</td>\n",
       "      <td>[75103, 4315, 4586, 2428, 4581, 2618, 3266, 2171]</td>\n",
       "      <td>[46070, 4141, 5089, 6895, 12692, 12437, 4715, 5201]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0.656857</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.656857</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0.451891</td>\n",
       "      <td>0.451891</td>\n",
       "      <td>0.451891</td>\n",
       "      <td>0.604689</td>\n",
       "      <td>0.604689</td>\n",
       "      <td>0.604689</td>\n",
       "      <td>0.020941</td>\n",
       "      <td>0.065078</td>\n",
       "      <td>0.626146</td>\n",
       "      <td>0.607255</td>\n",
       "      <td>0.626145</td>\n",
       "      <td>0.571201</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>0.705325</td>\n",
       "      <td>0.605132</td>\n",
       "      <td>0.544967</td>\n",
       "      <td>0.612772</td>\n",
       "      <td>0.741832</td>\n",
       "      <td>0.570274</td>\n",
       "      <td>0.609048</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>0.540736</td>\n",
       "      <td>0.548271</td>\n",
       "      <td>[45500, 6498, 8392, 11368, 5328, 8234, 6886, 5046]</td>\n",
       "      <td>[43872, 4398, 7416, 4822, 4616, 5938, 14280, 11726]</td>\n",
       "      <td>[68750, 1518, 2834, 1936, 7002, 10502, 2636, 2224]</td>\n",
       "      <td>[49874, 5030, 4732, 8700, 3716, 9712, 5872, 9378]</td>\n",
       "      <td>[52914, 3188, 9836, 6426, 6070, 4472, 7020, 7404]</td>\n",
       "      <td>[46334, 3904, 11540, 2474, 9882, 11494, 6984, 4260]</td>\n",
       "      <td>[42966, 6202, 15176, 8192, 11262, 4948, 4206, 4428]</td>\n",
       "      <td>[57874, 5474, 8832, 3732, 4654, 3132, 3768, 10476]</td>\n",
       "      <td>[70210, 7390, 4412, 3128, 5098, 2338, 3494, 2998]</td>\n",
       "      <td>[43862, 3830, 9486, 7016, 19134, 3818, 4024, 6070]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>0.995159</td>\n",
       "      <td>0.996749</td>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.706679</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.470759</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.651274</td>\n",
       "      <td>0.622143</td>\n",
       "      <td>0.636708</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>0.073221</td>\n",
       "      <td>0.653338</td>\n",
       "      <td>0.625748</td>\n",
       "      <td>0.661820</td>\n",
       "      <td>0.619437</td>\n",
       "      <td>0.623199</td>\n",
       "      <td>0.733087</td>\n",
       "      <td>0.598245</td>\n",
       "      <td>0.611534</td>\n",
       "      <td>0.654791</td>\n",
       "      <td>0.804417</td>\n",
       "      <td>0.619616</td>\n",
       "      <td>0.621649</td>\n",
       "      <td>0.589890</td>\n",
       "      <td>0.541346</td>\n",
       "      <td>0.592509</td>\n",
       "      <td>[42924, 10369, 7385, 10076, 4499, 9695, 7083, 5221]</td>\n",
       "      <td>[37150, 4002, 2340, 1723, 9489, 13091, 12541, 16732]</td>\n",
       "      <td>[54510, 3235, 5853, 4363, 7535, 9866, 8389, 3651]</td>\n",
       "      <td>[47298, 5761, 4074, 5170, 7006, 11860, 8444, 7401]</td>\n",
       "      <td>[52980, 8211, 8824, 6727, 3839, 6330, 6171, 4248]</td>\n",
       "      <td>[45756, 5223, 5410, 6528, 7079, 11233, 11441, 4202]</td>\n",
       "      <td>[38883, 9274, 10716, 8243, 9800, 6100, 10356, 4008]</td>\n",
       "      <td>[55056, 9750, 4998, 4711, 5786, 2082, 9538, 6021]</td>\n",
       "      <td>[72525, 5743, 4464, 2922, 3650, 2529, 4837, 2398]</td>\n",
       "      <td>[42347, 4599, 4950, 6887, 14091, 12194, 6430, 5742]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.911664</td>\n",
       "      <td>0.664353</td>\n",
       "      <td>0.788009</td>\n",
       "      <td>0.664353</td>\n",
       "      <td>0.911664</td>\n",
       "      <td>0.463722</td>\n",
       "      <td>0.463722</td>\n",
       "      <td>0.463722</td>\n",
       "      <td>0.618058</td>\n",
       "      <td>0.618058</td>\n",
       "      <td>0.618058</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.058368</td>\n",
       "      <td>0.630644</td>\n",
       "      <td>0.614631</td>\n",
       "      <td>0.634751</td>\n",
       "      <td>0.605734</td>\n",
       "      <td>0.604527</td>\n",
       "      <td>0.714990</td>\n",
       "      <td>0.617381</td>\n",
       "      <td>0.588821</td>\n",
       "      <td>0.619772</td>\n",
       "      <td>0.734656</td>\n",
       "      <td>0.603945</td>\n",
       "      <td>0.615026</td>\n",
       "      <td>0.581164</td>\n",
       "      <td>0.556304</td>\n",
       "      <td>0.548517</td>\n",
       "      <td>[42096, 7564, 7350, 10414, 6060, 11762, 6466, 5540]</td>\n",
       "      <td>[41550, 4726, 7316, 6204, 4092, 12670, 7498, 13012]</td>\n",
       "      <td>[61342, 2520, 2388, 3970, 9978, 12718, 1444, 3042]</td>\n",
       "      <td>[45002, 6598, 6134, 6964, 4348, 13870, 5772, 8326]</td>\n",
       "      <td>[50636, 5974, 7262, 5918, 6642, 6522, 5540, 8836]</td>\n",
       "      <td>[50416, 5322, 5538, 4050, 12564, 10216, 3294, 5472]</td>\n",
       "      <td>[39566, 8434, 14950, 6858, 7332, 8940, 6398, 4902]</td>\n",
       "      <td>[56504, 6228, 4612, 4030, 6134, 4520, 6700, 9214]</td>\n",
       "      <td>[75968, 7144, 2918, 2256, 3620, 2050, 2754, 2358]</td>\n",
       "      <td>[42902, 7786, 7682, 6112, 17384, 7754, 3222, 4398]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.999828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999914</td>\n",
       "      <td>0.709069</td>\n",
       "      <td>0.735511</td>\n",
       "      <td>0.495712</td>\n",
       "      <td>0.478257</td>\n",
       "      <td>0.495712</td>\n",
       "      <td>0.658047</td>\n",
       "      <td>0.631495</td>\n",
       "      <td>0.644771</td>\n",
       "      <td>0.016597</td>\n",
       "      <td>0.075293</td>\n",
       "      <td>0.665452</td>\n",
       "      <td>0.634398</td>\n",
       "      <td>0.664523</td>\n",
       "      <td>0.629593</td>\n",
       "      <td>0.629889</td>\n",
       "      <td>0.750079</td>\n",
       "      <td>0.587470</td>\n",
       "      <td>0.619324</td>\n",
       "      <td>0.673124</td>\n",
       "      <td>0.810435</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0.637280</td>\n",
       "      <td>0.595256</td>\n",
       "      <td>0.554179</td>\n",
       "      <td>0.597484</td>\n",
       "      <td>[42216, 9943, 5851, 8560, 6382, 11278, 7825, 5197]</td>\n",
       "      <td>[36040, 3674, 2130, 2365, 6392, 11750, 15862, 18855]</td>\n",
       "      <td>[55369, 3817, 6565, 3857, 7553, 8449, 6788, 5004]</td>\n",
       "      <td>[46723, 7934, 4722, 5959, 5308, 11409, 9097, 5862]</td>\n",
       "      <td>[52908, 7671, 8883, 6737, 4589, 6255, 5721, 4566]</td>\n",
       "      <td>[45543, 5192, 5154, 6548, 6812, 10160, 12826, 4637]</td>\n",
       "      <td>[39478, 8398, 11161, 8398, 7417, 7330, 10668, 4530]</td>\n",
       "      <td>[57720, 7950, 6216, 4903, 5624, 2604, 7657, 5268]</td>\n",
       "      <td>[74007, 5180, 4295, 2320, 3831, 2654, 4605, 2176]</td>\n",
       "      <td>[44568, 4370, 4871, 6689, 13460, 11456, 5749, 6077]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.968732</td>\n",
       "      <td>0.669654</td>\n",
       "      <td>0.819193</td>\n",
       "      <td>0.669654</td>\n",
       "      <td>0.968732</td>\n",
       "      <td>0.458673</td>\n",
       "      <td>0.458673</td>\n",
       "      <td>0.458673</td>\n",
       "      <td>0.612312</td>\n",
       "      <td>0.612312</td>\n",
       "      <td>0.612312</td>\n",
       "      <td>0.017310</td>\n",
       "      <td>0.060833</td>\n",
       "      <td>0.631914</td>\n",
       "      <td>0.609907</td>\n",
       "      <td>0.631660</td>\n",
       "      <td>0.588293</td>\n",
       "      <td>0.599786</td>\n",
       "      <td>0.708523</td>\n",
       "      <td>0.595221</td>\n",
       "      <td>0.569419</td>\n",
       "      <td>0.624522</td>\n",
       "      <td>0.742783</td>\n",
       "      <td>0.565534</td>\n",
       "      <td>0.611042</td>\n",
       "      <td>0.580212</td>\n",
       "      <td>0.549705</td>\n",
       "      <td>0.576160</td>\n",
       "      <td>[45576, 6794, 6006, 9412, 6228, 8690, 9472, 5074]</td>\n",
       "      <td>[41936, 3840, 4236, 4194, 7206, 6418, 14256, 14982]</td>\n",
       "      <td>[64878, 2510, 4990, 2234, 6460, 9218, 4160, 2952]</td>\n",
       "      <td>[46962, 5968, 6056, 8566, 5668, 10870, 7176, 5748]</td>\n",
       "      <td>[52488, 4852, 9832, 6612, 7004, 4072, 6602, 5868]</td>\n",
       "      <td>[46022, 3874, 9630, 3926, 9904, 15766, 4464, 3286]</td>\n",
       "      <td>[44732, 7048, 10142, 5362, 9596, 6980, 8858, 4662]</td>\n",
       "      <td>[56972, 5830, 5946, 3732, 7226, 4180, 7344, 6712]</td>\n",
       "      <td>[74388, 5444, 3728, 3242, 3560, 4368, 3098, 1240]</td>\n",
       "      <td>[40726, 7180, 9826, 6936, 15304, 8396, 4754, 4118]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.705617</td>\n",
       "      <td>0.743060</td>\n",
       "      <td>0.497541</td>\n",
       "      <td>0.478500</td>\n",
       "      <td>0.497541</td>\n",
       "      <td>0.659025</td>\n",
       "      <td>0.632198</td>\n",
       "      <td>0.645612</td>\n",
       "      <td>0.014943</td>\n",
       "      <td>0.073521</td>\n",
       "      <td>0.664734</td>\n",
       "      <td>0.634393</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>0.634618</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>0.753740</td>\n",
       "      <td>0.605932</td>\n",
       "      <td>0.622046</td>\n",
       "      <td>0.676639</td>\n",
       "      <td>0.801224</td>\n",
       "      <td>0.625256</td>\n",
       "      <td>0.630688</td>\n",
       "      <td>0.595509</td>\n",
       "      <td>0.547757</td>\n",
       "      <td>0.597327</td>\n",
       "      <td>[42446, 9278, 6553, 9111, 5516, 10569, 8638, 5141]</td>\n",
       "      <td>[36482, 4664, 2492, 2254, 8022, 11546, 15862, 15746]</td>\n",
       "      <td>[55267, 4024, 4524, 4389, 8905, 9610, 5266, 5417]</td>\n",
       "      <td>[46697, 7943, 5550, 6077, 5003, 10698, 9116, 5930]</td>\n",
       "      <td>[52829, 7941, 8685, 6531, 4439, 6180, 6278, 4447]</td>\n",
       "      <td>[44920, 5266, 5575, 6203, 7407, 10399, 11652, 5450]</td>\n",
       "      <td>[38973, 7966, 10911, 8703, 8031, 7583, 10927, 4286]</td>\n",
       "      <td>[58575, 8694, 6085, 4534, 4646, 2318, 7832, 5258]</td>\n",
       "      <td>[74081, 5478, 4171, 2392, 3639, 2806, 4381, 2120]</td>\n",
       "      <td>[45777, 4279, 4948, 6980, 13640, 11114, 5304, 5198]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.853297</td>\n",
       "      <td>0.643881</td>\n",
       "      <td>0.748589</td>\n",
       "      <td>0.643881</td>\n",
       "      <td>0.853297</td>\n",
       "      <td>0.452398</td>\n",
       "      <td>0.452398</td>\n",
       "      <td>0.452398</td>\n",
       "      <td>0.602787</td>\n",
       "      <td>0.602787</td>\n",
       "      <td>0.602787</td>\n",
       "      <td>0.021670</td>\n",
       "      <td>0.058866</td>\n",
       "      <td>0.631133</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.623114</td>\n",
       "      <td>0.571568</td>\n",
       "      <td>0.592584</td>\n",
       "      <td>0.692634</td>\n",
       "      <td>0.576650</td>\n",
       "      <td>0.590511</td>\n",
       "      <td>0.600005</td>\n",
       "      <td>0.732167</td>\n",
       "      <td>0.569561</td>\n",
       "      <td>0.608356</td>\n",
       "      <td>0.554941</td>\n",
       "      <td>0.538904</td>\n",
       "      <td>0.564140</td>\n",
       "      <td>[46424, 8868, 6586, 8572, 2108, 11888, 8242, 4564]</td>\n",
       "      <td>[44724, 4656, 3808, 3726, 2792, 7846, 18892, 10624]</td>\n",
       "      <td>[61164, 4114, 3688, 1566, 2418, 15870, 5024, 3558]</td>\n",
       "      <td>[50018, 4362, 3584, 9578, 2320, 12168, 8456, 6528]</td>\n",
       "      <td>[51386, 5092, 9268, 5896, 3596, 5352, 9602, 7138]</td>\n",
       "      <td>[46120, 3920, 10744, 4122, 4070, 16082, 5458, 6356]</td>\n",
       "      <td>[49072, 6622, 11480, 8812, 1636, 6492, 8284, 4982]</td>\n",
       "      <td>[55104, 6112, 6316, 4492, 2232, 7126, 5112, 11448]</td>\n",
       "      <td>[69100, 5760, 3594, 5358, 2076, 6262, 3288, 3630]</td>\n",
       "      <td>[40374, 5920, 14254, 8060, 8398, 9616, 4710, 5908]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.998096</td>\n",
       "      <td>0.995435</td>\n",
       "      <td>0.996765</td>\n",
       "      <td>0.698211</td>\n",
       "      <td>0.706486</td>\n",
       "      <td>0.489211</td>\n",
       "      <td>0.476982</td>\n",
       "      <td>0.489211</td>\n",
       "      <td>0.649136</td>\n",
       "      <td>0.628680</td>\n",
       "      <td>0.638908</td>\n",
       "      <td>0.016334</td>\n",
       "      <td>0.071779</td>\n",
       "      <td>0.656354</td>\n",
       "      <td>0.625847</td>\n",
       "      <td>0.661278</td>\n",
       "      <td>0.624991</td>\n",
       "      <td>0.626069</td>\n",
       "      <td>0.725044</td>\n",
       "      <td>0.596796</td>\n",
       "      <td>0.627875</td>\n",
       "      <td>0.662491</td>\n",
       "      <td>0.802336</td>\n",
       "      <td>0.637322</td>\n",
       "      <td>0.616978</td>\n",
       "      <td>0.593791</td>\n",
       "      <td>0.541222</td>\n",
       "      <td>0.585225</td>\n",
       "      <td>[41415, 9141, 5168, 9770, 6472, 11385, 7634, 6267]</td>\n",
       "      <td>[37343, 3374, 1480, 1827, 10040, 12340, 12463, 18201]</td>\n",
       "      <td>[52766, 4306, 4751, 5332, 8364, 8285, 8565, 5033]</td>\n",
       "      <td>[45398, 8195, 6305, 5492, 5776, 10291, 8822, 6735]</td>\n",
       "      <td>[52836, 7895, 7916, 7939, 3986, 6303, 6155, 4300]</td>\n",
       "      <td>[44567, 5742, 4865, 6015, 7394, 10392, 12169, 5728]</td>\n",
       "      <td>[42100, 7506, 10245, 8876, 10614, 6370, 8509, 3160]</td>\n",
       "      <td>[57417, 8907, 6313, 4825, 5089, 1912, 8253, 5226]</td>\n",
       "      <td>[71724, 4936, 5393, 2295, 5368, 2981, 4312, 2059]</td>\n",
       "      <td>[46306, 4456, 5878, 4610, 11684, 13356, 5765, 5185]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.931730</td>\n",
       "      <td>0.664664</td>\n",
       "      <td>0.798197</td>\n",
       "      <td>0.664664</td>\n",
       "      <td>0.931730</td>\n",
       "      <td>0.460390</td>\n",
       "      <td>0.460390</td>\n",
       "      <td>0.460390</td>\n",
       "      <td>0.611254</td>\n",
       "      <td>0.611254</td>\n",
       "      <td>0.611254</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.065610</td>\n",
       "      <td>0.637222</td>\n",
       "      <td>0.604904</td>\n",
       "      <td>0.626425</td>\n",
       "      <td>0.591159</td>\n",
       "      <td>0.596562</td>\n",
       "      <td>0.699278</td>\n",
       "      <td>0.589821</td>\n",
       "      <td>0.570229</td>\n",
       "      <td>0.636057</td>\n",
       "      <td>0.758954</td>\n",
       "      <td>0.549575</td>\n",
       "      <td>0.610440</td>\n",
       "      <td>0.581385</td>\n",
       "      <td>0.543534</td>\n",
       "      <td>0.573272</td>\n",
       "      <td>[47358, 7932, 5626, 8582, 7580, 6582, 8756, 4836]</td>\n",
       "      <td>[45850, 5588, 2882, 3062, 7460, 6964, 14236, 11026]</td>\n",
       "      <td>[68390, 2602, 2580, 1976, 8246, 6058, 4876, 2674]</td>\n",
       "      <td>[46788, 6546, 4812, 6008, 5050, 12050, 9764, 5996]</td>\n",
       "      <td>[52004, 5540, 7326, 7116, 3798, 4794, 9428, 7324]</td>\n",
       "      <td>[45114, 4676, 8444, 4904, 8016, 14036, 6520, 5162]</td>\n",
       "      <td>[45260, 7108, 10862, 5708, 9482, 6994, 8052, 3914]</td>\n",
       "      <td>[57534, 6276, 6316, 3256, 6288, 4824, 5844, 7604]</td>\n",
       "      <td>[70394, 7390, 3274, 2900, 3674, 5830, 3602, 2004]</td>\n",
       "      <td>[38044, 9604, 7380, 8276, 14102, 9836, 4612, 5386]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.996593</td>\n",
       "      <td>0.999428</td>\n",
       "      <td>0.998010</td>\n",
       "      <td>0.694501</td>\n",
       "      <td>0.733685</td>\n",
       "      <td>0.493273</td>\n",
       "      <td>0.480024</td>\n",
       "      <td>0.493273</td>\n",
       "      <td>0.652768</td>\n",
       "      <td>0.634384</td>\n",
       "      <td>0.643576</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.070920</td>\n",
       "      <td>0.664944</td>\n",
       "      <td>0.631108</td>\n",
       "      <td>0.664002</td>\n",
       "      <td>0.630363</td>\n",
       "      <td>0.627463</td>\n",
       "      <td>0.734992</td>\n",
       "      <td>0.609360</td>\n",
       "      <td>0.633395</td>\n",
       "      <td>0.659408</td>\n",
       "      <td>0.805729</td>\n",
       "      <td>0.623977</td>\n",
       "      <td>0.631596</td>\n",
       "      <td>0.595726</td>\n",
       "      <td>0.551977</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>[42048, 9804, 5262, 8838, 6210, 12453, 7481, 5156]</td>\n",
       "      <td>[37777, 4222, 1526, 2281, 8806, 10711, 12917, 18828]</td>\n",
       "      <td>[54502, 3730, 7351, 4109, 7903, 8165, 6825, 4817]</td>\n",
       "      <td>[45301, 8917, 5037, 6426, 4735, 10294, 9610, 6694]</td>\n",
       "      <td>[53130, 7842, 7508, 7309, 3373, 6323, 6899, 4946]</td>\n",
       "      <td>[44067, 4681, 5288, 5481, 6977, 11840, 13153, 5385]</td>\n",
       "      <td>[40051, 7710, 13559, 8997, 6755, 7379, 8943, 3986]</td>\n",
       "      <td>[58468, 8908, 6891, 4569, 4679, 1668, 7627, 5132]</td>\n",
       "      <td>[72450, 6850, 3990, 2452, 3710, 3309, 4096, 2211]</td>\n",
       "      <td>[44264, 5042, 5333, 6796, 13680, 10768, 5686, 5671]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.976865</td>\n",
       "      <td>0.659236</td>\n",
       "      <td>0.818051</td>\n",
       "      <td>0.659236</td>\n",
       "      <td>0.976865</td>\n",
       "      <td>0.455647</td>\n",
       "      <td>0.455647</td>\n",
       "      <td>0.455647</td>\n",
       "      <td>0.603518</td>\n",
       "      <td>0.603518</td>\n",
       "      <td>0.603518</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>0.063838</td>\n",
       "      <td>0.631018</td>\n",
       "      <td>0.606504</td>\n",
       "      <td>0.623316</td>\n",
       "      <td>0.567007</td>\n",
       "      <td>0.589746</td>\n",
       "      <td>0.696409</td>\n",
       "      <td>0.607574</td>\n",
       "      <td>0.577233</td>\n",
       "      <td>0.619894</td>\n",
       "      <td>0.734124</td>\n",
       "      <td>0.548738</td>\n",
       "      <td>0.604176</td>\n",
       "      <td>0.584608</td>\n",
       "      <td>0.519467</td>\n",
       "      <td>0.542961</td>\n",
       "      <td>[47638, 6552, 8508, 8536, 5218, 7744, 7266, 5790]</td>\n",
       "      <td>[43664, 6024, 5058, 4304, 3900, 9846, 11942, 12330]</td>\n",
       "      <td>[65486, 2056, 4098, 1660, 6686, 8754, 4014, 4648]</td>\n",
       "      <td>[46180, 5818, 6542, 7334, 4446, 11712, 5932, 9050]</td>\n",
       "      <td>[52054, 3518, 8814, 6314, 4524, 5436, 7862, 8808]</td>\n",
       "      <td>[42992, 5368, 10584, 4186, 7340, 18404, 5026, 2972]</td>\n",
       "      <td>[43786, 6496, 15370, 4938, 6812, 8178, 8168, 3632]</td>\n",
       "      <td>[53148, 6688, 8498, 3514, 5866, 5084, 5618, 9526]</td>\n",
       "      <td>[66340, 9346, 4616, 3032, 3626, 6634, 3294, 2180]</td>\n",
       "      <td>[39314, 5580, 13276, 8262, 10204, 10764, 4768, 5072]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.998779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999389</td>\n",
       "      <td>0.699404</td>\n",
       "      <td>0.755264</td>\n",
       "      <td>0.493533</td>\n",
       "      <td>0.478405</td>\n",
       "      <td>0.493533</td>\n",
       "      <td>0.654842</td>\n",
       "      <td>0.630839</td>\n",
       "      <td>0.642841</td>\n",
       "      <td>0.014914</td>\n",
       "      <td>0.071271</td>\n",
       "      <td>0.660639</td>\n",
       "      <td>0.628401</td>\n",
       "      <td>0.661378</td>\n",
       "      <td>0.633208</td>\n",
       "      <td>0.630578</td>\n",
       "      <td>0.738520</td>\n",
       "      <td>0.597106</td>\n",
       "      <td>0.630004</td>\n",
       "      <td>0.662173</td>\n",
       "      <td>0.801433</td>\n",
       "      <td>0.631039</td>\n",
       "      <td>0.629637</td>\n",
       "      <td>0.599676</td>\n",
       "      <td>0.548212</td>\n",
       "      <td>0.590606</td>\n",
       "      <td>[42230, 10184, 4602, 9612, 6009, 11735, 7537, 5343]</td>\n",
       "      <td>[37595, 3817, 2791, 2469, 7836, 10062, 13353, 19145]</td>\n",
       "      <td>[55216, 3843, 5485, 3705, 8562, 7940, 7509, 5142]</td>\n",
       "      <td>[46691, 8702, 5155, 5316, 5420, 9818, 9777, 6135]</td>\n",
       "      <td>[52928, 7649, 7906, 7770, 4297, 6035, 6048, 4697]</td>\n",
       "      <td>[46112, 4493, 6393, 5967, 7386, 10697, 10716, 5108]</td>\n",
       "      <td>[38796, 8481, 11897, 8895, 8268, 6376, 10022, 4645]</td>\n",
       "      <td>[56303, 9278, 7391, 5021, 5357, 1881, 7366, 5345]</td>\n",
       "      <td>[72099, 5732, 3460, 2784, 4115, 4313, 4315, 2250]</td>\n",
       "      <td>[42791, 5639, 6897, 6571, 10244, 12293, 5872, 6933]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth    conv      pretrain  \\\n",
       "0       8     all            no   \n",
       "1       8  single            no   \n",
       "2       8  single  yes_nonfixed   \n",
       "3       8  single  yes_nonfixed   \n",
       "4       8  single  yes_nonfixed   \n",
       "5       8  single           yes   \n",
       "6       8  single           yes   \n",
       "7       8  single           yes   \n",
       "8       2  single           yes   \n",
       "9       2  single           yes   \n",
       "10      2  single           yes   \n",
       "11      2  single           yes   \n",
       "12      2  single           yes   \n",
       "13      2  single           yes   \n",
       "14      3  single           yes   \n",
       "15      3  single           yes   \n",
       "16      3  single           yes   \n",
       "17      3  single           yes   \n",
       "18      3  single           yes   \n",
       "19      3  single           yes   \n",
       "20      4  single           yes   \n",
       "21      4  single           yes   \n",
       "22      4  single           yes   \n",
       "23      4  single           yes   \n",
       "24      4  single           yes   \n",
       "25      4  single           yes   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    training_config  \\\n",
       "0   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "1   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "2   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "3   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "4   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "5                                                                                               {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "6                                                                                               {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "7                                                                                               {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "8                                                                                                    {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "9   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "10                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "11  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "12                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "13  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "14                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "15  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "16                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "17  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "18                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "19  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "20                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "21  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "22                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "23  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "24                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "25  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "\n",
       "    train accuracy steady fold1  train accuracy steady fold2  \\\n",
       "0                      0.832824                     0.891702   \n",
       "1                      0.882529                     0.859839   \n",
       "2                      0.959673                     0.998564   \n",
       "3                      0.998241                     0.998068   \n",
       "4                      0.997923                     0.999701   \n",
       "5                      1.000000                     1.000000   \n",
       "6                      1.000000                     0.999820   \n",
       "7                      1.000000                     0.999773   \n",
       "8                      0.856633                     0.667566   \n",
       "9                      0.996431                     0.995991   \n",
       "10                     0.901621                     0.685970   \n",
       "11                     0.999726                     0.999232   \n",
       "12                     0.953040                     0.691374   \n",
       "13                     1.000000                     0.999630   \n",
       "14                     0.859060                     0.656857   \n",
       "15                     0.998338                     0.995159   \n",
       "16                     0.911664                     0.664353   \n",
       "17                     0.999828                     1.000000   \n",
       "18                     0.968732                     0.669654   \n",
       "19                     0.999934                     0.999934   \n",
       "20                     0.853297                     0.643881   \n",
       "21                     0.998096                     0.995435   \n",
       "22                     0.931730                     0.664664   \n",
       "23                     0.996593                     0.999428   \n",
       "24                     0.976865                     0.659236   \n",
       "25                     0.998779                     1.000000   \n",
       "\n",
       "    train accuracy steady avg2folds  validation accuracy steady fold1  \\\n",
       "0                          0.862263                          0.628401   \n",
       "1                          0.871184                          0.647656   \n",
       "2                          0.979119                          0.676759   \n",
       "3                          0.998154                          0.708974   \n",
       "4                          0.998812                          0.707988   \n",
       "5                          1.000000                          0.701014   \n",
       "6                          0.999910                          0.702524   \n",
       "7                          0.999886                          0.702832   \n",
       "8                          0.762099                          0.667566   \n",
       "9                          0.996211                          0.709019   \n",
       "10                         0.793796                          0.685970   \n",
       "11                         0.999479                          0.723029   \n",
       "12                         0.822207                          0.691374   \n",
       "13                         0.999815                          0.725700   \n",
       "14                         0.757958                          0.656857   \n",
       "15                         0.996749                          0.702716   \n",
       "16                         0.788009                          0.664353   \n",
       "17                         0.999914                          0.709069   \n",
       "18                         0.819193                          0.669654   \n",
       "19                         0.999934                          0.705617   \n",
       "20                         0.748589                          0.643881   \n",
       "21                         0.996765                          0.698211   \n",
       "22                         0.798197                          0.664664   \n",
       "23                         0.998010                          0.694501   \n",
       "24                         0.818051                          0.659236   \n",
       "25                         0.999389                          0.699404   \n",
       "\n",
       "    validation accuracy steady fold2  test accuracy fold1  \\\n",
       "0                           0.638954             0.441933   \n",
       "1                           0.635783             0.455730   \n",
       "2                           0.709262             0.461770   \n",
       "3                           0.736615             0.492865   \n",
       "4                           0.750072             0.488877   \n",
       "5                           0.764761             0.483799   \n",
       "6                           0.807746             0.478013   \n",
       "7                           0.840944             0.472904   \n",
       "8                           0.856633             0.452885   \n",
       "9                           0.716973             0.491747   \n",
       "10                          0.901621             0.460167   \n",
       "11                          0.735722             0.497365   \n",
       "12                          0.953040             0.459843   \n",
       "13                          0.742529             0.499154   \n",
       "14                          0.859060             0.451891   \n",
       "15                          0.706679             0.490566   \n",
       "16                          0.911664             0.463722   \n",
       "17                          0.735511             0.495712   \n",
       "18                          0.968732             0.458673   \n",
       "19                          0.743060             0.497541   \n",
       "20                          0.853297             0.452398   \n",
       "21                          0.706486             0.489211   \n",
       "22                          0.931730             0.460390   \n",
       "23                          0.733685             0.493273   \n",
       "24                          0.976865             0.455647   \n",
       "25                          0.755264             0.493533   \n",
       "\n",
       "    test accuracy fold2  test accuracy avg2folds  test accuracy steady fold1  \\\n",
       "0              0.449447                 0.441933                    0.602031   \n",
       "1              0.448515                 0.455730                    0.614778   \n",
       "2              0.472543                 0.461770                    0.617109   \n",
       "3              0.476860                 0.492865                    0.654383   \n",
       "4              0.479657                 0.488877                    0.651552   \n",
       "5              0.474300                 0.483799                    0.643965   \n",
       "6              0.471284                 0.478013                    0.636780   \n",
       "7              0.473583                 0.472904                    0.630852   \n",
       "8              0.452885                 0.452885                    0.599935   \n",
       "9              0.473887                 0.491747                    0.652992   \n",
       "10             0.460167                 0.460167                    0.613291   \n",
       "11             0.476320                 0.497365                    0.660166   \n",
       "12             0.459843                 0.459843                    0.612123   \n",
       "13             0.476909                 0.499154                    0.662457   \n",
       "14             0.451891                 0.451891                    0.604689   \n",
       "15             0.470759                 0.490566                    0.651274   \n",
       "16             0.463722                 0.463722                    0.618058   \n",
       "17             0.478257                 0.495712                    0.658047   \n",
       "18             0.458673                 0.458673                    0.612312   \n",
       "19             0.478500                 0.497541                    0.659025   \n",
       "20             0.452398                 0.452398                    0.602787   \n",
       "21             0.476982                 0.489211                    0.649136   \n",
       "22             0.460390                 0.460390                    0.611254   \n",
       "23             0.480024                 0.493273                    0.652768   \n",
       "24             0.455647                 0.455647                    0.603518   \n",
       "25             0.478405                 0.493533                    0.654842   \n",
       "\n",
       "    test accuracy steady fold2  test accuracy steady avg2folds  \\\n",
       "0                     0.605940                        0.603985   \n",
       "1                     0.605766                        0.610272   \n",
       "2                     0.623918                        0.620513   \n",
       "3                     0.629076                        0.641730   \n",
       "4                     0.633752                        0.642652   \n",
       "5                     0.626183                        0.635074   \n",
       "6                     0.624332                        0.630556   \n",
       "7                     0.626580                        0.628716   \n",
       "8                     0.599935                        0.599935   \n",
       "9                     0.626230                        0.639611   \n",
       "10                    0.613291                        0.613291   \n",
       "11                    0.630630                        0.645398   \n",
       "12                    0.612123                        0.612123   \n",
       "13                    0.631552                        0.647005   \n",
       "14                    0.604689                        0.604689   \n",
       "15                    0.622143                        0.636708   \n",
       "16                    0.618058                        0.618058   \n",
       "17                    0.631495                        0.644771   \n",
       "18                    0.612312                        0.612312   \n",
       "19                    0.632198                        0.645612   \n",
       "20                    0.602787                        0.602787   \n",
       "21                    0.628680                        0.638908   \n",
       "22                    0.611254                        0.611254   \n",
       "23                    0.634384                        0.643576   \n",
       "24                    0.603518                        0.603518   \n",
       "25                    0.630839                        0.642841   \n",
       "\n",
       "    test accuracy steady avg2folds std across sessions  \\\n",
       "0                                             0.012319   \n",
       "1                                             0.016849   \n",
       "2                                             0.019044   \n",
       "3                                             0.018011   \n",
       "4                                             0.017293   \n",
       "5                                             0.018878   \n",
       "6                                             0.020094   \n",
       "7                                             0.021979   \n",
       "8                                             0.026289   \n",
       "9                                             0.017223   \n",
       "10                                            0.022625   \n",
       "11                                            0.016065   \n",
       "12                                            0.026010   \n",
       "13                                            0.016744   \n",
       "14                                            0.020941   \n",
       "15                                            0.017367   \n",
       "16                                            0.012520   \n",
       "17                                            0.016597   \n",
       "18                                            0.017310   \n",
       "19                                            0.014943   \n",
       "20                                            0.021670   \n",
       "21                                            0.016334   \n",
       "22                                            0.017689   \n",
       "23                                            0.017108   \n",
       "24                                            0.023151   \n",
       "25                                            0.014914   \n",
       "\n",
       "    test accuracy steady avg2folds std across subjects  \\\n",
       "0                                             0.118277   \n",
       "1                                             0.106803   \n",
       "2                                             0.059948   \n",
       "3                                             0.070781   \n",
       "4                                             0.067983   \n",
       "5                                             0.070415   \n",
       "6                                             0.069056   \n",
       "7                                             0.069588   \n",
       "8                                             0.069445   \n",
       "9                                             0.073236   \n",
       "10                                            0.061504   \n",
       "11                                            0.071867   \n",
       "12                                            0.060849   \n",
       "13                                            0.070378   \n",
       "14                                            0.065078   \n",
       "15                                            0.073221   \n",
       "16                                            0.058368   \n",
       "17                                            0.075293   \n",
       "18                                            0.060833   \n",
       "19                                            0.073521   \n",
       "20                                            0.058866   \n",
       "21                                            0.071779   \n",
       "22                                            0.065610   \n",
       "23                                            0.070920   \n",
       "24                                            0.063838   \n",
       "25                                            0.071271   \n",
       "\n",
       "    test accuracy steady session6 avg2folds  \\\n",
       "0                                  0.616156   \n",
       "1                                  0.622817   \n",
       "2                                  0.642582   \n",
       "3                                  0.663354   \n",
       "4                                  0.661105   \n",
       "5                                  0.651722   \n",
       "6                                  0.647797   \n",
       "7                                  0.646831   \n",
       "8                                  0.636965   \n",
       "9                                  0.660144   \n",
       "10                                 0.645334   \n",
       "11                                 0.655506   \n",
       "12                                 0.637352   \n",
       "13                                 0.668169   \n",
       "14                                 0.626146   \n",
       "15                                 0.653338   \n",
       "16                                 0.630644   \n",
       "17                                 0.665452   \n",
       "18                                 0.631914   \n",
       "19                                 0.664734   \n",
       "20                                 0.631133   \n",
       "21                                 0.656354   \n",
       "22                                 0.637222   \n",
       "23                                 0.664944   \n",
       "24                                 0.631018   \n",
       "25                                 0.660639   \n",
       "\n",
       "    test accuracy steady session7 avg2folds  \\\n",
       "0                                  0.590528   \n",
       "1                                  0.589404   \n",
       "2                                  0.613916   \n",
       "3                                  0.633657   \n",
       "4                                  0.634181   \n",
       "5                                  0.626734   \n",
       "6                                  0.627941   \n",
       "7                                  0.625515   \n",
       "8                                  0.598142   \n",
       "9                                  0.625860   \n",
       "10                                 0.614970   \n",
       "11                                 0.637054   \n",
       "12                                 0.622971   \n",
       "13                                 0.638057   \n",
       "14                                 0.607255   \n",
       "15                                 0.625748   \n",
       "16                                 0.614631   \n",
       "17                                 0.634398   \n",
       "18                                 0.609907   \n",
       "19                                 0.634393   \n",
       "20                                 0.595536   \n",
       "21                                 0.625847   \n",
       "22                                 0.604904   \n",
       "23                                 0.631108   \n",
       "24                                 0.606504   \n",
       "25                                 0.628401   \n",
       "\n",
       "    test accuracy steady session8 avg2folds  \\\n",
       "0                                  0.620980   \n",
       "1                                  0.636001   \n",
       "2                                  0.643545   \n",
       "3                                  0.663246   \n",
       "4                                  0.665290   \n",
       "5                                  0.662666   \n",
       "6                                  0.658224   \n",
       "7                                  0.659600   \n",
       "8                                  0.616040   \n",
       "9                                  0.661206   \n",
       "10                                 0.629416   \n",
       "11                                 0.671994   \n",
       "12                                 0.637267   \n",
       "13                                 0.666245   \n",
       "14                                 0.626145   \n",
       "15                                 0.661820   \n",
       "16                                 0.634751   \n",
       "17                                 0.664523   \n",
       "18                                 0.631660   \n",
       "19                                 0.662958   \n",
       "20                                 0.623114   \n",
       "21                                 0.661278   \n",
       "22                                 0.626425   \n",
       "23                                 0.664002   \n",
       "24                                 0.623316   \n",
       "25                                 0.661378   \n",
       "\n",
       "    test accuracy steady session9 avg2folds  \\\n",
       "0                                  0.593185   \n",
       "1                                  0.599015   \n",
       "2                                  0.598913   \n",
       "3                                  0.621813   \n",
       "4                                  0.621955   \n",
       "5                                  0.613291   \n",
       "6                                  0.604172   \n",
       "7                                  0.599720   \n",
       "8                                  0.558392   \n",
       "9                                  0.626749   \n",
       "10                                 0.584667   \n",
       "11                                 0.633356   \n",
       "12                                 0.573689   \n",
       "13                                 0.629173   \n",
       "14                                 0.571201   \n",
       "15                                 0.619437   \n",
       "16                                 0.605734   \n",
       "17                                 0.629593   \n",
       "18                                 0.588293   \n",
       "19                                 0.634618   \n",
       "20                                 0.571568   \n",
       "21                                 0.624991   \n",
       "22                                 0.591159   \n",
       "23                                 0.630363   \n",
       "24                                 0.567007   \n",
       "25                                 0.633208   \n",
       "\n",
       "    test accuracy steady session10 avg2folds  \\\n",
       "0                                   0.599078   \n",
       "1                                   0.604123   \n",
       "2                                   0.603610   \n",
       "3                                   0.626578   \n",
       "4                                   0.630728   \n",
       "5                                   0.620956   \n",
       "6                                   0.614647   \n",
       "7                                   0.611913   \n",
       "8                                   0.590136   \n",
       "9                                   0.624097   \n",
       "10                                  0.592066   \n",
       "11                                  0.629081   \n",
       "12                                  0.589338   \n",
       "13                                  0.633378   \n",
       "14                                  0.592700   \n",
       "15                                  0.623199   \n",
       "16                                  0.604527   \n",
       "17                                  0.629889   \n",
       "18                                  0.599786   \n",
       "19                                  0.631355   \n",
       "20                                  0.592584   \n",
       "21                                  0.626069   \n",
       "22                                  0.596562   \n",
       "23                                  0.627463   \n",
       "24                                  0.589746   \n",
       "25                                  0.630578   \n",
       "\n",
       "    test accuracy steady subj0 avg2folds  \\\n",
       "0                               0.753866   \n",
       "1                               0.746648   \n",
       "2                               0.743256   \n",
       "3                               0.741090   \n",
       "4                               0.737263   \n",
       "5                               0.734223   \n",
       "6                               0.721906   \n",
       "7                               0.727337   \n",
       "8                               0.706369   \n",
       "9                               0.742451   \n",
       "10                              0.709041   \n",
       "11                              0.746291   \n",
       "12                              0.709736   \n",
       "13                              0.750426   \n",
       "14                              0.705325   \n",
       "15                              0.733087   \n",
       "16                              0.714990   \n",
       "17                              0.750079   \n",
       "18                              0.708523   \n",
       "19                              0.753740   \n",
       "20                              0.692634   \n",
       "21                              0.725044   \n",
       "22                              0.699278   \n",
       "23                              0.734992   \n",
       "24                              0.696409   \n",
       "25                              0.738520   \n",
       "\n",
       "    test accuracy steady subj1 avg2folds  \\\n",
       "0                               0.684087   \n",
       "1                               0.673362   \n",
       "2                               0.541221   \n",
       "3                               0.608682   \n",
       "4                               0.616858   \n",
       "5                               0.594341   \n",
       "6                               0.600570   \n",
       "7                               0.597063   \n",
       "8                               0.570263   \n",
       "9                               0.609140   \n",
       "10                              0.588212   \n",
       "11                              0.610501   \n",
       "12                              0.586230   \n",
       "13                              0.612469   \n",
       "14                              0.605132   \n",
       "15                              0.598245   \n",
       "16                              0.617381   \n",
       "17                              0.587470   \n",
       "18                              0.595221   \n",
       "19                              0.605932   \n",
       "20                              0.576650   \n",
       "21                              0.596796   \n",
       "22                              0.589821   \n",
       "23                              0.609360   \n",
       "24                              0.607574   \n",
       "25                              0.597106   \n",
       "\n",
       "    test accuracy steady subj2 avg2folds  \\\n",
       "0                               0.450906   \n",
       "1                               0.450906   \n",
       "2                               0.602619   \n",
       "3                               0.613583   \n",
       "4                               0.606898   \n",
       "5                               0.596575   \n",
       "6                               0.582704   \n",
       "7                               0.588965   \n",
       "8                               0.550364   \n",
       "9                               0.614546   \n",
       "10                              0.557945   \n",
       "11                              0.618789   \n",
       "12                              0.581170   \n",
       "13                              0.610064   \n",
       "14                              0.544967   \n",
       "15                              0.611534   \n",
       "16                              0.588821   \n",
       "17                              0.619324   \n",
       "18                              0.569419   \n",
       "19                              0.622046   \n",
       "20                              0.590511   \n",
       "21                              0.627875   \n",
       "22                              0.570229   \n",
       "23                              0.633395   \n",
       "24                              0.577233   \n",
       "25                              0.630004   \n",
       "\n",
       "    test accuracy steady subj3 avg2folds  \\\n",
       "0                               0.670771   \n",
       "1                               0.668524   \n",
       "2                               0.656861   \n",
       "3                               0.661406   \n",
       "4                               0.671402   \n",
       "5                               0.658075   \n",
       "6                               0.657313   \n",
       "7                               0.652800   \n",
       "8                               0.551571   \n",
       "9                               0.662740   \n",
       "10                              0.603116   \n",
       "11                              0.665040   \n",
       "12                              0.599906   \n",
       "13                              0.671816   \n",
       "14                              0.612772   \n",
       "15                              0.654791   \n",
       "16                              0.619772   \n",
       "17                              0.673124   \n",
       "18                              0.624522   \n",
       "19                              0.676639   \n",
       "20                              0.600005   \n",
       "21                              0.662491   \n",
       "22                              0.636057   \n",
       "23                              0.659408   \n",
       "24                              0.619894   \n",
       "25                              0.662173   \n",
       "\n",
       "    test accuracy steady subj4 avg2folds  \\\n",
       "0                               0.801496   \n",
       "1                               0.787117   \n",
       "2                               0.698363   \n",
       "3                               0.802737   \n",
       "4                               0.791050   \n",
       "5                               0.793397   \n",
       "6                               0.787451   \n",
       "7                               0.783393   \n",
       "8                               0.743818   \n",
       "9                               0.801409   \n",
       "10                              0.750749   \n",
       "11                              0.806116   \n",
       "12                              0.740620   \n",
       "13                              0.798918   \n",
       "14                              0.741832   \n",
       "15                              0.804417   \n",
       "16                              0.734656   \n",
       "17                              0.810435   \n",
       "18                              0.742783   \n",
       "19                              0.801224   \n",
       "20                              0.732167   \n",
       "21                              0.802336   \n",
       "22                              0.758954   \n",
       "23                              0.805729   \n",
       "24                              0.734124   \n",
       "25                              0.801433   \n",
       "\n",
       "    test accuracy steady subj5 avg2folds  \\\n",
       "0                               0.651356   \n",
       "1                               0.654543   \n",
       "2                               0.615981   \n",
       "3                               0.616337   \n",
       "4                               0.622458   \n",
       "5                               0.613355   \n",
       "6                               0.604389   \n",
       "7                               0.590392   \n",
       "8                               0.568019   \n",
       "9                               0.622619   \n",
       "10                              0.580515   \n",
       "11                              0.624437   \n",
       "12                              0.592312   \n",
       "13                              0.623259   \n",
       "14                              0.570274   \n",
       "15                              0.619616   \n",
       "16                              0.603945   \n",
       "17                              0.623077   \n",
       "18                              0.565534   \n",
       "19                              0.625256   \n",
       "20                              0.569561   \n",
       "21                              0.637322   \n",
       "22                              0.549575   \n",
       "23                              0.623977   \n",
       "24                              0.548738   \n",
       "25                              0.631039   \n",
       "\n",
       "    test accuracy steady subj6 avg2folds  \\\n",
       "0                               0.541126   \n",
       "1                               0.558581   \n",
       "2                               0.615221   \n",
       "3                               0.622353   \n",
       "4                               0.630461   \n",
       "5                               0.625503   \n",
       "6                               0.627124   \n",
       "7                               0.630161   \n",
       "8                               0.642014   \n",
       "9                               0.621077   \n",
       "10                              0.613696   \n",
       "11                              0.627375   \n",
       "12                              0.627469   \n",
       "13                              0.635534   \n",
       "14                              0.609048   \n",
       "15                              0.621649   \n",
       "16                              0.615026   \n",
       "17                              0.637280   \n",
       "18                              0.611042   \n",
       "19                              0.630688   \n",
       "20                              0.608356   \n",
       "21                              0.616978   \n",
       "22                              0.610440   \n",
       "23                              0.631596   \n",
       "24                              0.604176   \n",
       "25                              0.629637   \n",
       "\n",
       "    test accuracy steady subj7 avg2folds  \\\n",
       "0                               0.477420   \n",
       "1                               0.477420   \n",
       "2                               0.594285   \n",
       "3                               0.597427   \n",
       "4                               0.602135   \n",
       "5                               0.589611   \n",
       "6                               0.582811   \n",
       "7                               0.581199   \n",
       "8                               0.580132   \n",
       "9                               0.585088   \n",
       "10                              0.590188   \n",
       "11                              0.598092   \n",
       "12                              0.578533   \n",
       "13                              0.599882   \n",
       "14                              0.568533   \n",
       "15                              0.589890   \n",
       "16                              0.581164   \n",
       "17                              0.595256   \n",
       "18                              0.580212   \n",
       "19                              0.595509   \n",
       "20                              0.554941   \n",
       "21                              0.593791   \n",
       "22                              0.581385   \n",
       "23                              0.595726   \n",
       "24                              0.584608   \n",
       "25                              0.599676   \n",
       "\n",
       "    test accuracy steady subj8 avg2folds  \\\n",
       "0                               0.536560   \n",
       "1                               0.538243   \n",
       "2                               0.549048   \n",
       "3                               0.559118   \n",
       "4                               0.553396   \n",
       "5                               0.554981   \n",
       "6                               0.552383   \n",
       "7                               0.552252   \n",
       "8                               0.521666   \n",
       "9                               0.545317   \n",
       "10                              0.552930   \n",
       "11                              0.552015   \n",
       "12                              0.538976   \n",
       "13                              0.554783   \n",
       "14                              0.540736   \n",
       "15                              0.541346   \n",
       "16                              0.556304   \n",
       "17                              0.554179   \n",
       "18                              0.549705   \n",
       "19                              0.547757   \n",
       "20                              0.538904   \n",
       "21                              0.541222   \n",
       "22                              0.543534   \n",
       "23                              0.551977   \n",
       "24                              0.519467   \n",
       "25                              0.548212   \n",
       "\n",
       "    test accuracy steady subj9 avg2folds  \\\n",
       "0                               0.472264   \n",
       "1                               0.547374   \n",
       "2                               0.588277   \n",
       "3                               0.594563   \n",
       "4                               0.594600   \n",
       "5                               0.590679   \n",
       "6                               0.588909   \n",
       "7                               0.583597   \n",
       "8                               0.565136   \n",
       "9                               0.591726   \n",
       "10                              0.586513   \n",
       "11                              0.605326   \n",
       "12                              0.566282   \n",
       "13                              0.612895   \n",
       "14                              0.548271   \n",
       "15                              0.592509   \n",
       "16                              0.548517   \n",
       "17                              0.597484   \n",
       "18                              0.576160   \n",
       "19                              0.597327   \n",
       "20                              0.564140   \n",
       "21                              0.585225   \n",
       "22                              0.573272   \n",
       "23                              0.589600   \n",
       "24                              0.542961   \n",
       "25                              0.590606   \n",
       "\n",
       "                      test preds steady subj0 avg2folds  \\\n",
       "0    [41807, 8911, 7252, 11555, 5250, 8265, 8493, 5719]   \n",
       "1     [43159, 9577, 8003, 8520, 5472, 8625, 8858, 5038]   \n",
       "2   [41603, 10600, 6281, 9057, 5419, 10113, 8405, 5774]   \n",
       "3    [42732, 9380, 6097, 10432, 5282, 9455, 8193, 5681]   \n",
       "4    [43115, 9285, 6600, 7995, 5412, 10557, 8257, 6031]   \n",
       "5     [43426, 8087, 5520, 9837, 5046, 9789, 8088, 7459]   \n",
       "6    [44290, 7628, 4814, 9366, 4816, 8933, 10043, 7362]   \n",
       "7    [44030, 8163, 4897, 8939, 4745, 8144, 10883, 7451]   \n",
       "8    [43032, 8378, 7302, 7696, 5438, 10330, 8276, 6800]   \n",
       "9    [41349, 9469, 5210, 9590, 5803, 10619, 9143, 6069]   \n",
       "10    [45276, 7468, 6290, 9056, 5316, 9490, 9510, 4846]   \n",
       "11   [42768, 8915, 5681, 8781, 5823, 11728, 8433, 5123]   \n",
       "12  [44844, 6784, 3978, 10430, 6180, 8998, 10810, 5228]   \n",
       "13   [42581, 9477, 5652, 8762, 5915, 11209, 8413, 5243]   \n",
       "14   [45500, 6498, 8392, 11368, 5328, 8234, 6886, 5046]   \n",
       "15  [42924, 10369, 7385, 10076, 4499, 9695, 7083, 5221]   \n",
       "16  [42096, 7564, 7350, 10414, 6060, 11762, 6466, 5540]   \n",
       "17   [42216, 9943, 5851, 8560, 6382, 11278, 7825, 5197]   \n",
       "18    [45576, 6794, 6006, 9412, 6228, 8690, 9472, 5074]   \n",
       "19   [42446, 9278, 6553, 9111, 5516, 10569, 8638, 5141]   \n",
       "20   [46424, 8868, 6586, 8572, 2108, 11888, 8242, 4564]   \n",
       "21   [41415, 9141, 5168, 9770, 6472, 11385, 7634, 6267]   \n",
       "22    [47358, 7932, 5626, 8582, 7580, 6582, 8756, 4836]   \n",
       "23   [42048, 9804, 5262, 8838, 6210, 12453, 7481, 5156]   \n",
       "24    [47638, 6552, 8508, 8536, 5218, 7744, 7266, 5790]   \n",
       "25  [42230, 10184, 4602, 9612, 6009, 11735, 7537, 5343]   \n",
       "\n",
       "                        test preds steady subj1 avg2folds  \\\n",
       "0    [37130, 10021, 4311, 7235, 14533, 10389, 6589, 6860]   \n",
       "1     [35555, 9297, 6584, 7208, 13775, 12204, 6015, 6430]   \n",
       "2     [34901, 2659, 906, 1782, 5888, 11619, 12282, 27031]   \n",
       "3    [37894, 3918, 1626, 2037, 8323, 13903, 13684, 15683]   \n",
       "4    [39793, 4466, 1305, 3008, 8029, 11032, 14869, 14566]   \n",
       "5     [39860, 4575, 1939, 3178, 7745, 7514, 14139, 18118]   \n",
       "6     [41895, 3567, 1726, 3068, 7367, 8923, 15480, 15042]   \n",
       "7     [41544, 4669, 1501, 3609, 6368, 5648, 19503, 14226]   \n",
       "8     [40844, 3362, 4898, 3110, 6048, 8036, 16512, 14258]   \n",
       "9   [37060, 4211, 2475, 1805, 10566, 11821, 12975, 16155]   \n",
       "10    [42562, 5104, 5338, 3826, 4726, 7174, 15276, 13062]   \n",
       "11  [36844, 3902, 1468, 2232, 11468, 12234, 13798, 15122]   \n",
       "12    [42326, 5950, 5062, 5870, 4482, 6176, 14960, 12242]   \n",
       "13  [37310, 3475, 1478, 1919, 10970, 12726, 13156, 16034]   \n",
       "14    [43872, 4398, 7416, 4822, 4616, 5938, 14280, 11726]   \n",
       "15   [37150, 4002, 2340, 1723, 9489, 13091, 12541, 16732]   \n",
       "16    [41550, 4726, 7316, 6204, 4092, 12670, 7498, 13012]   \n",
       "17   [36040, 3674, 2130, 2365, 6392, 11750, 15862, 18855]   \n",
       "18    [41936, 3840, 4236, 4194, 7206, 6418, 14256, 14982]   \n",
       "19   [36482, 4664, 2492, 2254, 8022, 11546, 15862, 15746]   \n",
       "20    [44724, 4656, 3808, 3726, 2792, 7846, 18892, 10624]   \n",
       "21  [37343, 3374, 1480, 1827, 10040, 12340, 12463, 18201]   \n",
       "22    [45850, 5588, 2882, 3062, 7460, 6964, 14236, 11026]   \n",
       "23   [37777, 4222, 1526, 2281, 8806, 10711, 12917, 18828]   \n",
       "24    [43664, 6024, 5058, 4304, 3900, 9846, 11942, 12330]   \n",
       "25   [37595, 3817, 2791, 2469, 7836, 10062, 13353, 19145]   \n",
       "\n",
       "                     test preds steady subj2 avg2folds  \\\n",
       "0                         [97402, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1                         [97402, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2   [55468, 3750, 4904, 4073, 7213, 10869, 5904, 5221]   \n",
       "3   [54454, 3561, 6678, 4240, 7351, 10790, 6140, 4188]   \n",
       "4    [59113, 3787, 5261, 3256, 9532, 8669, 3745, 4039]   \n",
       "5    [55611, 3600, 6652, 2922, 7491, 9971, 5388, 5767]   \n",
       "6   [55801, 2912, 6968, 2807, 7047, 11267, 5424, 5176]   \n",
       "7   [57643, 3724, 5541, 3079, 7479, 10554, 4307, 5075]   \n",
       "8   [67090, 2956, 1696, 1016, 6354, 12694, 2574, 3022]   \n",
       "9    [56629, 4186, 5231, 4385, 8075, 6273, 6299, 6324]   \n",
       "10    [68540, 3578, 4910, 942, 3044, 12096, 3402, 890]   \n",
       "11   [53796, 4478, 4587, 4879, 6607, 9344, 7211, 6500]   \n",
       "12  [61836, 2212, 4056, 2434, 7178, 10384, 7304, 1998]   \n",
       "13   [56681, 3802, 3779, 3764, 7380, 7321, 8420, 6255]   \n",
       "14  [68750, 1518, 2834, 1936, 7002, 10502, 2636, 2224]   \n",
       "15   [54510, 3235, 5853, 4363, 7535, 9866, 8389, 3651]   \n",
       "16  [61342, 2520, 2388, 3970, 9978, 12718, 1444, 3042]   \n",
       "17   [55369, 3817, 6565, 3857, 7553, 8449, 6788, 5004]   \n",
       "18   [64878, 2510, 4990, 2234, 6460, 9218, 4160, 2952]   \n",
       "19   [55267, 4024, 4524, 4389, 8905, 9610, 5266, 5417]   \n",
       "20  [61164, 4114, 3688, 1566, 2418, 15870, 5024, 3558]   \n",
       "21   [52766, 4306, 4751, 5332, 8364, 8285, 8565, 5033]   \n",
       "22   [68390, 2602, 2580, 1976, 8246, 6058, 4876, 2674]   \n",
       "23   [54502, 3730, 7351, 4109, 7903, 8165, 6825, 4817]   \n",
       "24   [65486, 2056, 4098, 1660, 6686, 8754, 4014, 4648]   \n",
       "25   [55216, 3843, 5485, 3705, 8562, 7940, 7509, 5142]   \n",
       "\n",
       "                      test preds steady subj3 avg2folds  \\\n",
       "0    [45596, 6682, 6356, 7039, 7494, 12349, 5225, 6273]   \n",
       "1    [43968, 8248, 5617, 8590, 6828, 12203, 6886, 4674]   \n",
       "2    [47962, 9259, 4465, 3970, 6029, 11772, 8041, 5516]   \n",
       "3     [45895, 8892, 4499, 4612, 6601, 9868, 9783, 6864]   \n",
       "4    [46457, 7633, 5563, 5589, 6069, 10831, 8701, 6171]   \n",
       "5    [46824, 7787, 5527, 6266, 5439, 10751, 8663, 5757]   \n",
       "6    [46969, 7024, 5467, 6470, 6181, 11766, 7567, 5570]   \n",
       "7    [46061, 6775, 5052, 6628, 6273, 11788, 8376, 6061]   \n",
       "8   [41908, 6354, 1982, 12918, 2768, 15794, 9088, 6202]   \n",
       "9     [47119, 8846, 5968, 4902, 6268, 9855, 8812, 5244]   \n",
       "10   [48908, 7524, 2918, 7512, 4406, 12862, 5976, 6908]   \n",
       "11   [47216, 7182, 6307, 6177, 7185, 10310, 7793, 4844]   \n",
       "12   [45874, 8146, 5394, 7372, 5254, 11914, 5652, 7408]   \n",
       "13   [47842, 8296, 5295, 6366, 6466, 10196, 7675, 4878]   \n",
       "14    [49874, 5030, 4732, 8700, 3716, 9712, 5872, 9378]   \n",
       "15   [47298, 5761, 4074, 5170, 7006, 11860, 8444, 7401]   \n",
       "16   [45002, 6598, 6134, 6964, 4348, 13870, 5772, 8326]   \n",
       "17   [46723, 7934, 4722, 5959, 5308, 11409, 9097, 5862]   \n",
       "18   [46962, 5968, 6056, 8566, 5668, 10870, 7176, 5748]   \n",
       "19   [46697, 7943, 5550, 6077, 5003, 10698, 9116, 5930]   \n",
       "20   [50018, 4362, 3584, 9578, 2320, 12168, 8456, 6528]   \n",
       "21   [45398, 8195, 6305, 5492, 5776, 10291, 8822, 6735]   \n",
       "22   [46788, 6546, 4812, 6008, 5050, 12050, 9764, 5996]   \n",
       "23   [45301, 8917, 5037, 6426, 4735, 10294, 9610, 6694]   \n",
       "24   [46180, 5818, 6542, 7334, 4446, 11712, 5932, 9050]   \n",
       "25    [46691, 8702, 5155, 5316, 5420, 9818, 9777, 6135]   \n",
       "\n",
       "                     test preds steady subj4 avg2folds  \\\n",
       "0    [51944, 7929, 7942, 7570, 4418, 6030, 6991, 4506]   \n",
       "1    [52325, 8817, 7123, 7560, 3986, 5596, 8221, 3702]   \n",
       "2   [53005, 4404, 10682, 2972, 8192, 4930, 3816, 9329]   \n",
       "3    [53051, 7970, 9514, 5260, 4665, 5714, 6734, 4422]   \n",
       "4    [52701, 7551, 8620, 6385, 4664, 5717, 7183, 4509]   \n",
       "5    [52223, 6711, 8547, 6653, 4597, 5466, 7289, 5844]   \n",
       "6    [52604, 6767, 8361, 6425, 4677, 4804, 7843, 5849]   \n",
       "7    [52326, 6627, 8173, 6890, 4463, 4476, 8202, 6173]   \n",
       "8    [51428, 5758, 8040, 8244, 4586, 5878, 8318, 5078]   \n",
       "9    [53032, 7977, 7167, 8271, 3994, 6180, 6706, 4003]   \n",
       "10   [52700, 5520, 7428, 8946, 5586, 5190, 6936, 5024]   \n",
       "11   [53109, 7812, 8266, 7943, 3603, 6168, 6077, 4352]   \n",
       "12   [51798, 5426, 4964, 8540, 5622, 5736, 8210, 7034]   \n",
       "13   [53236, 8404, 7571, 8628, 3582, 6026, 5828, 4055]   \n",
       "14   [52914, 3188, 9836, 6426, 6070, 4472, 7020, 7404]   \n",
       "15   [52980, 8211, 8824, 6727, 3839, 6330, 6171, 4248]   \n",
       "16   [50636, 5974, 7262, 5918, 6642, 6522, 5540, 8836]   \n",
       "17   [52908, 7671, 8883, 6737, 4589, 6255, 5721, 4566]   \n",
       "18   [52488, 4852, 9832, 6612, 7004, 4072, 6602, 5868]   \n",
       "19   [52829, 7941, 8685, 6531, 4439, 6180, 6278, 4447]   \n",
       "20   [51386, 5092, 9268, 5896, 3596, 5352, 9602, 7138]   \n",
       "21   [52836, 7895, 7916, 7939, 3986, 6303, 6155, 4300]   \n",
       "22   [52004, 5540, 7326, 7116, 3798, 4794, 9428, 7324]   \n",
       "23   [53130, 7842, 7508, 7309, 3373, 6323, 6899, 4946]   \n",
       "24   [52054, 3518, 8814, 6314, 4524, 5436, 7862, 8808]   \n",
       "25   [52928, 7649, 7906, 7770, 4297, 6035, 6048, 4697]   \n",
       "\n",
       "                      test preds steady subj5 avg2folds  \\\n",
       "0    [41677, 5808, 6131, 5142, 8978, 13131, 9451, 6554]   \n",
       "1   [44828, 5115, 8246, 5673, 7816, 10145, 10383, 4666]   \n",
       "2   [44820, 4944, 5629, 6212, 6996, 11084, 12243, 4944]   \n",
       "3    [44650, 5211, 6166, 7183, 6559, 12127, 9700, 5276]   \n",
       "4   [43665, 5496, 6070, 7341, 7203, 11224, 11157, 4716]   \n",
       "5   [44634, 5457, 6599, 6428, 7675, 10613, 10054, 5412]   \n",
       "6    [46128, 5281, 6577, 6521, 8039, 10520, 8288, 5518]   \n",
       "7    [45202, 5160, 7282, 6341, 8477, 10099, 9139, 5172]   \n",
       "8   [43684, 4420, 5670, 6176, 14072, 13966, 4870, 4014]   \n",
       "9   [46397, 5147, 4691, 6809, 7651, 10073, 11881, 4223]   \n",
       "10  [48278, 5982, 6684, 4460, 10034, 13402, 4310, 3722]   \n",
       "11  [47272, 5153, 4917, 6763, 7963, 10022, 10793, 3989]   \n",
       "12  [47532, 5336, 6984, 5246, 10372, 11996, 5834, 3572]   \n",
       "13  [46323, 4716, 4715, 6521, 8965, 10314, 11395, 3923]   \n",
       "14  [46334, 3904, 11540, 2474, 9882, 11494, 6984, 4260]   \n",
       "15  [45756, 5223, 5410, 6528, 7079, 11233, 11441, 4202]   \n",
       "16  [50416, 5322, 5538, 4050, 12564, 10216, 3294, 5472]   \n",
       "17  [45543, 5192, 5154, 6548, 6812, 10160, 12826, 4637]   \n",
       "18   [46022, 3874, 9630, 3926, 9904, 15766, 4464, 3286]   \n",
       "19  [44920, 5266, 5575, 6203, 7407, 10399, 11652, 5450]   \n",
       "20  [46120, 3920, 10744, 4122, 4070, 16082, 5458, 6356]   \n",
       "21  [44567, 5742, 4865, 6015, 7394, 10392, 12169, 5728]   \n",
       "22   [45114, 4676, 8444, 4904, 8016, 14036, 6520, 5162]   \n",
       "23  [44067, 4681, 5288, 5481, 6977, 11840, 13153, 5385]   \n",
       "24  [42992, 5368, 10584, 4186, 7340, 18404, 5026, 2972]   \n",
       "25  [46112, 4493, 6393, 5967, 7386, 10697, 10716, 5108]   \n",
       "\n",
       "                       test preds steady subj6 avg2folds  \\\n",
       "0   [36381, 7532, 12002, 7409, 12199, 6717, 10644, 4496]   \n",
       "1    [35180, 7878, 8591, 6620, 12448, 9308, 12962, 4393]   \n",
       "2    [38982, 7702, 10332, 7889, 8685, 8117, 11674, 3999]   \n",
       "3     [38431, 7660, 9622, 8804, 9962, 8500, 10432, 3969]   \n",
       "4    [38720, 7628, 11747, 7897, 9376, 7944, 10219, 3849]   \n",
       "5     [39161, 8029, 10414, 9881, 8867, 8154, 8687, 4187]   \n",
       "6     [39324, 8032, 10180, 9062, 9033, 8306, 9025, 4418]   \n",
       "7      [40511, 7815, 9473, 9209, 8422, 7682, 9360, 4908]   \n",
       "8      [38168, 9210, 8626, 7652, 9938, 9642, 8862, 5282]   \n",
       "9    [39878, 8133, 8943, 6423, 10118, 7564, 11789, 4532]   \n",
       "10    [45246, 7790, 12434, 6740, 5646, 7046, 8728, 3750]   \n",
       "11    [39115, 7736, 9892, 7711, 8090, 8379, 12700, 3757]   \n",
       "12    [39886, 8680, 10852, 8656, 6620, 9854, 8820, 4012]   \n",
       "13    [38583, 7840, 9710, 7760, 8381, 9161, 12250, 3695]   \n",
       "14   [42966, 6202, 15176, 8192, 11262, 4948, 4206, 4428]   \n",
       "15   [38883, 9274, 10716, 8243, 9800, 6100, 10356, 4008]   \n",
       "16    [39566, 8434, 14950, 6858, 7332, 8940, 6398, 4902]   \n",
       "17   [39478, 8398, 11161, 8398, 7417, 7330, 10668, 4530]   \n",
       "18    [44732, 7048, 10142, 5362, 9596, 6980, 8858, 4662]   \n",
       "19   [38973, 7966, 10911, 8703, 8031, 7583, 10927, 4286]   \n",
       "20    [49072, 6622, 11480, 8812, 1636, 6492, 8284, 4982]   \n",
       "21   [42100, 7506, 10245, 8876, 10614, 6370, 8509, 3160]   \n",
       "22    [45260, 7108, 10862, 5708, 9482, 6994, 8052, 3914]   \n",
       "23    [40051, 7710, 13559, 8997, 6755, 7379, 8943, 3986]   \n",
       "24    [43786, 6496, 15370, 4938, 6812, 8178, 8168, 3632]   \n",
       "25   [38796, 8481, 11897, 8895, 8268, 6376, 10022, 4645]   \n",
       "\n",
       "                     test preds steady subj7 avg2folds  \\\n",
       "0                         [97942, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1                         [97942, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2    [54955, 8021, 7953, 4404, 5753, 3206, 7772, 5878]   \n",
       "3    [57191, 8231, 5578, 4329, 5725, 2475, 8116, 6297]   \n",
       "4    [56763, 8292, 5717, 4860, 5750, 2313, 8025, 6222]   \n",
       "5    [54289, 8005, 5533, 5024, 7263, 3152, 7692, 6984]   \n",
       "6    [55071, 6924, 4619, 5158, 7330, 2840, 8064, 7936]   \n",
       "7    [56393, 6867, 4145, 4588, 6427, 3156, 8034, 8332]   \n",
       "8    [51120, 9808, 4566, 5468, 4404, 5906, 8770, 7900]   \n",
       "9    [58284, 9226, 6306, 3983, 5022, 2858, 8093, 4170]   \n",
       "10   [56820, 6956, 5276, 4440, 4846, 3866, 7830, 7908]   \n",
       "11   [57839, 9572, 5084, 4445, 5530, 2836, 8598, 4038]   \n",
       "12   [53948, 7610, 3666, 3852, 6784, 5568, 7046, 9468]   \n",
       "13   [57281, 9302, 5210, 4879, 5957, 2791, 8536, 3986]   \n",
       "14  [57874, 5474, 8832, 3732, 4654, 3132, 3768, 10476]   \n",
       "15   [55056, 9750, 4998, 4711, 5786, 2082, 9538, 6021]   \n",
       "16   [56504, 6228, 4612, 4030, 6134, 4520, 6700, 9214]   \n",
       "17   [57720, 7950, 6216, 4903, 5624, 2604, 7657, 5268]   \n",
       "18   [56972, 5830, 5946, 3732, 7226, 4180, 7344, 6712]   \n",
       "19   [58575, 8694, 6085, 4534, 4646, 2318, 7832, 5258]   \n",
       "20  [55104, 6112, 6316, 4492, 2232, 7126, 5112, 11448]   \n",
       "21   [57417, 8907, 6313, 4825, 5089, 1912, 8253, 5226]   \n",
       "22   [57534, 6276, 6316, 3256, 6288, 4824, 5844, 7604]   \n",
       "23   [58468, 8908, 6891, 4569, 4679, 1668, 7627, 5132]   \n",
       "24   [53148, 6688, 8498, 3514, 5866, 5084, 5618, 9526]   \n",
       "25   [56303, 9278, 7391, 5021, 5357, 1881, 7366, 5345]   \n",
       "\n",
       "                     test preds steady subj8 avg2folds  \\\n",
       "0    [73835, 5280, 3776, 2604, 3837, 2701, 4316, 2719]   \n",
       "1    [74923, 3967, 4307, 2553, 4213, 2639, 4008, 2458]   \n",
       "2    [73546, 4095, 4041, 2086, 4118, 2839, 4980, 3363]   \n",
       "3    [75288, 4047, 3520, 2274, 4141, 3326, 4238, 2234]   \n",
       "4    [75015, 4012, 4625, 2322, 3900, 3177, 3601, 2416]   \n",
       "5    [72966, 4448, 4262, 2559, 4544, 3116, 3788, 3385]   \n",
       "6    [72851, 5088, 4719, 2535, 4165, 2882, 4261, 2567]   \n",
       "7    [73831, 4962, 3703, 2337, 4295, 2823, 4136, 2981]   \n",
       "8   [65166, 10368, 2506, 4848, 4748, 6562, 3484, 1386]   \n",
       "9    [74421, 5471, 3941, 2283, 4063, 3345, 4055, 1489]   \n",
       "10   [74936, 4176, 4784, 3334, 3426, 3664, 2756, 1992]   \n",
       "11   [74457, 4772, 4305, 2390, 4635, 2889, 3701, 1919]   \n",
       "12   [73482, 5908, 3010, 3600, 2856, 5824, 2790, 1598]   \n",
       "13   [75103, 4315, 4586, 2428, 4581, 2618, 3266, 2171]   \n",
       "14   [70210, 7390, 4412, 3128, 5098, 2338, 3494, 2998]   \n",
       "15   [72525, 5743, 4464, 2922, 3650, 2529, 4837, 2398]   \n",
       "16   [75968, 7144, 2918, 2256, 3620, 2050, 2754, 2358]   \n",
       "17   [74007, 5180, 4295, 2320, 3831, 2654, 4605, 2176]   \n",
       "18   [74388, 5444, 3728, 3242, 3560, 4368, 3098, 1240]   \n",
       "19   [74081, 5478, 4171, 2392, 3639, 2806, 4381, 2120]   \n",
       "20   [69100, 5760, 3594, 5358, 2076, 6262, 3288, 3630]   \n",
       "21   [71724, 4936, 5393, 2295, 5368, 2981, 4312, 2059]   \n",
       "22   [70394, 7390, 3274, 2900, 3674, 5830, 3602, 2004]   \n",
       "23   [72450, 6850, 3990, 2452, 3710, 3309, 4096, 2211]   \n",
       "24   [66340, 9346, 4616, 3032, 3626, 6634, 3294, 2180]   \n",
       "25   [72099, 5732, 3460, 2784, 4115, 4313, 4315, 2250]   \n",
       "\n",
       "                       test preds steady subj9 avg2folds finetune  \n",
       "0      [68396, 2106, 2865, 3539, 6398, 6286, 4247, 3403]      NaN  \n",
       "1     [46626, 4676, 7847, 6280, 10477, 9842, 6589, 4903]      NaN  \n",
       "2    [45239, 4659, 6754, 5554, 11795, 11614, 5909, 5716]     25.0  \n",
       "3    [44094, 4775, 5595, 7104, 12449, 13096, 4691, 5436]     50.0  \n",
       "4    [45302, 5342, 4967, 6092, 12818, 12275, 5342, 5102]     75.0  \n",
       "5    [43836, 4776, 6043, 7410, 13825, 10651, 5289, 5410]     25.0  \n",
       "6    [44943, 4741, 6018, 7261, 13895, 10012, 5111, 5259]     50.0  \n",
       "7    [42834, 5174, 6948, 7438, 13104, 10737, 5819, 5186]     50.0  \n",
       "8    [36440, 7978, 7538, 13962, 14572, 7888, 4650, 4212]        n  \n",
       "9    [45078, 4215, 5158, 7160, 11889, 14052, 4732, 4956]        n  \n",
       "10    [42438, 8546, 9462, 7578, 14704, 6542, 3638, 4332]        n  \n",
       "11   [45846, 4180, 5189, 7095, 12485, 11424, 5391, 5630]        n  \n",
       "12    [39034, 9392, 8460, 9852, 12194, 9552, 4064, 4692]        n  \n",
       "13   [46070, 4141, 5089, 6895, 12692, 12437, 4715, 5201]        n  \n",
       "14    [43862, 3830, 9486, 7016, 19134, 3818, 4024, 6070]        n  \n",
       "15   [42347, 4599, 4950, 6887, 14091, 12194, 6430, 5742]        n  \n",
       "16    [42902, 7786, 7682, 6112, 17384, 7754, 3222, 4398]        n  \n",
       "17   [44568, 4370, 4871, 6689, 13460, 11456, 5749, 6077]        n  \n",
       "18    [40726, 7180, 9826, 6936, 15304, 8396, 4754, 4118]        n  \n",
       "19   [45777, 4279, 4948, 6980, 13640, 11114, 5304, 5198]        n  \n",
       "20    [40374, 5920, 14254, 8060, 8398, 9616, 4710, 5908]        n  \n",
       "21   [46306, 4456, 5878, 4610, 11684, 13356, 5765, 5185]        n  \n",
       "22    [38044, 9604, 7380, 8276, 14102, 9836, 4612, 5386]        n  \n",
       "23   [44264, 5042, 5333, 6796, 13680, 10768, 5686, 5671]        n  \n",
       "24  [39314, 5580, 13276, 8262, 10204, 10764, 4768, 5072]        n  \n",
       "25   [42791, 5639, 6897, 6571, 10244, 12293, 5872, 6933]        n  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>pretrain</th>\n",
       "      <th>training_config</th>\n",
       "      <th>test accuracy steady avg2folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>no</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.603985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>no</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.610272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>yes_nonfixed</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.620513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>yes_nonfixed</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.641730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>yes_nonfixed</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.642652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.635074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.630556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.628716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.599935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.639611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.613291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.645398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.612123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.647005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.604689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.636708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.618058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.644771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.612312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.645612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.602787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.638908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.611254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.643576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}</td>\n",
       "      <td>0.603518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}</td>\n",
       "      <td>0.642841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth      pretrain  \\\n",
       "0       8            no   \n",
       "1       8            no   \n",
       "2       8  yes_nonfixed   \n",
       "3       8  yes_nonfixed   \n",
       "4       8  yes_nonfixed   \n",
       "5       8           yes   \n",
       "6       8           yes   \n",
       "7       8           yes   \n",
       "8       2           yes   \n",
       "9       2           yes   \n",
       "10      2           yes   \n",
       "11      2           yes   \n",
       "12      2           yes   \n",
       "13      2           yes   \n",
       "14      3           yes   \n",
       "15      3           yes   \n",
       "16      3           yes   \n",
       "17      3           yes   \n",
       "18      3           yes   \n",
       "19      3           yes   \n",
       "20      4           yes   \n",
       "21      4           yes   \n",
       "22      4           yes   \n",
       "23      4           yes   \n",
       "24      4           yes   \n",
       "25      4           yes   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    training_config  \\\n",
       "0   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "1   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "2   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "3   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "4   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "5                                                                                               {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "6                                                                                               {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "7                                                                                               {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "8                                                                                                    {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "9   {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "10                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "11  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "12                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "13  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "14                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "15  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "16                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "17  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "18                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "19  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "20                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "21  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "22                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "23  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "24                                                                                                   {'epochs': 20, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'StepLR', 'lr_scheduler_hparams': {'step_size': 10, 'gamma': 0.1}}   \n",
       "25  {'epochs': 75, 'batch_size': 64, 'optim': 'AdamW', 'optim_hparams': {'lr': 0, 'betas': (0.9, 0.999), 'weight_decay': 0.01}, 'lr_scheduler': 'CyclicLR', 'lr_scheduler_hparams': {'base_lr': 1e-07, 'max_lr': 0.001, 'step_size_up': 50, 'step_size_down': None, 'mode': 'triangular', 'cycle_momentum': False}}   \n",
       "\n",
       "    test accuracy steady avg2folds  \n",
       "0                         0.603985  \n",
       "1                         0.610272  \n",
       "2                         0.620513  \n",
       "3                         0.641730  \n",
       "4                         0.642652  \n",
       "5                         0.635074  \n",
       "6                         0.630556  \n",
       "7                         0.628716  \n",
       "8                         0.599935  \n",
       "9                         0.639611  \n",
       "10                        0.613291  \n",
       "11                        0.645398  \n",
       "12                        0.612123  \n",
       "13                        0.647005  \n",
       "14                        0.604689  \n",
       "15                        0.636708  \n",
       "16                        0.618058  \n",
       "17                        0.644771  \n",
       "18                        0.612312  \n",
       "19                        0.645612  \n",
       "20                        0.602787  \n",
       "21                        0.638908  \n",
       "22                        0.611254  \n",
       "23                        0.643576  \n",
       "24                        0.603518  \n",
       "25                        0.642841  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['depth', 'pretrain','training_config', 'test accuracy steady avg2folds']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
